1、平台搭建
2、日志采集、带宽统计、日志打包

```
rsyslog -> kafka -> fluentd -> es/hdfs
fluentd -> es/hdfs
	fluentd -> kafka -> fluentd -> es/hdfs
	flume -> es/hdfs
		flume -> kafka -> flume -> es/hdfs
			flume -> kafka -> flume -> es/hbase
				flume -> kafka -> flume/spark -> es/hbase
```

日志完整性、fluentd源码修改、flume source修改、flume监控、优化
统计、监控
运维工具、sparkSQL、hue（hive/hbase整合，sql查询）
yarn资源调度

flume配置采集厂商日志
flume处理多个业务日志（从多个kafka主题接收数据入es，type值不同）


平台：
	cdh、hue、elasticsearch、kibana、kopf、x-pack、elasticsearch-head、
	jenkins、kafka manager、配置文件同步工具
定时任务：
	crontab、jenkins、Curator
elasticsearch
	备份：龙存、hdfs（repository-hdfs）
	数据定时迁移
	ssd和sata

高可用：
	keepalived、

监控
	微信报警、速率、日志条数统计核对、

带宽核对(logstach 同步数据到mysql)、修复


内网ip、vnc


# 2 问题
- 各集群版本
  - fluentd : 2.3
  - flume : apache-flume-1.6.0-cdh5.8.3
  - kafka ：kafka_2.11-0.10.2.0
  - es : 5.2.0
  - cdh: 5.8.3
  - Hadoop: 2.6.0-cdh5.8.3
  - hbase: 1.2.0+cdh5.8.3
  - hive: 1.1.0+cdh5.8.3
  - zookeeper: 3.4.5+cdh5.8.3
  - spark
- es备份、迁移、合并，x-pack破解踩的坑
- es单个服务器多进程；标签host、ssd；es代理；client、master、datanode；映射
- hbase-hive
- yarn
- 




nginx高可用、内网、es代理、curator（删除、合并）、es迁移、es扩容、es升级、es下架某台机器、x-pack破解遇到的问题（kibana里x-pack有没有处理）

cdh扩容、下架机器、logstash-mysql、python提供的web服务、python上传文件到云盘、hive-hbase整合、kylin

带宽统计（java、spark-es、spark-streaming）、快手指标分析、带宽核对、修复

日志下载（朱伟 python）

druid、kafka-offset-mgr_linux_amd64.zip、kafka-offset-setter-master.zip

# 3. jenkins带宽重算-hbase
## 命令行-自建CDN删除带宽:
```
curl -XPOST '172.18.128.47:9200/statsdata_5m_2018.01.17,statsdata_m_2018.01.18/bandself/_delete_by_query?scroll_size=5000&pretty' -d '
{
  "query": {
    "bool": {
      "must": [   
 {
          "range": {
            "from_time": {

              "gte": "1513144800",
              "lt": "1513148400"
            }
          }
        },
        {
          "terms": {
            "firm_name": [
              "JX"
            ]
          }
        }
      ]
    }
  }
}'|tee -a  delete_es.log
```



## 命令行-融合CDN删除带宽:
```
curl -XPOST '172.18.128.47:9200/statsdata_5m_2018.01.23,statsdata_m_2018.01.23/bandwidth/_delete_by_query?scroll_size=5000&pretty' -d '
{
    "query": {
        "bool": {
            "must": [
                {
                    "terms": {
                        "firm_name": [
                            "BS"
                        ]
                    }
                },
                {
                    "range": {
                        "from_time": {
                            "gte": "1516672800",
                            "lt": "1516723200"
                        }
                    }
                }
            ]
        }
    }
}'
```


## 命令行-融合CDN根据文件名删除带宽
根据索引名和文件名删除带宽
```
curl -XPOST '183.131.54.135:9200/statsdata_5m_2017.05.03,statsdata_m_2017.05.03/bandwidth/_delete_by_query?scroll_size=5000&pretty' -d '
{
  "query": {
    "bool": {
      "must": [      
        {
          "terms": {
            "file_name": ["8156840_jxy.pp.starschinalive.com_2017050301_TX.access_2000000_10_3"]
          }
        }
      ]
    }
  }
}'
```

## JX带宽重算（Hase）(54.145)
功能：重新计算JX的1分钟和5分钟带宽
场景：正常带宽计算出现问题时使用该工具，修正带宽数据。先通过脚本删除指定时间日志。再通过此工具将删除日志重算
限制条件： 此工具产生的数据可以互相覆盖，但和线上的数据主健产生规则不同。必须先删除旧数据，再重新计算
输入参数：1：开始时间(例如：2017040500) 2：结束时间 3：ES Client IP
```
su - hdfs -s /bin/bash -c "spark-submit --class com.sinobbd.strider.dota.band.JXBandwidthHbase --conf spark.yarn.queue="es"  --master yarn --deploy-mode cluster --driver-memory 4g --num-executors 30 --executor-memory 5G --executor-cores 3 --total-executor-cores 91  --conf spark.default.parallelism=300 --conf spark.storage.memoryFraction=0.4 --conf spark.shuffle.memoryFraction=0.3 --jars /bbd/sata09/bigdata/sinobbd/task/sparkonHbase/snakeyaml-1.15.jar,/bbd/sata09/bigdata/sinobbd/task/sparkonHbase/metrics-core-2.2.0.jar  /bbd/sata09/bigdata/sinobbd/task/sparkonHbase/fusionCDNStream.jar 201801180740 201801180820 183.136.128.47"
```

## 融合CDN带宽重算(Hbase)(54.145)
功能描述：从Hbase读取日志，计算指定时间和厂商的带宽，更新到带宽结果表中
参数说明：开始时间（例如：201705030000）结束时间、EShost（例如：183.131.54.181）、厂商（例如：BS）可空，空为所有
```
su - hdfs -s /bin/bash -c "spark-submit --class com.sinobbd.strider.dota.band.FusionBandHbase_new --conf spark.yarn.queue="es"  --master yarn --deploy-mode cluster --driver-memory 3g --num-executors 30 --executor-memory 2G --executor-cores 3 --total-executor-cores 91 --conf spark.default.parallelism=300 --conf spark.storage.memoryFraction=0.4 --conf spark.shuffle.memoryFraction=0.3  --jars /bbd/sata09/bigdata/sinobbd/task/sparkonHbase/snakeyaml-1.15.jar,/bbd/sata09/bigdata/sinobbd/task/sparkonHbase/metrics-core-2.2.0.jar  /bbd/sata09/bigdata/sinobbd/task/sparkonHbase/fusionCDNStream.jar 201801231000 201801240000 183.136.128.47 BS"
```

## 03-融合CDN带宽按文件重算(Hbase)
根据文件名计算带宽(因rowkey的组成变更，20170503之后的数据可用）
参数：开始时间、结束时间、ES Host、厂商编码、文件名列表 
示例：201705120000 201705130000 183.136.128.47 BS 8766699_rd2cdn.kuro.cn_20170512013500_BS.access_50,8766317_test.sinobbd.com_20170512011000_BS.access_145
注意：重算前请先删除原有带宽数据
```
su - hdfs -s /bin/bash -c "spark-submit --class com.sinobbd.strider.dota.band.GenFusionBandByFileNames --conf spark.yarn.queue="es"  --master yarn --deploy-mode cluster --driver-memory 4g --num-executors 20 --executor-memory 4G --executor-cores 3 --total-executor-cores 60 --conf spark.default.parallelism=300 --conf spark.storage.memoryFraction=0.4 --conf spark.shuffle.memoryFraction=0.2  --jars /bbd/sata09/bigdata/sinobbd/task/sparkonHbase/elasticsearch-spark-20_2.11-5.2.0.jar,/bbd/sata09/bigdata/sinobbd/task/sparkonHbase/snakeyaml-1.15.jar,/bbd/sata09/bigdata/sinobbd/task/sparkonHbase/metrics-core-2.2.0.jar  /bbd/sata09/bigdata/sinobbd/task/sparkonHbase/fusionCDNStream.jar 201707210000 201707220210 183.136.128.47 UP1 all"
```


## 融合CDN根据文件名重算带宽
根据文件名计算带宽(因rowkey的组成变更，20170503之后的数据可用）
参数：开始时间、结束时间、ES Host、厂商编码、文件名列表 
示例：201705120000 201705130000 183.136.128.47 BS 8766699_rd2cdn.kuro.cn_20170512013500_BS.access_50,8766317_test.sinobbd.com_20170512011000_BS.access_145
注意：重算前请先删除原有带宽数据
```
su - hdfs -s /bin/bash -c "spark-submit --class com.sinobbd.strider.dota.band.GenFusionBandByFileNames --conf spark.yarn.queue="es"  --master yarn --deploy-mode cluster --driver-memory 4g --num-executors 20 --executor-memory 4G --executor-cores 3 --total-executor-cores 60 --conf spark.default.parallelism=300 --conf spark.storage.memoryFraction=0.4 --conf spark.shuffle.memoryFraction=0.2  --jars /bbd/sata09/bigdata/sinobbd/task/sparkonHbase/elasticsearch-spark-20_2.11-5.2.0.jar,/bbd/sata09/bigdata/sinobbd/task/sparkonHbase/snakeyaml-1.15.jar,/bbd/sata09/bigdata/sinobbd/task/sparkonHbase/metrics-core-2.2.0.jar  /bbd/sata09/bigdata/sinobbd/task/sparkonHbase/fusionCDNStream.jar 201707260000 201707270000 183.131.54.181 TX 13276529_bbd.qiniu.sohu.vmoiver.com_201707261530_TX.access_518"
```



## 01-融合带宽校验(Hbase)
按天校验带宽数据。sparkstreaming计算的带宽和从Hbase查询计算的带宽，按照文件维度进行核对。将核对结果写入ES索引log-band-check中
```
su - hdfs -s /bin/bash -c "spark-submit --class com.sinobbd.strider.dota.band.FusionBandCheck --conf spark.yarn.queue="es"  --master yarn --deploy-mode cluster --driver-memory 4g --num-executors 30 --executor-memory 3G --executor-cores 2 --total-executor-cores 61 --conf spark.default.parallelism=300 --conf spark.storage.memoryFraction=0.4 --conf spark.shuffle.memoryFraction=0.2  --jars /bbd/sata09/bigdata/sinobbd/task/sparkonHbase/elasticsearch-spark-20_2.11-5.2.0.jar,/bbd/sata09/bigdata/sinobbd/task/sparkonHbase/snakeyaml-1.15.jar,/bbd/sata09/bigdata/sinobbd/task/sparkonHbase/metrics-core-2.2.0.jar  /bbd/sata09/bigdata/sinobbd/task/sparkonHbase/fusionCDNStream.jar 183.136.128.47 20170702"
```


## 02-融合带宽异常数据删除
检查 log-band-chk中的异常文件，按照异常文件将带宽计算结果中的数据删除
```
python /bbd/sata09/bigdata/sinobbd/task/sparkonHbase/deleteBandByFiles.py statsdata_5m_2017.05.05
```
deleteBandByFiles.py
```
#!/usr/bin/env python
#coding=utf-8
import time,datetime
import requests
import os,sys,json
import argparse
from datetime import datetime
from elasticsearch import Elasticsearch
from elasticsearch import  helpers

reload(sys)
sys.setdefaultencoding('utf8')

parser = argparse.ArgumentParser()
parser.add_argument("message")
args = parser.parse_args()
band_index=args.message
es = Elasticsearch("183.131.54.135:9200", timeout=1000, sniff_on_start=True, sniff_on_connection_fail=True,
                   sniffer_timeout=60, sniff_timeout=10, max_retries=10)
bodys1 = {"query": {"bool": {"must": [{"term": {"is_normal": "N"}}, {"term": {"band_index": band_index}}]}}, "from": 0,
          "size": 10000}
res = es.search(index="log-band-chk", doc_type="task", body=json.dumps(bodys1))
successMap = {}
filesdel=res['hits']['hits']
total=str(len(filesdel))
print('待处理文件个数:'+total+'')
count=0
for hit in filesdel:
    file_name = hit["_source"]["file_name"]
    band_index = hit["_source"]["band_index"]
    time_stamp = hit["_source"]["log_date"] / 1000
    current_time = time.localtime(time_stamp)
    curr_time_str = time.strftime("%Y.%m.%d", current_time)
    count+=1
    print(str(count)+'/'+total+' 文件名:' + file_name + '')
    #print(curr_time_str)

    index_names = []
    for pre in ("statsdata_m_", "statsdata_5m_"):
#	print(pre + curr_time_str)
        if es.indices.exists(pre + curr_time_str):
           index_names.append(pre + curr_time_str)
        else:
	    print('不存在索引:' + (pre + curr_time_str) + '')
    existsIndexName = ",".join(index_names)
    if(existsIndexName.strip()==''):
        print('不存在索引，跳过文件'+file_name)
        continue
    query2 = {"query": {"bool": {"must": [{"term": {"file_name": file_name}}]}}}
    try:
        es.delete_by_query(index=existsIndexName, body=json.dumps(query2),
                           doc_type='bandwidth', conflicts="proceed", wait_for_completion=True, timeout="60m")
        res1 = es.search(index=existsIndexName, doc_type="bandwidth", body=json.dumps(query2))
        print(res1['hits']['total'])
        if res1['hits']['total'] != 0:
            continue

        #insert success info into es
        '''
        data = {
            "@timestamp": datetime.now().strftime("%Y-%m-%dT%H:%M:%S.000+0800"),
            "file_name": file_name,
            "band_index": band_index,
            "status":0
        }
        '''
        rowid = band_index+"----" + file_name
        print("delete success: "+rowid)
        #resIndex= es.index(index="log-band-redo", doc_type="task",id=rowid, body=data)
    except Exception as e:
        print(e)
        print("1")
```

## 根据文件名计算带宽（Hbase）
根据文件名计算带宽(因rowkey的组成变更，20170503之后的数据可用） 
参数：开始时间、结束时间、ES Host、厂商编码、文件名列表 
示例：201705120000 201705130000 183.136.128.47 BS 8766699_rd2cdn.kuro.cn_20170512013500_BS.access_50,8766317_test.sinobbd.com_20170512011000_BS.access_145 
注意：重算前请先删除原有带宽数据
```
su - hdfs -s /bin/bash -c "spark-submit --class com.sinobbd.strider.dota.band.GenFusionBandByFileNames --conf spark.yarn.queue="es"  --master yarn --deploy-mode cluster --driver-memory 4g --num-executors 20 --executor-memory 4G --executor-cores 3 --total-executor-cores 60 --conf spark.default.parallelism=300 --conf spark.storage.memoryFraction=0.4 --conf spark.shuffle.memoryFraction=0.2  --jars /bbd/sata09/bigdata/sinobbd/task/sparkonHbase/elasticsearch-spark-20_2.11-5.2.0.jar,/bbd/sata09/bigdata/sinobbd/task/sparkonHbase/snakeyaml-1.15.jar,/bbd/sata09/bigdata/sinobbd/task/sparkonHbase/metrics-core-2.2.0.jar  /bbd/sata09/bigdata/sinobbd/task/sparkonHbase/fusionCDNStream.jar 201708010000 201708030100  183.131.54.181 TX 14026915_img.wp.levect.com_201708022215_TX.access_255165"
```


# jenkins常用工具
## elasticsearch备份数据恢复
将已经备份的索引在删除后进行索引的恢复，使用方法：
/bbd/ssd01/app/zrk/recover_index.sh indexTagName
查看已备份的数据
```
curl -XGET 'http://183.136.128.47:9200/_snapshot/my_hdfs_repository5/_all?pretty'
```
恢复数据
```
/bbd/ssd01/app/zrk/recover_index.sh statsdata2017.06
```
recover_index.sh
```
snapshotName=$1
curl -XPOST 183.136.128.47:9200/_snapshot/my_hdfs_repository5/$snapshotName/_restore
```




## Hbase查询统计
描述
> 生成的结果ES索引名：hbase_aggbysql/task
>
> 参数：1：hbase tablename 2：RowKey Begin 3：RowKey end 4：Info valueFilters 5：Info valueFilters 6：SQL 
> SQL中表名：tempdata
> 可用列名：
> info列：clientip, response_time, time_local, domain, request,  http_status, hit_status,body_bytes_sent, 
> info1列： firm_name, time_unix,prov, city, ISP, host, layer, req_count, server_addr, X_Info_request_id, file_name, line_num
> param4、param5如果为空，用-表示
>
> 示例1：SELECT domain, request,sum(req_count) as count FROM tempdata where domain= 'gx.a.yximgs.com' and layer=1 group by domain, request ORDER BY count DESC LIMIT 50
> 示例2：SELECT * FROM tempdata where domain= 'bbd.qiniu.sohu.vmoiver.com' LIMIT 50
>
> sql="\"SELECT cast(time_unix*1000 as long) as log_date,(body_bytes_sent/response_time) as downratio,* FROM tempdata WHERE domain = '\"$parame_domain\"' and layer='4' and host like '\"$parame_host%\"'   \""

Command
```
table_name="jx20171125"
begin_time="201711251930"
end_time="201711252000"
parame_domain="-"
parame_host="-"

sql="\"SELECT '1237' as tag,cast(time_unix*1000 as long) as log_date,domain,request,p_server   FROM tempdata WHERE ISP = '移动' and prov='山东' and p_server in ('172.16.140.22:9090','172.16.140.20:9090','172.16.140.7:9090') \""

su - hdfs -c "/bbd/sata09/bigdata/sinobbd/task/yunwei/response_time_avg/aggbysql $table_name $begin_time $end_time $parame_domain $parame_host $sql"
```



## Project 七牛回源统计导出
描述
> 按照def1 分组计算每个5分钟点的带宽，并导出到csv文件，每个厂商一个文件  导出列名为：  时间、流量（M）、带宽(Mbps) ，所需要的参数是indexName，indextype，导出的文件目录为：c3n-zj-nb1-183-136-128-47下的/bbd/ssd01/app/zrk/resultdir

Command
```
sh /bbd/ssd01/flask/app/aggESToFileAll.sh nginx-qn2-201802 fluentd
```
aggESToFileAll.sh
```
indexName=$1
indexType=$2
month=${indexName:10}
echo ${month:0:4}
echo ${month:4}
startTime=`date -d "${month:0:4}-${month:4}-01" +%s`
startMi=${startTime}000
endTime=`expr $startTime + 2678400`
echo $endTime
endMi=${endTime}000
echo $startMi
echo $endMi
/bbd/ssd01/flask/flask/bin/python /bbd/ssd01/flask/app/aggESToFileAll.py $indexName $indexType /bbd/ssd01/app/zrk/resultdir $startMi $endMi
```
aggESToFileAll.py
```
import hashlib
import os
import shutil
import json
import time
import sys
import requests
from elasticsearch import Elasticsearch
from elasticsearch import helpers
#statsdata_5m_2017.11.08  bandwidth
es = Elasticsearch("183.136.128.47:9200", timeout=1000, sniff_on_start=True, sniff_on_connection_fail=True,
                   sniffer_timeout=60, sniff_timeout=10, max_retries=10)
successMap = {}
bodys1 = {"size":0,"query":{"bool":{"must":[{"range":{"time_local":{"gte":sys.argv[4],"lte":sys.argv[5],"format":"epoch_millis"}}}],"must_not":[]}},"_source":{"excludes":[]},"aggs":{"2":{"date_histogram":{"field":"time_local","interval":"5m","time_zone":"Asia/Shanghai","min_doc_count":1,"format" : "yyyy-MM-dd HH:mm:ss"},"aggs":{"3":{"terms":{"field":"def1","size":5,"order":{"1":"desc"}},"aggs":{"1":{"sum":{"field":"body_bytes_sent"}}}}}}}}
res = es.search(index=sys.argv[1], doc_type=sys.argv[2], body=json.dumps(bodys1))
#res1=requests.post("http://183.136.128.47:9200/logrestore-task/fluentd/_search",json.dumps(bodys1))
path=sys.argv[3]
try:
    os.mkdir(path)
except OSError:
    pass
shutil.rmtree(path)
os.mkdir(path)
# source=res[]
actions=[]

for hit in res['aggregations']['2']['buckets']:
    # print(hit)
    # hit1=json.loads(hit)
    time_key = hit["key_as_string"]
    for domain1 in hit["3"]["buckets"]:
        firm_name = domain1["key"]
        # print(domain)
        traff=str(domain1["1"]["value"])
        traffic = str(domain1["1"]["value"]*8//300000000)
        with open(path+'/'+firm_name + '.csv', 'a') as f:
            if os.path.getsize(path+'/'+firm_name + '.csv') == 0:
                f.write("时间,带宽,流量(Mbps)\n")
            f.write(time_key + "," + traffic+","+traff+"\n")
print("access task success")
        # return "access task success"

```

## 异常带宽分析工具
1、根据时间范围和厂商查找流量差异率大的域名和时间
2、根据时间、域名等从带宽表中统计文件名和总行数
3、根据文件名(按文件前缀匹配)，从带宽表中统计总行数
```
python /bbd/ssd01/zz/python/checkFilesByBand.py "1" 1515544200 1515544500 "TX"
#python /bbd/ssd01/zz/python/checkFilesByBand.py "2" 1515497100 1515497400 "BS" "cdn-3.cp44.ott.cibntv.net"
#python /bbd/ssd01/zz/python/checkFilesByBand.py "3" "statsdata_5m_2018.01.08,statsdata_5m_2018.01.09,statsdata_5m_2018.01.10" "30582022_cdn-3.cp44.ott.cibntv.net_20180109192500_BS.access_294"
```
checkFilesByBand.py
```
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import time,datetime
import requests
import os,sys,json

def timestamp_datetime(value,format = '%Y.%m.%d'):
   # format = '%Y.%m.%d'
    # value为传入的值为时间戳(整形)，如：1332888820
    value = time.localtime(value)
    ## 经过localtime转换后变成
    ## time.struct_time(tm_year=2012, tm_mon=3, tm_mday=28, tm_hour=6, tm_min=53, tm_sec=40, tm_wday=2, tm_yday=88, tm_isdst=0)
    # 最后再经过strftime函数转换为正常日期格式。
    dt = time.strftime(format, value)
    return dt

def check_byfile():
    band_names = sys.argv[2]  # 'statsdata_5m_2017.12.12'
    filenames = sys.argv[3] # '22005554_gxbvideo-gx.gaoxiaobang.com_201712121400_TX.access'
   # datestr = band_dates.split(',')
    queryjson ={"from":0,"size":0,"query":{"bool":{"must":[{"wildcard":{"file_name":filenames+'*'}},
                                                           {"match_all":{}}],"must_not":[],"should":[]}},
                "aggregations":{"file_name":{"terms":{"field":"file_name","size":1000},
                                             "aggregations":{"req_count":{"sum":{"field":"req_count"}}}}}}


    try:

         t = requests.get("http://esclient.bbdcdn.net:9200/"+band_names+"/_search", data=json.dumps(queryjson))
    except Exception as e:
         print e
    else:
    	files=t.json()['aggregations']['file_name']['buckets']
        if (len(files) == 0):
            print("no result ")
            print(queryjson)
        else:
            print("the result is ---------------------------")
            for i in range(0, len(files)):
                str1= json.dumps(files[i]['key'], encoding="UTF-8", separators=(',', ':'))
                str2=json.dumps(files[i]['req_count']['value'], encoding="UTF-8", separators=(',', ':'))
                print(str1+":"+str2)

    	#traffic=t.json()['aggregations']['traffic']['value']
    	#print "%d\t%d" %(req_count,traffic)

def check_bycond():
    start_datetime = sys.argv[2]#1513058700
    end_datetime =  sys.argv[3]  #1513058710
    start_datetime = long(sys.argv[2])  # 1512835200
    end_datetime = long(sys.argv[3])  # 1512921600
    firm_name = sys.argv[4]# "TX"
    domain = sys.argv[5]  #"gxbvideo-gx.gaoxiaobang.com"

    datestr = timestamp_datetime(end_datetime)

    queryjson ={"from":0,"size":0,"query":{"bool":{"must":[{"term":{"firm_name":firm_name}},{"term":{"domain":domain}},
                                                           {"range":{"from_time":{"gte":start_datetime,"lt":end_datetime}}}],"must_not":[],"should":[]}},
                "aggregations":{"file_name":{"terms":{"field":"file_name"},"aggregations":{"req_count":{"sum":{"field":"req_count"}}}}}}


    try:
         t = requests.get("http://esclient.bbdcdn.net:9200/statsdata_5m_"+datestr+"/_search", data=json.dumps(queryjson))
    except Exception as e:
         print e
    else:
    	files=t.json()['aggregations']['file_name']['buckets']
        if(len(files) == 0):
           print("no result ")
           print(datestr)
           print(queryjson)
        else:
            print("the result is ---------------------------")
            for i in range(0, len(files)):
                str1= json.dumps(files[i]['key'], encoding="UTF-8", separators=(',', ':'))
                str2=json.dumps(files[i]['req_count']['value'], encoding="UTF-8", separators=(',', ':'))
                print(str1+":"+str2)

 #通过对比厂商流量和日志流量，找出差异大的点（时间、厂商、域名），输入：时间范围
def getfrombandinfo():
    start_datetime =long(sys.argv[2]) # 1512835200
    end_datetime =  long(sys.argv[3])  # 1512921600
    firm_param= sys.argv[4] #"BS"
    datestr = timestamp_datetime(end_datetime, '%Y.%m')
    queryjson ={"query":{"bool":{"must":[{"term": {"firm_name": firm_param}},{"range":{"from_time":{"gte":start_datetime*1000,"lt":end_datetime*1000}}},{"range":{"mysql_traffic":{"gte":6000000000}}}],"must_not":[{"range":{"traff":{"gt":"-0.1","lt":"0.15"}}},
                                             {"terms":{"firm_name":["JSY","RX"]}},
                                             {"wildcard":{"domain":"*live*"}}],"should":[]}}, "from":0,"size":20,"sort":[],"aggs":{}}


    try:
         t = requests.get("http://esclient.bbdcdn.net:9200/aggresult_"+datestr+"/_search", data=json.dumps(queryjson))
    except Exception as e:
         print e
    else:
    	rows=t.json()['hits']['hits']
        if(len(rows)==0):
            print("no result ")
            print (queryjson)
        else :
            print("compare with vendor's traffic, some error data as follows: ")
            for line in rows:
                #print(line)
                str1= json.dumps(line["_source"]['firm_name'], encoding="UTF-8", separators=(',', ':'))
                str2=json.dumps(line["_source"]['domain'], encoding="UTF-8", separators=(',', ':'))
                fromtime=line["_source"]['from_time']
                str3 = timestamp_datetime(fromtime/1000, '%Y.%m.%d %H:%M:%S')
                print(str3+"->"+str1+":"+str2)


flag = sys.argv[1]#1513058700
if(flag.startswith("1")):# python checkFilesByBand.py "1" 1512835200 1512921600 "BS"
    getfrombandinfo()
elif(flag.startswith("2")): # python checkFilesByBand.py "2" 1513058700 1513058710 "TX" "gxbvideo-gx.gaoxiaobang.com"
    check_bycond()  #   python checkFilesByBand.py "3" "statsdata_5m_2017.12.12" "22005554_gxbvideo-gx.gaoxiaobang.com_201712121400_TX.access"
else:
    check_byfile()
```


## 查找缺失域名与厂商名
查找日志中缺失的域名与厂商名
```
sh /bbd/ssd01/flask/app/cronGetMissDomain.sh
```
cronGetMissDomain.sh
```
startDate=`date -d "-1 day" +%Y.%m.%d`

sourceIndex=statsdata_5m_$startDate
sourceType=bandwidth
sourceIndex1=aggresult_$startDate
sourceType1=aggresult
PYTHON_HOME=/bbd/ssd01/flask/flask
echo $sourceIndex
echo $sourceType
echo $sourceIndex1
echo $sourceType1
threadNum=`ps -ef|grep aggESToES.py|grep -v grep |wc -l`
echo $threadNum
if [ $threadNum -eq "0" ]
then
	echo "starting"
	$PYTHON_HOME/bin/python /bbd/ssd01/flask/app/getMissDomain.py $sourceIndex1 $sourceType1 $sourceIndex $sourceType
fi
```

# jenkins运维工具
## 从hbase导融合某个厂商的日志
sql="\"SELECT  '1227' as tag, cast(time_unix*1000 as long) as log_date,* FROM tempdata WHERE domain in ('gxmov.a.yximgs.com','gx.a.yximgs.com') and host like 'cmb-sd%'  and firm_name = 'JX'  \""
```
table_name="jx20171226" #hbase表名，自建表：jx20171219          融合表：nginx20171024
begin_time="201712261851"
end_time="201712261852"
parame_domain="183.3.251.16,1791,avatar.xueleyun.com"
#parame_host="-"

sql="\"SELECT  'test' as tag, cast(time_unix*1000 as long) as log_date,* FROM tempdata WHERE domain in ('avatar.xueleyun.com')  \""

su - hdfs -c "/bbd/sata09/bigdata/sinobbd/task/yunwei/hbaselog_to_es/aggbysql $table_name $begin_time $end_time $parame_domain $sql"
```
aggbysql
```
#spark-submit --class com.sinobbd.tercelsrear.urlrank.AggBySQL --conf spark.yarn.queue="es"  --master yarn --deploy-mode cluster --driver-memory 2g --num-executors 20 --executor-memory 10G --executor-cores 3 --total-executor-cores 61   --jars /bbd/sata09/bigdata/sinobbd/task/sparkonHbase/snakeyaml-1.15.jar,/bbd/sata09/bigdata/sinobbd/task/sparkonHbase/metrics-core-2.2.0.jar  /bbd/sata09/bigdata/sinobbd/task/sparkonHbase/fusionCDNStream.jar jx20170826 201708026530 201708261600 bbd.qiniu.sohu.vmoiver.com - "SELECT * FROM tempdata where domain= 'bbd.qiniu.sohu.vmoiver.com' LIMIT 50 "



#运维统计response_time平均值
#table_name="jx20171019"
#begin_time="201710201520"
#end_time="201710201530"
#parame_domain="gxmov.a.yximgs.com"
#parame_host="cmb-ha"
#sql="SELECT $begin_time as beging_time, avg(response_time) average FROM tempdata WHERE domain = '"$parame_domain"' and layer='4' and host like '"$parame_host%"'"

table_name=$1
begin_time=$2
end_time=$3
parame_domain=$4
#parame_host=$5
############sql="\\\"SELECT $begin_time as beging_time, avg(response_time) average FROM tempdata WHERE domain = '\\\"$parame_domain\\\"' and layer='4' and host like '\\\"$parame_host%\\\"'\\\""

#sql="SELECT avg(response_time) average FROM tempdata WHERE domain = '"$parame_domain"' and layer='4' and host like '"$parame_host%"'"
echo "sql========================================"
echo $5
echo "==========================================="
#exit 0


spark-submit \
  --class com.sinobbd.tercelsrear.urlrank.AggBySQL_new \
  --conf spark.yarn.queue="es"  \
  --master yarn \
  --deploy-mode cluster \
  --driver-memory 2g \
  --num-executors 20 \
  --executor-memory 10G \
  --executor-cores 3 \
  --total-executor-cores 61 \
  --jars /bbd/sata09/bigdata/sinobbd/task/sparkonHbase/snakeyaml-1.15.jar,/bbd/sata09/bigdata/sinobbd/task/sparkonHbase/metrics-core-2.2.0.jar /bbd/sata09/bigdata/sinobbd/task/sparkonHbase/fusionCDNStream.jar $table_name $begin_time $end_time $parame_domain "$5"
#select * from tempdata where domain='gxmov.a.yximgs.com' and layer='4' and host like 
#"SELECT * FROM tempdata where domain= 'gxmov.a.yximgs.com' LIMIT 50 "
# "SELECT avg(response_time) FROM tempdata where domain= 'gxmov.a.yximgs.com' and layer = 4 and host like cmb-ha% and time_unix > 1508397600 and time_unix < 1508398200 group by domain"

```


## Project 从hbase查询response_time平均值
参数：1：hbase tablename 2：RowKey Begin 3：RowKey end 4：Info valueFilters 5：Info valueFilters 6：SQL 
SQL中表名：tempdata 
可用列名： 
info列：clientip, response_time, time_local, domain, request,  http_status, hit_status,body_bytes_sent, 
info1列： firm_name, time_unix,prov, city, ISP, host, layer, req_count, server_addr, X_Info_request_id, file_name, line_num 
param4、param5如果为空，用-表示 

示例1：SELECT domain, request,sum(req_count) as count FROM tempdata where domain= 'gx.a.yximgs.com' and layer=1 group by domain, request ORDER BY count DESC LIMIT 50 
示例2：SELECT * FROM tempdata where domain= 'bbd.qiniu.sohu.vmoiver.com' LIMIT 50 
示例3： 
sql="\"SELECT cast(time_unix*1000 as long) as log_date,(body_bytes_sent/response_time) as downratio,* FROM tempdata WHERE domain = '\"$parame_domain\"' and layer='4' and host like '\"$parame_host%\"'   \""
```
table_name="jx20180222"
begin_time="201802221830"
end_time="201802222230"
parame_domain="gxmov.a.yximgs.com"
parame_host="cmb-hl"

sql="\"SELECT $begin_time as begingtime, avg(response_time) as average FROM tempdata WHERE domain = '\"$parame_domain\"' and layer='4' and host like '\"$parame_host%\"'\""

  su - hdfs -c "/bbd/sata09/bigdata/sinobbd/task/yunwei/response_time_avg/aggbysql $table_name $begin_time $end_time $parame_domain $parame_host $sql"
```
aggbysql
```
table_name=$1
begin_time=$2
end_time=$3
parame_domain=$4
#parame_host=$5
############sql="\\\"SELECT $begin_time as beging_time, avg(response_time) average FROM tempdata WHERE domain = '\\\"$parame_domain\\\"' and layer='4' and host like '\\\"$parame_host%\\\"'\\\""

#sql="SELECT avg(response_time) average FROM tempdata WHERE domain = '"$parame_domain"' and layer='4' and host like '"$parame_host%"'"
echo "sql========================================"
echo $5
echo "==========================================="
#exit 0


spark-submit \
  --class com.sinobbd.tercelsrear.urlrank.AggBySQL_new \
  --conf spark.yarn.queue="es"  \
  --master yarn \
  --deploy-mode cluster \
  --driver-memory 2g \
  --num-executors 20 \
  --executor-memory 10G \
  --executor-cores 3 \
  --total-executor-cores 61 \
  --jars /bbd/sata09/bigdata/sinobbd/task/sparkonHbase/snakeyaml-1.15.jar,/bbd/sata09/bigdata/sinobbd/task/sparkonHbase/metrics-core-2.2.0.jar /bbd/sata09/bigdata/sinobbd/task/sparkonHbase/fusionCDNStream.jar $table_name $begin_time $end_time $parame_domain "$5"
```


## 导出明细数据和平均下载时间
参数：1：hbase tablename 2：RowKey Begin 3：RowKey end 4：Info valueFilters 5：Info valueFilters 6：SQL 
SQL中表名：tempdata 
可用列名： 
info列：clientip, response_time, time_local, domain, request,  http_status, hit_status,body_bytes_sent, 
info1列： firm_name, time_unix,prov, city, ISP, host, layer, req_count, server_addr, X_Info_request_id, file_name, line_num 
param4、param5如果为空，用-表示 

示例1：SELECT domain, request,sum(req_count) as count FROM tempdata where domain= 'gx.a.yximgs.com' and layer=1 group by domain, request ORDER BY count DESC LIMIT 50 
示例2：SELECT * FROM tempdata where domain= 'bbd.qiniu.sohu.vmoiver.com' LIMIT 50 
```
table_name="jx20180205"
begin_time="201802050600"
end_time="201802050800"
parame_domain="gxmov.a.yximgs.com"
parame_host="cmb-ha-zz1-111-6"

sql="\"SELECT cast(time_unix*1000 as long) as log_date,(body_bytes_sent/response_time) as downratio,* FROM tempdata WHERE domain like '\"%$parame_domain\"' and layer='4' and host like '\"$parame_host%\"'   \""

su - hdfs -c "/bbd/sata09/bigdata/sinobbd/task/yunwei/response_time_avg/aggbysql $table_name $begin_time $end_time $parame_domain $sql"
```


## 快手http质量数据问题查询
检查快手http质量数据没有在kibana中查不到的原因
1、查看入es待处理条数（正常的快手5分钟发一次，每次近10000条，如果待处理条数较多说明有数据堆积，可能入库异常）
2、查看快手最后一条日志入es的时间（入库时间，判断快手多久没有发数据过来）
3、查看入es的快手日志最晚时间（日志时间，用于判断快手日志是否延迟很大，即发来的日志是很早以前的日志）
4、查看快手服务ip是否被防火墙拦截
执行该工具后，查看只进行结果：
INFO为正常信息，
ERROR为可能出现的问题
```
/bbd/ssd01/zz/shell/task/yunwei/kshttplog_problem_handler/kshttplog_problem_handler.sh
```
kshttplog_problem_handler.sh
```
#!/bin/bash

#快手质量数据异常检测

ks_ip1="114.118.4.94"
ks_ip2="123.59.169.35"
zookeeper="172.18.54.162:2181,172.18.54.163:2181,172.18.54.164:2181"
topic="kshttplog"               #快手日志入kafka主题
consumer_group="kshttpgroup"    #快手日志入es消费组
temp_json="/bbd/ssd01/zz/shell/task/yunwei/kshttplog_problem_handler/result.json"                   #查询es的临时数据


#快手入es的lag值，未消费的条数
ks_es_lag=`/bbd/ssd01/app/kafka/bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker \
	--group $consumer_group --topic $topic --zookeeper $zookeeper \
	| grep $consumer_group | awk '{ print $6 }' | awk '{sum+=$1}END{print sum}'`

function get_es_max_timestamp(){
	json_file=$1
	filed=$2
	year_month=`date '+%Y%m'` #es索引时间
	curl -XGET http://esclient.bbdcdn.net:9200/kshttplogpron-$year_month/fluentd/_search/ -d '{"size":0,"aggs":{"return_max_balance":{"max":{"field":"'$filed'"}}}}' > $json_file
	log_timestamp=`jq .aggregations.return_max_balance.value $json_file` #获取时间戳
	log_timestamp=`echo ${log_timestamp:0:10}` #10位
	# log_time=`date -d @$log_timestamp '+%Y-%m-%d %H:%M:%S'` #标准时间
	echo $log_timestamp
}

kslog_intoes_maxtime=`get_es_max_timestamp $temp_json "@timestamp"` #获取es中最后入的一条数据
kslog_maxtime=`get_es_max_timestamp $temp_json "date"` #获取es中快手日志中日志时间最晚的一条数据

ks_ip_iptable1=`ssh -p23432 172.18.54.166 "fgrep '114.118.4.94' /etc/sysconfig/iptables"`
ks_ip_iptable2=`ssh -p23432 172.18.54.166 "fgrep '123.59.169.35' /etc/sysconfig/iptables"`
ks_ip_iptable=$ks_ip_iptable1$ks_ip_iptable2
count=`echo $ks_ip_iptable | grep $ks_ip1 | grep $ks_ip2| wc -l`

echo "`date` [INFO] =============================================================快手问题查询结果如下："

if [[ $ks_es_lag -lt 50000 ]]; then
	echo "`date` [INFO] ======>快手日志入es处理正常，待处理条数：$ks_es_lag"
else
	echo "`date` [ERROR] ======>快手日志入库异常，入es待处理日志条数：$ks_es_lag"
fi

#入库时间\允许有25分钟延迟，否则异常
value=$((`date +%s`-1800))
if [[ $kslog_intoes_maxtime -lt $value ]]; then
	echo "`date` [ERROR] ======>快手日志延迟，快手日志最后一条日志入es时间（最大入库时间）：`date -d @$kslog_intoes_maxtime '+%Y-%m-%d %H:%M:%S'`"
else
	echo "`date` [INFO] ======>快手日志入库正常，最后一条日志入es时间（最大入库时间）：`date -d @$kslog_intoes_maxtime '+%Y-%m-%d %H:%M:%S'`"
fi

if [[ $kslog_maxtime -lt $value ]]; then
	echo "`date` [ERROR] ======>快手日志延迟，入es中的快手日志最晚时间（最大日志时间）：`date -d @$kslog_maxtime '+%Y-%m-%d %H:%M:%S'`"
else
	echo "`date` [INFO] ======>快手日志入库正常，入es中的快手日志最晚时间（最大日志时间）：`date -d @$kslog_maxtime '+%Y-%m-%d %H:%M:%S'`"
fi

if [[ $count -gt 0 ]]; then
	echo "`date` [INFO] ======>快手服务器在iptables的配置：$ks_ip_iptable"
else
	echo "`date` [ERROR] ======>快手服务器ip被防火墙拦截了，请修改防火墙设置(服务器:183.131.54.166)，快手发送数据服务器ip(2个)：$ks_ip1   $ks_ip2"
fi

message_into_kafka=`ssh -p23432 172.18.54.166 "fgrep [error] /var/log/td-agent/td-agent-kshttpkafka1.log | tail -n 1"`
message_into_es=`ssh -p23432 172.18.54.166 "fgrep [error] /var/log/td-agent/td-agent-kshttpes1.log | tail -n 1"`
echo "`date` ========> 快手日志入kafka、es，最后一条异常信息如下 (注：如果为空，则无异常; 如果异常信息时间距离当前较近，在最后入库时间左右，则有可能入库有问题)"
echo "`date` ========> 快手日志入kafka日志，最后一条异常信息：$message_into_kafka"
echo "`date` ========> 快手日志入es日志，最后一条异常信息：$message_into_kafka"

echo "`date` [INFO] =============================================================以上是快手日志排查结果(都是INFO为正常；有ERROR为异常）"
```


# jenkins定时任务
## elasticsearch数据备份
将elasticsearch 中的statsdata*索引数据进行备份至HDFS中（默认为按月进行备份不需参数，每月15号备份上月数据）如需指定备份其它索引，需要两个参数来指定索引前缀和索引所属的年月。
例如指定statsdata的二月份数据：sh /bbd/ssd01/app/zrk/snapshot_es.sh statsdata 2017.02
201801 已备份
0 0 15 * *
/bbd/ssd01/app/zrk/snapshot_es.sh
```
if [ $# != 2 ];then
        prefix=statsdata
        month=`date -d "-15 day" +%Y.%m`
else
        prefix=$1
        month=$2
fi
echo $prefix
echo $month     

#prefix=$1
#month=`date -d "-3 day" +%Y.%m`
curl -XPUT "http://183.136.128.47:9200/_snapshot/my_hdfs_repository5/statsdata$month" -d '{"indices":"statsdata_*'$month'*"}'
echo 'statsdata_*'$month'*  backup success' >> snapshot_es.log
#echo curl -XPUT "http://183.136.128.47:9200/_snapshot/my_hdfs_repository5/statsdata$month" -d \'{\"indices\":\"statsdata_*$month*\"}\' >zrr.sh
#echo 
#chmod +x zrr.sh
#sh zrr.sh
```

## es各节点进程监控
监控es各服务器上的es进程是否正常（监控的是进程的个数）
* * * * *
/bbd/ssd01/zz/shell/task/01_watch/esnode_process_watch/esnode_process_watch.sh
```
#!/bin/bash

#监控es节点进程是否正常

basepath="/bbd/ssd01/zz/shell/task/01_watch/esnode_process_watch"
log_basepath="$basepath/logs"
wx="/bbd/ssd01/zz/shell/wx.py"
now_month=`date +%Y%m`
last_month=`date -d "-2 month" +%Y%m` # 上一个月
temp_log_temp="$log_basepath/temp_log_$now_month.log" # 记录log信息，保存一个月
temp_lastmonthlog_temp="$log_basepath/temp_log_$last_month.log" # 拼接将要删除的文件名
# record_temp="$log_basepath/record"
now_time=`date '+%Y-%m-%d %H:%M:%S'`

# 日志临时记录文件处理
if [[ ! -d $log_basepath ]]; then # 文件夹判断是否存在
    mkdir -p $log_basepath
    touch $temp_log_temp
elif [[ ! -f $temp_log_temp ]]; then # 判断文件是否存在
    touch $temp_log_temp
fi
if [[ -f $temp_lastmonthlog_temp ]]; then # 删除上个月的记录
    rm -f $temp_lastmonthlog_temp
fi

######################################################################################
echo "[INFO] `date` es进程监控脚本开始执行----------------------------------------------------" >> $temp_log_temp
declare -A es_host_map
es_host_map=(["c3n-zj-nb1-183-136-128-47"]="1" ["c3n-zj-nb1-115-231-183-213"]="1" ["c3n-zj-nb1-115-231-183-214"]="1" ["c3n-zj-nb1-183-131-54-135"]="1" ["c3n-zj-nb1-183-131-54-152"]="1" ["c3n-zj-nb1-183-136-141-77"]="1" ["c3n-zj-nb1-183-136-141-79"]="1" ["c3n-zj-nb1-183-131-54-153"]="2" ["c3n-zj-nb1-183-131-54-154"]="2" ["c3n-zj-nb1-183-131-54-155"]="2" ["c3n-zj-nb1-183-131-54-156"]="2" ["c3n-zj-nb1-183-131-54-157"]="2" ["c3n-zj-nb1-183-131-54-158"]="2" ["c3n-zj-nb1-183-131-54-159"]="2" ["c3n-zj-nb1-183-131-54-160"]="2" ["c3n-zj-nb1-183-131-54-187"]="2" ["c3n-zj-nb1-183-131-54-188"]="2" ["c3n-zj-nb1-183-131-54-189"]="2" ["c3n-zj-nb1-183-136-128-58"]="2" ["c3n-zj-nb1-183-136-128-59"]="2" ["c3n-zj-nb1-183-136-128-60"]="2" ["c3n-zj-nb1-183-136-128-61"]="2" ["c3n-zj-nb1-183-136-128-62"]="2" ["c3n-zj-nb1-183-136-141-80"]="2" ["c3n-zj-nb1-183-136-141-81"]="2" ["c3n-zj-nb1-183-136-141-82"]="2" ["c3n-zj-nb1-183-136-141-83"]="2" ["c3n-zj-nb1-183-136-141-84"]="2" ["c3n-zj-nb1-183-136-141-85"]="2")
#es_host_map=(["c3n-zj-nb1-183-136-128-47"]="1" ["c3n-zj-nb1-183-136-141-85"]="2" ["c3n-zj-nb1-115-231-183-213"]="1" ["c3n-zj-nb1-115-231-183-214"]="1")
info_message="" #单台来讲正常了是否发送信息
error_message="" #单台来讲异常了是否发送信息
error_message_send="" #最终发送微信的异常消息

function ssh_option(){

    ssh_record_temp=$1
    ssh_host=$2
    if [[ ! -f $ssh_record_temp ]]; then
        touch $ssh_record_temp
    fi
    ssh_error_count=`cat $ssh_record_temp | grep ERROR | wc -l`
    if [[ $ssh_error_count -gt 0 ]]; then
        ssh_record_info="INFO"\\n"时间：$now_time"\\n"服务器128.47 到 ${key} 执行ssh命令 正常"
        echo "INFO 服务器172.18.128.47上执行ssh -p23432 $ssh_host 正常" > $ssh_record_temp
        python $wx "$ssh_record_info"
    fi
}

for key in ${!es_host_map[*]}; do
    # echo ${key}
    record_temp=$log_basepath/${key}
    if [[ ! -f $record_temp ]]; then
        touch $record_temp
    fi
    
    ssh_record_temp=$log_basepath/${key}"_ssh"

    es_process_count=`ssh -p23432 ${key} "ps -ef|grep Elasticsearch | grep -v grep | wc -l"` #该服务器es进程个数
    if [[ $? -eq 0 ]]; then

        ssh_option $ssh_record_temp ${key} #ssh结果 > 临时文件

        es_due_process_count=${es_host_map[${key}]} #该服务器应有进程个数
        error_count=`cat $record_temp | grep ERROR | wc -l` #上次是否出现异常
        if [[ $es_due_process_count == $es_process_count ]]; then #进程个数正常
            # info_message="[INFO]"\\n"时间：$now_time"\\n"服务器 ${key}"\\n"es进程正常，应有进程个 $es_due_process_count 个，现有进程 $es_process_count 个"
            if [[ $error_count -gt 0 ]]; then #本次正常，上次异常
                if [[ $info_message == "" ]]; then
                    info_message="服务器 ${key}"\\n"es进程正常，应有进程个 $es_due_process_count 个，现有进程 $es_process_count 个"
                else
                    info_message="$info_message"\\n"服务器 ${key}"\\n"es进程正常，应有进程个 $es_due_process_count 个，现有进程 $es_process_count 个"
                fi
                echo $info_message > $record_temp
                # python $wx "$info_message"
            fi
            echo $info_message >> $temp_log_temp
        else
            # error_message="ERROR"\\n"时间：$now_time"\\n"服务器 ${key}"\\n"es进程异常，应有进程个 $es_due_process_count 个，现有进程 $es_process_count 个"
            if [[ $error_count -lt 1 ]]; then #本次异常，上次正常
                if [[ $error_message == "" ]]; then
                    error_message="服务器 ${key}"\\n"es进程异常，应有进程个 $es_due_process_count 个，现有进程 $es_process_count 个"
                else
                    error_message="$error_message"\\n"服务器 ${key}"\\n"es进程异常，应有进程个 $es_due_process_count 个，现有进程 $es_process_count 个"
                fi
                echo "ERROR"$error_message > $record_temp
                # python $wx "$error_message"
            fi

            #最终发送微信报警的异常信息
            if [[ $error_message_send == "" ]]; then
                error_message_send="服务器 ${key}"\\n"es进程异常，应有进程个 $es_due_process_count 个，现有进程 $es_process_count 个"
            else
                error_message_send="$error_message_send"\\n"服务器 ${key}"\\n"es进程异常，应有进程个 $es_due_process_count 个，现有进程 $es_process_count 个"
            fi

            echo $error_message >> $temp_log_temp
        fi
    else
        #error_message_exit="ERROR"\\n"时间：$now_time"\\n"服务器 ${key}"\\n"服务器172.18.128.47上执行ssh -p23432 ${key} 失败"
        ssh_error_count=`cat $ssh_record_temp | grep ERROR | wc -l`
        if [[ $ssh_error_count -lt 1 ]]; then
            error_message_exit="ERROR"\\n"时间：$now_time"\\n"服务器 ${key} 宕机"
            echo "ERROR 服务器172.18.128.47上执行ssh -p23432 ${key} 异常" > $ssh_record_temp
            python $wx "$error_message_exit"
        fi
        
        echo $error_message_exit >> $temp_log_temp
    fi
done

if [[ $error_message_send == "" ]]; then #全部正常
    if [[ $info_message != "" ]]; then #判断是否有上次异常这次正常的消息
	python $wx "[INFO]"\\n"时间：$now_time"\\n"各服务器es进程都正常"
	echo "发送报警信息--> [INFO]"\\n"时间：$now_time"\\n"各服务器es进程都正常"
    fi
else #异常
    if [[ $error_message != "" || $info_message != "" ]]; then #判断是否有上次正常这次异常的消息  或者是  上次异常这次正常的
        python $wx "ERROR"\\n"时间：$now_time"\\n"$error_message_send"
	echo "发送报警信息--> ERROR"\\n"时间：$now_time"\\n"$error_message_send"
    fi
fi

echo "[INFO] `date` es进程监控脚本执行结束=========================================================" >> $temp_log_temp

```


## flume、kafka监控
监控各服务器上的flume、kafka进程是否异常
*/5 * * * *
/bbd/ssd01/zz/shell/task/01_watch/server_watch_main.sh flume
/bbd/ssd01/zz/shell/task/01_watch/server_watch_main.sh kafka
server_watch_main.sh
```
#!/bin/bash
#汇总集群监控信息，处理并发送报警信息
group=$1 #监控的组（flume or kafka）
base_path="/bbd/ssd01/zz/shell"
wx_path="$base_path/wx.py" #微信公众号发送信息脚本
flume_ip="$base_path/conf/$group.ip"	#flume服务器ip列表
log_file="$base_path/task/01_watch/logs/$group""_watch.log" #记录上次监控的值（只保存异常信息）
log_file_temp="$base_path/task/01_watch/logs/temp_`date '+%Y%m'`.log"
shell_script="$base_path/task/01_watch/"$group"_watch.sh" #flume服务器上的监控脚本
now_time=`date '+%Y-%m-%d %H:%M:%S'`
message="" #存放所有的flume服务器上flume监控脚本执行结果（返回的异常信息）

if [[ $# -ne 1 ]]; then
	echo "`date` [INFO] "$group"_watch_main: 参数缺少服务监控的组" >> $log_file_temp
	exit 1
fi
if [[ ! -f $flume_ip ]]; then
	echo "`date` [INFO] "$group"_watch_main: 文件$flume_ip不存在..." >> $log_file_temp
	exit 1
fi
if [[ ! -f $log_file ]]; then
	touch $log_file
fi

#调用flume个服务器上的脚本，获取监控结果
function server_watch_main(){
	for i in `cat $flume_ip`;do
		temp=`ssh -p23432 $i $shell_script` #循环调用flume各服务器上的监控脚本
		if [[ $temp != "OK" ]]; then #异常
			if [[ $message ]]; then
				message="$message@###@$temp"
			else
				message=$temp
			fi
		fi
	done
	echo $message
}

#字符转数组
function str_to_arr(){
	str=$1 #字符
	sep=$2 #分隔符
	
	OLD_IFS="$IFS"
	IFS="$sep"
	arr=($str)
	IFS="$OLD_IFS"

	echo "${arr[@]}"
	#echo "("${arr[@]}")"
	#for s in ${arr[@]};do
	#	echo "$s" 
	#done
}

#获取汇总异常信息中所有的主机名
function host_in_message(){
	arr=$1
	hosts=""
	for host_message in ${arr[@]}; do
		temp=${host_message%%,*} #截取主机名
		if [[ $hosts ]]; then
			hosts=$hosts"\\n"$temp
		else
			hosts=$temp
		fi
	done
	echo $hosts
}

now_message=`server_watch_main`	#本次汇总的监控值
last_message=`cat $log_file`	#上次汇总的监控值
send_message=""					#发送微信报警信息

#异常消息处理，判断是否发送信息到微信报警
function message_handler(){
	if [[ $now_message ]]; then #本次ERROR
		now_message_arr=`str_to_arr $now_message "@##@"` #各服务器信息放到数组里
		now_message_hosts=`host_in_message "${now_message_arr[@]}"` #异常主机名列表
		echo $now_message > $log_file
		echo "`date` [ERROR] "$group"_watch_main: $now_message" >> $log_file_temp

		if [[ $last_message ]]; then #上次ERROR本次ERROR
			if [[ $now_message != $last_message ]]; then
				send_message="[ERROR]\\n时间: $now_time\\n"$group"进程异常,异常服务器主机名:\\n$now_message_hosts"
			fi
		else #上次OK本次ERROR
			send_message="[ERROR]\\n时间: $now_time\\n"$group"进程异常,异常服务器主机名:\\n$now_message_hosts"
		fi
	else #本次OK
		if [[ $last_message ]]; then #上次ERROR本次OK
			send_message="[INFO]\\n时间: $now_time\\n各服务器"$group"进程正常"
		fi
		> $log_file
	fi
}

message_handler
if [[ $send_message ]]; then
	python $wx_path "$send_message" #发送微信消息报警
	echo "`date` [INFO] "$group"_watch_main: 发送了报警消息: $send_message" >> $log_file_temp
else
	echo "`date` [INFO] "$group"_watch_main: "$group"正常或和上次状态一样, 没有发送报警信息" >> $log_file_temp
fi
```
flume_watch.sh
```
#!/bin/bash
# 监控flume日志中的关键字，有关键的返回异常信息
local_hostname=`hostname`
flume_log_path="/bbd/ssd01/app/flume/flumelogs"		#flume的日志目录
flume_jx_kafka="$flume_log_path/flume_jx_kafka*"	#jx日志入kafka
flumeJX_hbase="$flume_log_path/flumeJX_hbase*"		#jx日志入hbase
flume_kafka="$flume_log_path/flume_kafka*"			#rh日志入kafka

function flume_watch(){
	error_count=`fgrep Restart $flume_jx_kafka $flumeJX_hbase $flume_kafka | wc -l`
	if [[ $error_count -gt 0 ]]; then	#有异常
		echo "$local_hostname,ERROR" #返回主机名和ERROR（中间用分隔符隔开）
	else
		echo "OK"
	fi
}

message=`flume_watch`	#调用函数
echo $message
```
kafka-watch.sh
```
# 162 - 166 kafka进程、端口监控
local_hostname=`hostname`

function kafka_watch(){
    kafka_proc_count=`netstat -ntulp | grep 9092 | wc -l`
    if [[ $kafka_proc_count -lt 1 ]]; then   #有异常
        echo "$local_hostname,ERROR" #返回主机名和ERROR（中间用分隔符隔开）
    else
        echo "OK"
    fi
}

message=`kafka_watch`   #调用函数
echo $message
```


## jenkins报活
每分钟向128.47 打印一条日志报活
```
* * * * *
echo "时间:`date '+%Y-%m-%d %H:%M:%S'` , timeUnix:`date +%s`" > /bbd/ssd01/zz/shell/task/01_watch/jenkins_machine_watch/logs/record_time
```
128.47 crontab
```
* * * * * sh /bbd/ssd01/zz/shell/task/01_watch/jenkins_machine_watch/jenkins_watch.sh  > /dev/null 2>&1
```
jenkins_watch.sh
```
#!/bin/bash

#根据jenkins报活信息，判断jenkins是否工作正常；并发送日志到183.213服务器报活

basepath="/bbd/ssd01/zz/shell/task/01_watch/jenkins_machine_watch"
log_basepath="$basepath/logs"
wx="/bbd/ssd01/zz/shell/wx.py"
now_month=`date +%Y%m`
last_month=`date -d "-2 month" +%Y%m` # 上一个月
temp_log_temp="$log_basepath/temp_log_$now_month.log" # 记录log信息，保存一个月
temp_lastmonthlog_temp="$log_basepath/temp_log_$last_month.log" # 拼接将要删除的文件名
record_time="$log_basepath/record_time"
record_temp="$log_basepath/record_temp"
now_time=`date '+%Y-%m-%d %H:%M:%S'`

# 日志临时记录文件处理
if [[ ! -d $log_basepath ]]; then # 文件夹判断是否存在
    mkdir -p $log_basepath
    touch $temp_log_temp
elif [[ ! -f $temp_log_temp ]]; then # 判断文件是否存在
    touch $temp_log_temp
fi
if [[ -f $temp_lastmonthlog_temp ]]; then # 删除上个月的记录
    rm -f $temp_lastmonthlog_temp
fi

if [[ ! -f $record_time ]]; then
    echo "timeUnix:0" > $record_time
fi
if [[ ! -f $record_temp ]]; then
    echo "" > $record_temp
fi
######################################################################

#向183.213报活
ssh -p 23432 172.18.183.213 "echo "时间:`date '+%Y-%m-%d %H:%M:%S'` , timeUnix:`date +%s`" > $record_time"

#检查jenkins报活信息（检测jenkins任务执行是否正常）
#echo "时间:`date '+%Y-%m-%d %H:%M:%S'` , timeUnix:`date +%s`"
time_message=`tail -n 1 $record_time | awk -F 'timeUnix:' '{print $2}'`
now_time_m=`date +%s`
count=$(($now_time_m - $time_message))
error_count=`cat $record_temp | grep ERROR | wc -l`
if [[ $count -gt 120 ]]; then
    error_message="[ERROR]"\\n"时间：$now_time"\\n"jenkins报活异常，上次报活时间：`tail -n 1 $record_time`"
    if [[ $error_count -lt 1 ]]; then
        echo $error_message > $record_temp
        python $wx "$error_message"
    fi
    echo "$error_message" >> $temp_log_temp
else
    info_message="[INFO]"\\n"时间：$now_time"\\n"jenkins报活正常，上次报活时间：`tail -n 1 $record_time`"
    if [[ $error_count -gt 0 ]]; then #这次正常，上次异常
        echo $info_message > $record_temp
        python $wx "$info_message"
    fi
    echo $info_message >> $temp_log_temp
fi
```
115-231-183-213服务器
```
* * * * * sh /bbd/ssd01/zz/shell/task/01_watch/jenkins_machine_watch/128-47_watch.sh  > /dev/null 2>&1
```
128-47_watch.sh
```
#!/bin/bash

#根据jenkins报活信息，判断jenkins是否工作正常；并发送日志到183.213服务器报活

basepath="/bbd/ssd01/zz/shell/task/01_watch/jenkins_machine_watch"
log_basepath="$basepath/logs"
wx="/bbd/ssd01/zz/shell/wx.py"
now_month=`date +%Y%m`
last_month=`date -d "-2 month" +%Y%m` # 上一个月
temp_log_temp="$log_basepath/temp_log_$now_month.log" # 记录log信息，保存一个月
temp_lastmonthlog_temp="$log_basepath/temp_log_$last_month.log" # 拼接将要删除的文件名
record_time="$log_basepath/record_time"
record_temp="$log_basepath/record_temp"
now_time=`date '+%Y-%m-%d %H:%M:%S'`

# 日志临时记录文件处理
if [[ ! -d $log_basepath ]]; then # 文件夹判断是否存在
    mkdir -p $log_basepath
    touch $temp_log_temp
elif [[ ! -f $temp_log_temp ]]; then # 判断文件是否存在
    touch $temp_log_temp
fi
if [[ -f $temp_lastmonthlog_temp ]]; then # 删除上个月的记录
    rm -f $temp_lastmonthlog_temp
fi

if [[ ! -f $record_time ]]; then
    echo "timeUnix:0" > $record_time
fi
if [[ ! -f $record_temp ]]; then
    echo "" > $record_temp
fi
######################################################################

#向183.213报活
# ssh -p 23432 172.18.183.213 "echo "时间:`date '+%Y-%m-%d %H:%M:%S'` , timeUnix:`date +%s`" >> $record_time"

#检查47服务器报活信息（检测47服务器任务执行是否正常）
#echo "时间:`date '+%Y-%m-%d %H:%M:%S'` , timeUnix:`date +%s`"
time_message=`tail -n 1 $record_time | awk -F 'timeUnix:' '{print $2}'`
now_time_m=`date +%s`
count=$(($now_time_m - $time_message))
error_count=`cat $record_temp | grep ERROR | wc -l`
if [[ $count -gt 120 ]]; then
    error_message="[ERROR]"\\n"时间：$now_time"\\n"服务器 183.136.128.47 报活异常，上次报活时间：`tail -n 1 $record_time`"
    if [[ $error_count -lt 1 ]]; then
        echo $error_message > $record_temp
        python $wx "$error_message"
    fi
    echo "$error_message" >> $temp_log_temp
else
    info_message="[INFO]"\\n"时间：$now_time"\\n"服务器 183.136.128.47 报活正常，上次报活时间：`tail -n 1 $record_time`"
    if [[ $error_count -gt 0 ]]; then #这次正常，上次异常
        echo $info_message > $record_temp
        python $wx "$info_message"
    fi
    echo $info_message >> $temp_log_temp
fi
```


## qiniu_customer打包日志备份
136 日志备份至白山云
```
30 9 * * *
ssh -p23432 172.18.54.136 "nohup /bbd/ssd/zrk/qiniu_customer_logs_upload.sh > /dev/null 2>&1 &"
```
qiniu_customer_logs_upload.sh
```
predate=`date -d "-7 day" +%Y%m%d`
nohup trickle -s -u 1000000 python /bbd/ssd/zrk/qiniu_customer_logs_upload.py /nfs/data/customer_logs/$predate &
```
qiniu_customer_logs_upload.py
```
# -*- coding: utf-8 -*-
import os
import sys
import re
from qiniu import Auth, put_file, etag, urlsafe_base64_encode
import qiniu.config
access_key = 'p11bD15Gr0dcUNYpzvlxCkyCzsKvjHT8HBzJKy_X'
secret_key = 'XxOyUFvavavnHtzS28bcNghMpxkIkipm_ZXTTtED'

q = Auth(access_key, secret_key)
bucket_name = 'bigdata'

g = os.walk(sys.argv[1])
for path,dir_list,file_list in g:
    for file_name in file_list:
        full_name=os.path.join(path, file_name)
        list2 = full_name.split("/")
        full_date2 = list2[4]
        month1 = full_date2[:6]
        date1 = full_date2[6:]
        tmp = list2[5]
        domain1 = list2[6]

        key1='customer_logs_backup/'+month1+'/'+date1+'/'+tmp+'/'+domain1+'/'+file_name
        token = q.upload_token(bucket_name, key1, 3600)
        ret, info = put_file(token, key1, full_name)
```

## qiniu_firm_logs日志备份1
162-165firm_logs 日志备份，每天10点跑
```
0 11 * * *
ssh -p23432 172.18.54.162 "nohup /bbd/ssd01/app/zrk/qiniu_firm_logs_upload.sh > /dev/null 2>&1 &"
ssh -p23432 172.18.54.163 "nohup /bbd/ssd01/app/zrk/qiniu_firm_logs_upload.sh > /dev/null 2>&1 &"
ssh -p23432 172.18.54.164 "nohup /bbd/ssd01/app/zrk/qiniu_firm_logs_upload.sh > /dev/null 2>&1 &"
ssh -p23432 172.18.54.165 "nohup /bbd/ssd01/app/zrk/qiniu_firm_logs_upload.sh > /dev/null 2>&1 &"
```
qiniu_firm_logs_upload.sh
```
for i in {01..12}
do
        predate=`date -d "-20 day" +%Y%m%d`
        trickle -s -u 1000000 python /bbd/ssd01/app/zrk/qiniu_firm_logs_upload.py /bbd/sata$i/down_logs $predate 06
        find /bbd/sata$i/down_logs  -name "*.gz" -atime +25 |xargs rm -f
done
```
qiniu_firm_logs_upload.py 
```
# -*- coding: utf-8 -*-
import os
import sys
import re
from qiniu import Auth, put_file, etag, urlsafe_base64_encode
import qiniu.config
access_key = 'p11bD15Gr0dcUNYpzvlxCkyCzsKvjHT8HBzJKy_X'
secret_key = 'XxOyUFvavavnHtzS28bcNghMpxkIkipm_ZXTTtED'

q = Auth(access_key, secret_key)
bucket_name = 'bigdata'

option_date=sys.argv[2]
match_id=sys.argv[3]
g = os.walk(sys.argv[1])
for path,dir_list,file_list in g:
    for file_name in file_list:
        full_name=os.path.join(path, file_name)

      #  list5 = file_name.split("_")


        mat = re.search(r"(\d{8})", file_name)
        if mat is None:
            print(file_name,path)
            continue
        full_date5=mat.group(0)
        month5 = full_date5[:6]
        date5=full_date5[6:8]
        full_list=full_name.split("/")
        domain5 = full_list[4]
        disk5=full_list[2][-2:]
        firm_id=full_list[5]
        key1='firm_logs_backup/'+month5+'/'+date5+'/'+match_id+'/'+disk5+'/'+domain5+'/'+firm_id + '/'+file_name
        token = q.upload_token(bucket_name, key1, 3600)
        if option_date==full_date5:
            ret, info = put_file(token, key1, full_name)
```


## spark入es监控
自建、融合 日志从kafka入es监控（spark），5分钟跑一次
```
*/5 * * * *
#自建spark入es
ssh -p23432 172.18.54.147 "/bbd/ssd01/zz/shell/jxLogToEs/spark-jxLogToEs-watch.sh"
#融合spark入es
ssh -p23432 172.18.54.142 "nohup /bbd/ssd01/zrk/task/spark-es-watch.sh > /dev/null 2>&1 &"
#自建CDN spark带宽计算
ssh -p23432 172.18.54.145 " nohup /bbd/sata09/bigdata/sinobbd/task/weixin/runjxbandstatis.sh > /dev/null 2>&1 &"
#融合CDN spark 带宽计算
ssh -p23432 172.18.54.145 " nohup /bbd/sata09/bigdata/sinobbd/task/weixin/fusionstatis.sh > /dev/null 2>&1 &"
```
spark-jxLogToEs-watch.sh
```
#!/bin/bash
# 监控jx日志spark入es程序是否异常，异常重启

local_hostname=`hostname`
log_basepath="/bbd/ssd01/zz/shell/jxLogToEs/logs" # 日志根目录
wx_basepath="/bbd/ssd01/zz/shell"
wx="/bbd/ssd01/zz/shell/wx.py"
now_month=`date +%Y%m`
last_month=`date -d "-31 day" +%Y%m` # 上一个月
temp_log_temp="$log_basepath/temp_log_$now_month.log" # 记录log信息，保存一个月
temp_lastmonthlog_temp="$log_basepath/temp_log_$last_month.log" # 拼接将要删除的文件名
temp_log_file="$log_basepath/spark-jxLogToEs-watch.log" #记录单条日志的文件
now_time=`date '+%Y-%m-%d %H:%M:%S'`

# 日志临时记录文件处理
if [[ ! -d $log_basepath ]]; then # 文件夹判断是否存在
    mkdir -p $log_basepath
    touch $temp_log_file
elif [[ ! -f $temp_log_file ]]; then # 判断文件是否存在
    touch $temp_log_file
fi
if [[ -f $temp_lastmonthlog_temp ]]; then # 删除上个月的记录
	rm -f $temp_lastmonthlog_temp
fi

error_count=`ps -ef|grep "com.sinobbd.jx.JXLogToEs" | grep -v grep | wc -l`
error_count_temp=`fgrep ERROR $temp_log_file | wc -l`

if [[ $error_count -lt 1 ]]; then # 有异常
    error_message="[ERROR] 时间:$now_time 主机:$local_hostname jx日志spark入库es有异常,进程个数:$error_count"
    if [[ $error_count_temp -lt 1 ]]; then # 有异常，而且上次定时任务记录的log是正常的（上次就是异常不再重复发送）
        python $wx "$error_message" # 调用python脚本发送信息
    fi
    /bbd/ssd01/zz/shell/jxLogToEs/start_nohup.sh
#	if [[ $? -eq 0 ]];then
#		python $wx_basepath/zz.py "spark-jxLogToEs 重启成功... `date`"
#	fi
    echo `date`" [ERROR] $error_message" > $temp_log_file # 记录单条的文件
    echo `date`" [ERROR] $error_message" >> $temp_log_temp # 保存一个月的log信息

else
    ok_message="[INFO] 时间:$now_time  主机:$local_hostname jx日志spark入库es正常,进程个数:$error_count"
    warn_message="[WARN] 时间:$now_time  主机:$local_hostname jx日志spark入库es,进程个数: $error_count"
    if [[ $error_count_temp -gt 0 ]]; then # 监控正常，但是定时执行记录的log是异常信息，发送信息（有异常转入正常）
	if [[ $error_count -gt 1 ]]; then
		
		python $wx "$warn_message"
		echo "$warn_message" >> $temp_log_temp
	else
		python $wx "$ok_message"
	fi
    fi
	if [[ $error_count -gt 1 ]]; then
		python $wx "$warn_message"
		echo "$warn_message"  >> $temp_log_temp
	fi

    echo `date`" [OK] $ok_message" > $temp_log_file
    echo `date`" [OK] $ok_message">>$temp_log_temp
fi

```


## 快手post数据监控
快手post过来的质量数据监控
9,19,29,39,49,59 * * * *
54.162
/bbd/sata01/shell/task/ks_http_kafka-watch.sh
```
#!/bin/bash
# 快手post请求数据在kafka中的消费监控
# 3,8,13,18,23,28,33,38,43,48,53,58 * * * * sh /bbd/sata01/shell/task/ks_http_kafka-watch.sh > /dev/null 2>&1

# local_hostname=`hostname`
log_basepath="/bbd/sata01/shell/task/log_task"  # 日志根目录
#wx_basepath="/opt/rh"
#wx_basepath="/bbd/sata01/shell/task/zz2.py"
wx="/bbd/sata01/shell/task/zz2.py"
now_month=`date +%Y%m`
last_month=`date -d "-1 month" +%Y%m` # 上一个月
temp_log_temp="$log_basepath/temp_log_$now_month.log" # 记录log信息，保存一个月
temp_lastmonthlog_temp="$log_basepath/temp_log_$last_month.log" # 拼接将要删除的文件名
temp_log_file="$log_basepath/kshttp-kafka-watch.log" # 记录log信息，记录一条，判断是否要发送log信息（异常、正常信息）
now_time=`date '+%Y-%m-%d %H:%M:%S'`

# 日志临时记录文件处理
if [[ ! -d $log_basepath ]]; then # 文件夹判断是否存在
    mkdir -p $log_basepath
    touch $temp_log_file
elif [[ ! -f $temp_log_file ]]; then # 判断文件是否存在
    touch $temp_log_file
fi
if [[ -f $temp_lastmonthlog_temp ]]; then # 删除上个月的记录
    rm -f $temp_lastmonthlog_temp
fi

error_count_temp=`fgrep ERROR $temp_log_file | wc -l` # 临时日志中的异常个数

ks_http_topic=kshttplog # 快手http post请求主题
ks_http_es_group=kshttpgroup #从kafka消费ks post数据到es的组
ks_http_tx_group=kshttpgroup-tx #从kafka消费ks post数据post给tx的组
ks_http_yf_group=kshttpgroup-yf #从kafka消费ks post数据post给云帆的组
ks_http_yftx_group=kshttpgroup-yftx #从kafka消费ks post数据post给云帆和腾讯的组

kafka_shfile_path="/bbd/ssd01/app/kafka/bin"
zookeeper="183.131.54.162:2181,183.131.54.163:2181,183.131.54.164:2181"

# 环境变量
export JAVA_HOME=/bbd/ssd01/app/jdk
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export ZOOKEEPER_HOME=/usr/local/zookeeper
export PATH=:$PATH:$JAVA_HOME/bin:$ZOOKEEPER_HOME/bin

############################## ks http post 主题 kshttpnew 总数统计 ###########
ks_logsize=`$kafka_shfile_path/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker \
    --group $ks_http_es_group --topic $ks_http_topic --zookeeper $zookeeper \
    | grep $ks_http_es_group | awk '{ print $5 }' | awk '{sum+=$1}END{print sum}'`

############################## ks-http-post-es lag统计 #########################
ks_es_lag=`$kafka_shfile_path/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker \
   --group $ks_http_es_group --topic $ks_http_topic --zookeeper $zookeeper \
   | grep $ks_http_es_group | awk '{ print $6 }' | awk '{sum+=$1}END{print sum}'`

############################## ks-http-post-yftx lag统计 #########################
ks_yftx_lag=`$kafka_shfile_path/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker \
   --group $ks_http_yftx_group --topic $ks_http_topic --zookeeper $zookeeper \
   | grep $ks_http_yftx_group | awk '{ print $6 }' | awk '{sum+=$1}END{print sum}'`

############################## ks-http-post-tx lag统计 #########################
#ks_tx_lag=`$kafka_shfile_path/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker \
#   --group $ks_http_tx_group --topic $ks_http_topic --zookeeper $zookeeper \
#   | grep $ks_http_tx_group | awk '{ print $6 }' | awk '{sum+=$1}END{print sum}'`

############################## ks-http-post-tx lag统计 #########################
#ks_yf_lag=`$kafka_shfile_path/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker \
#   --group $ks_http_yf_group --topic $ks_http_topic --zookeeper $zookeeper \
#   | grep $ks_http_yf_group | awk '{ print $6 }' | awk '{sum+=$1}END{print sum}'`

ks_http_message=`cat $temp_log_file`
ok_or_error=`echo $ks_http_message | awk '{print $1}'`
ks_logsize_increase=`echo $ks_http_message | awk '{print $2}'`
ks_logsize_increase=`expr $ks_logsize - $ks_logsize_increase` # ks post 增量值

#if [[ $ks_logsize_increase -lt 100 ]] || [[ $ks_es_lag -gt 20000 ]] || [[ $ks_yf_lag -gt 20000 ]] || [[ $ks_tx_lag -gt 200000 ]]; then
if [[ $ks_logsize_increase -lt 100 ]] || [[ $ks_es_lag -gt 20000 ]] || [[ $ks_yftx_lag -gt 20000 ]]; then
    #error_message="ERROR ks http es/tx: ks_logsize/ks_logsize_increase/ks_es_lag/ks_tx_lag: $ks_logsize $ks_logsize_increase $ks_es_lag $ks_tx_lag"
    #error_message="[WARN] 时间:$now_time 快手post的质量数据处理警告: 快手post过来的条数:$ks_logsize_increase 待处理条数[入es:$ks_es_lag post给腾讯:$ks_tx_lag]" #post过来的数据入es未处理条数:$ks_es_lag post给腾讯未处理条数:$ks_tx_lag" #/post腾讯的lag值: $ks_logsize $ks_logsize_increase $ks_es_lag $ks_tx_lag
#	error_message="[WARN] 时间:$now_time 快手post的质量数据警告: 快手post过来的条数:$ks_logsize_increase 待处理条数[入es:$ks_es_lag post给云帆:$ks_yf_lag post给腾讯:$ks_tx_lag ]" #post过来的数据入es未处理条数:$ks_es_lag post给腾讯未处理条数:$ks_tx_lag" #/post腾讯的lag值: $ks_logsize $ks_logsize_increase $ks_es_lag $ks_tx_lag
	error_message="[WARN] 时间:$now_time 快手post的质量数据警告: 快手post过来的条数:$ks_logsize_increase 待处理条数[入es:$ks_es_lag post给云帆和腾讯:$ks_yftx_lag ]" #post过来的数据入es未处理条数:$ks_es_lag post给腾讯未处理条数:$ks_tx_lag" #/post腾讯的lag值: $ks_logsize $ks_logsize_increase $ks_es_lag $ks_tx_lag
	echo 1 > /etc/zabbix/scripts/log.txt #zabbix报警及监控用
	echo $error_message >> /etc/zabbix/scripts/log_temp.txt
	if [[ $ok_or_error != "ERROR" ]]; then
		python $wx "$error_message" # 调用python脚本发送信息
		echo `date` "发送异常信息"$error_message >> $temp_log_temp
	fi
	echo "ERROR $ks_logsize $ks_logsize_increase $ks_es_lag $ks_tx_lag" > $temp_log_file # 记录单条的文件,用来切分取值
	echo "ERROR $ks_logsize $ks_logsize_increase $ks_es_lag $ks_tx_lag" >> $temp_log_temp # 记录单条的文件,用来切分取值
	echo `date`" [ERROR] $error_message" >> $temp_log_temp # 保存一个月的log信息
else
	#ok_message="OK 快手质量数据post腾讯: 快手质量数据log值/增量值/入es的lag值/post腾讯的lag值: $ks_logsize $ks_logsize_increase $ks_es_lag $ks_tx_lag"
	#ok_message="[INFO] 时间:$now_time 快手post的质量数据处理正常: 快手post过来的条数:$ks_logsize_increase 待处理条数[入es:$ks_es_lag post给腾讯:$ks_tx_lag]" #post过来的数据入es未处理条数:$ks_es_lag post给腾讯未处理条数:$ks_tx_lag
	ok_message="[INFO] 时间:$now_time 快手post的质量数据处理正常: 快手post过来的条数:$ks_logsize_increase 待处理条数[入es:$ks_es_lag post给云帆和腾讯:$ks_yftx_lag ]" #post过来的数据入es未处理条数:$ks_es_lag post给腾讯未处理条数:$ks_tx_lag
	echo 0 > /etc/zabbix/scripts/log.txt
	echo $ok_message >> /etc/zabbix/scripts/log_temp.txt
 	if [[ $ok_or_error == "ERROR" ]]; then
		python $wx "$ok_message"
		echo `date` "发送正常信息"$ok_message >> $temp_log_temp
	fi
	echo "OK $ks_logsize $ks_logsize_increase $ks_es_lag $ks_tx_lag" > $temp_log_file
	echo "OK $ks_logsize $ks_logsize_increase $ks_es_lag $ks_tx_lag" >> $temp_log_temp
	echo `date`" [OK] $ok_message" >> $temp_log_temp
fi
```

## 快手质量数据日志（fiile）下载
下载快手file格式的质量数据日志文件 
快手提供日志时间范围：12、13、21、22 
脚本中取前3个小时作为日志时间，所以定时的时候前后多加了几个小时15 - 13,00 - 02
10 15,16,00,01 * * *
128.49
/bbd/ssd01/zz/shell/task/03_kuaishou_filelog/start_downlog.sh
```
#!/bin/bash
#定时执行下载快手质量数据日志（快手提供12、13、21、22）
#晚4个小时执行一个小时的（16、17、01、02执行）

LOGTIME_DAY=`date -d '-3 hour' '+%Y-%m-%d'`
LOGTIME_HOUR=`date -d '-3 hour' '+%H'`
#echo "$LOGTIME_DAY-$LOGTIME_HOUR-$i" "$LOGTIME_DAY-$LOGTIME_HOUR" "$LOGTIME_HOUR"
for i in {00..59}; do
        echo "$LOGTIME_DAY-$LOGTIME_HOUR-$i" "$LOGTIME_DAY-$LOGTIME_HOUR" "$LOGTIME_HOUR"
        /bbd/ssd01/zz/shell/task/03_kuaishou_filelog/downlog.sh "$LOGTIME_DAY-$LOGTIME_HOUR-$i" "$LOGTIME_DAY-$LOGTIME_HOUR" "$LOGTIME_HOUR"
done
```
downlog.sh
```
#!/bin/bash
#下载快手file格式的质量数据日志文件
#   快手提供日志时间范围：12、13、21、22
#LOGTIME=`date -d '-3 hour' '+%Y-%m-%d-%H-%M'`
#LOGTIME_DAY=`date -d '-3 hour' '+%Y-%m-%d'`
#LOGTIME_HOUR=`date -d '-3 hour' '+%H'`

LOGTIME=$1
LOGTIME_DAY=$2
LOGTIME_HOUR=$3

domain=all.gxmov.a.yximgs.com
log_basepath="/bbd/ssd01/zz/shell/task/03_kuaishou_filelog/logs"
shell_log="$log_basepath/recording_num.log" #记录盘符
record_log="$log_basepath/recording_ksfilesplit.log" #记录日志
wx="/bbd/ssd01/zz/shell/zz.py"
now_time=`date '+%Y-%m-%d %H:%M:%S'`

if [[ $LOGTIME_HOUR == 12 || $LOGTIME_HOUR == 13 || $LOGTIME_HOUR == 21 || $LOGTIME_HOUR == 22 ]]; then
    echo "OK,`date` $LOGTIME"
else
    echo "执行时间: $now_time 日志时间: $LOGTIME 退出" >> $record_log
    exit 0
fi

# 快手接口修改：一分钟一个文件
if [[ ! -f $shell_log ]]; then
       echo "01" > $shell_log
fi
num=`tail -n 1 $shell_log`

#到相应的目录下下载解压文件
cd /bbd/sata$num/ksfile_logs_handler/ksfile_handle_temp
LOGTIME_FILE="$log_basepath/wgetlog/$LOGTIME_DAY/$LOGTIME_HOUR/$LOGTIME.log"
if [ ! -d "$log_basepath/wgetlog/$LOGTIME_DAY/$LOGTIME_HOUR" ]; then
    mkdir -p "$log_basepath/wgetlog/$LOGTIME_DAY/$LOGTIME_HOUR"
fi
wget -S http://180.186.38.176/data/gxfwtTTKaXMBR/$domain.$LOGTIME.log.gz -o $LOGTIME_FILE
echo "http://180.186.38.176/data/gxfwtTTKaXMBR/$domain.$LOGTIME.log.gz -o $LOGTIME_FILE" >> $record_log
gunzip $domain.$LOGTIME.log.gz
mv $domain.$LOGTIME.log /bbd/sata$num/ksfile_logs_handler/ksfile_logs
if [[ $? -ne 0 ]]; then
    #python $wx "[ERROR]"\\n"时间: $now_time"\\n"快手日志下载、解压、移动异常"\\n"日志时间: $LOGTIME"
    mv $LOGTIME_FILE $log_basepath/error # 移动下载异常文件到异常目录
fi

#磁盘号处理
echo "$domain.$LOGTIME.log --> /bbd/sata$num/ksfile_logs_handler/ksfile_logs" >> $record_log

#磁盘号处理
function sata_num_handler(){
    num_param=$1
    num_param_a=${num_param:0:1}
    num_param_b=${num_param:1:1}
    if [[ $num_param == "09" ]]; then
        echo 10
    elif [[ $num_param == "12" ]]; then
        echo "01"
    else
        echo $num_param_a$(($num_param_b+1))
    fi
}

num=`sata_num_handler $num`
echo $num > $shell_log

echo `date` "[INFO] mv log OK!" >> $LOGTIME_FILE
```

## 快手质量数据日志（fiile）下载异常处理
快手file质量数据日志下载，定时检查错误异常日志处理 
执行时间：02、17点
```
00 02,17 * * *
128.49
/bbd/ssd01/zz/shell/task/03_kuaishou_filelog/check404_log.sh
```
check404_log.sh
```
#!/bin/bash
#快手file质量数据日志下载，定时检查错误异常日志处理
wx="/bbd/ssd01/zz/shell/zz.py"
now_time=`date '+%Y-%m-%d %H:%M:%S'`
base_path="/bbd/ssd01/zz/shell/task/03_kuaishou_filelog/logs"
errorlog_path="$base_path/error"
check404_path="$base_path/check_404"
domain="all.gxmov.a.yximgs.com"
shell_log="$log_basepath/recording_num.log"
record_log="$log_basepath/recording_ksfilesplit_check404.log"



# start
SELF_NAME=`basename $0`
PID_FILE=/var/run/$SELF_NAME.pid
# running lock 
if ([ -f $PID_FILE ] && kill -0 `cat $PID_FILE` 2>/dev/null); then
	# if grep -q $SELF_NAME /proc/`cat $PID_FILE 2>/dev/null`/cmdline; then
	echo "$SELF_NAME already is running... "
	python $wx "[ERROR"\\n"时间:$now_time"\\n"快手file质量数据日志下载容错程序同时执行个数 > 1,本定时任务已做退出处理"
	exit 9
	# fi
fi
echo $$ >$PID_FILE
trap "rm -f $PID_FILE; exit 1" 2



#body

#磁盘号处理
function sata_num_handler(){
    num_param=$1
    num_param_a=${num_param:0:1}
    num_param_b=${num_param:1:1}
    if [[ $num_param == "09" ]]; then
        echo 10
    elif [[ $num_param == "12" ]]; then
        echo "01"
    else
        echo $num_param_a$(($num_param_b+1))
    fi
}

error_files=`ls $errorlog_path`
for file in $error_files; do
	mv $errorlog_path/$file $check404_path
	logfile="$check404_path/$file"
	file_date=`echo $file | awk -F '\\\.log' '{print $1}'`
	DLFILE="http://180.186.38.176/data/gxfwtTTKaXMBR/$domain.$file_date.log.gz"
	curl -I $DLFILE > $base_path/log.tmp 2>&1
	grep -q "HTTP/1.1 404 Not Found" $base_path/log.tmp
	if [ $? -eq 0 ];then #有404
		rm -f $base_path/log.tmp
		mv $check404_path/$file $errorlog_path
		echo "[ERROR]"\\n"时间:$now_time"\\n"快手check_404失败,日志: $domain.$file_date.log.gz" >> $record_log
	else

		#删除上次下载处理异常的压缩日志包
		for i in {01..12}; do
			last_log_temp="/bbd/sata$i/ksfile_logs_handler/ksfile_handle_temp/$domain.$file_date.log.gz"
			if [[ -f $last_log_temp ]]; then #如果下载的压缩文件存在则删除
				rm -f $domain.$file_date.log.gz
			fi
		done

		# 快手接口修改：一分钟一个文件
		if [[ ! -f $shell_log ]]; then
			   echo "1" > $shell_log
		fi
		num=`tail -n 1 $shell_log`

		cd /bbd/sata$num/ksfile_logs_handler/ksfile_handle_temp
		wget -S $DLFILE -o $logfile
		gunzip $domain.$file_date.log.gz
		if [[ $? -ne 0 ]]; then
		  mv $logfile $errorlog_path
		  echo "[ERROR]"\\n"时间:$now_time"\\n"快手check_404失败,日志: $domain.$file_date.log.gz" >> $record_log
		  python $wx "[ERROR]"\\n"时间:$now_time"\\n"快手check_404失败,日志: $domain.$file_date.log.gz"
		fi
		######################################
		
		mv $domain.$file_date.log /bbd/sata0$num/ksfile_logs_handler/ksfile_logs
		echo "$domain.$file_date.log --> /bbd/sata0$num/ksfile_logs_handler/ksfile_logs" >> $record_log
		
		num=`sata_num_handler $num`
		echo $num > $shell_log
		rm -f log.tmp
		echo "mv log OK!" >>$logfile
		########################################
	fi
done



# end
rm -f $PID_FILE; exit 0
```


## 快手质量数据日志（fiile）入库监控
*/10 * * * *
128.49
/bbd/ssd01/zz/shell/task/01_watch/ksfilelogs/ksfilelogs_flumehandler_watch.sh
```
#!/bin/bash

#快手file质量数据处理监控
#监控flume 的file channel大小（/bbd/sata{01..12}/flume_cache）
basepath="/bbd/ssd01/zz/shell/task/01_watch/ksfilelogs"
log_basepath="$basepath/logs"
wx="/bbd/ssd01/zz/shell/zz.py"
now_month=`date +%Y%m`
last_month=`date -d "-2 month" +%Y%m` # 上一个月
temp_log_temp="$log_basepath/temp_log_$now_month.log" # 记录log信息，保存一个月
temp_lastmonthlog_temp="$log_basepath/temp_log_$last_month.log" # 拼接将要删除的文件名
record_log="$log_basepath/ksfilelog"
now_time=`date '+%Y-%m-%d %H:%M:%S'`

# 日志临时记录文件处理
if [[ ! -d $log_basepath ]]; then # 文件夹判断是否存在
    mkdir -p $log_basepath
    touch $temp_log_temp
elif [[ ! -f $temp_log_temp ]]; then # 判断文件是否存在
    touch $temp_log_temp
fi
if [[ -f $temp_lastmonthlog_temp ]]; then # 删除上个月的记录
    rm -f $temp_lastmonthlog_temp
fi

if [[ ! -f $record_log ]]; then
    touch $record_log
fi

################################################################################

is_or_not_send(){
    log=$1
    message=$2
    size=$3
    error_count_last=`cat $log | grep ERROR | wc -l`
    if [[ $message == "ERROR" ]]; then
        if [[ $error_count_last -lt 1 ]]; then #上次正常，本次异常，发送异常信息
            # echo "ERROR"
            error_message="[ERROR]"\\n"时间：$now_time"\\n"服务器`hostname`, 快手file质量数据处理cache过大,值(G)：$size"
            echo $error_message > $record_log
            python $wx "$error_message"
        fi
    else
        if [[ $error_count_last -gt 0 ]]; then #上次异常，本次正常，发送正常信息
            # echo "INFO"
            info_message="[INFO]"\\n"时间：$now_time"\\n"服务器`hostname`, 快手file质量数据处理正常,cache值(G)：$size"
            echo $info_message > $record_log
            python $wx "$info_message"
        fi
    fi
}

for i in {01..12}; do
    folder_size=`du -s /bbd/sata$i/flume_cache | awk '{print $1}'`
    folder_size_G=$(($folder_size / 1048576))
    if [[ $folder_size_G -gt 5 ]]; then #如果大于3G报警
        is_or_not_send  $record_log ERROR $folder_size_G
    else
        is_or_not_send "$record_log" "INFO" $folder_size_G
    fi
    echo "时间：$now_time 目录：/bbd/sata$i/flume_cache 大小(G):$folder_size_G" >> $temp_log_temp
done
```

## 快手质量数据日志（fiile）删除

下载的快手的质量数据日志定时删除
00 10 * * *
128.49
/bbd/ssd01/zz/shell/task/03_kuaishou_filelog/delete_ksfilelog.sh
```
#!/bin/bash
# 定时删除从快手下载的质量数据日志（file格式的）
last_week=`date -d "14 days ago" +%Y-%m-%d` # 要删除的日志时间
base_path="/bbd/ssd01/zz/shell/task/03_kuaishou_filelog/logs"
shelllog="$base_path/delete_ksfilelogs.log"
wx="/bbd/ssd01/zz/shell/zz.py"
now_time=`date '+%Y-%m-%d %H:%M:%S'`
ks_filelog_path="/bbd/sata{01..12}/ksfile_logs_handler/ksfile_logs"

rm -f $ks_filelog_path/all.gxmov.a.yximgs.com.$last_week*.log*
if [[ $? -eq 0 ]]; then
        echo `date` " [INFO] 成功删除 all.gxmov.a.yximgs.com.$last_week*" >> $shelllog
else
        error_message="[ERROR] 时间:$now_time ,快手质量日志(file)删除失败,文件: all.gxmov.a.yximgs.com.$last_week*"
        #python $wx "$error_message"
        echo `date` " [ERROR] 快手质量日志(file)删除失败,文件: all.gxmov.a.yximgs.com.$last_week*" >> $shelllog
fi
echo `date`" 脚本执行完成===========================================================" >> $shelllog
```

## 日志监控_flume

监控flume的日志（融合入kafka、自建入kafka、自建入hbase） 
每分钟监控一次（查找上一分钟出现的异常），异常微信报警
* * * * *
128.47
/bbd/ssd01/zz/shell/task/01_watch/flume_log_watch/flume_logs_watch_main.sh
```
#!/bin/bash
#flume log监控执行主脚本

/bbd/ssd01/zz/shell/my_ssh.sh flume "nohup /bbd/ssd01/zz/shell/task/01_watch/flume_log_watch/rh_flume_kafka_log_watch.sh > /dev/null 2>&1 &"
#/bbd/ssd01/zz/shell/my_ssh.sh jx_flume_kafka "nohup /bbd/ssd01/zz/shell/task/01_watch/flume_log_watch/jx_flume_hbase_log_watch.sh > /dev/null 2>&1 &"
/bbd/ssd01/zz/shell/my_ssh.sh jx_flume_kafka "nohup /bbd/ssd01/zz/shell/task/01_watch/flume_log_watch/jx_flume_kafka_log_watch.sh > /dev/null 2>&1 &"
```
rh_flume_kafka_log_watch.sh
```
#!/bin/bash
wx="/bbd/ssd01/zz/shell/zz2.py"
now_time=`date '+%Y-%m-%d %H:%M:%S'`
flume_error_log="/bbd/logs/flume/flume_kafka_error.log" #flume日志路径
shell_log_temp="/bbd/ssd01/zz/shell/task/01_watch/flume_log_watch/logs/rh_flume_kafka" #记录结果的临时文件
shell_log="/bbd/ssd01/zz/shell/task/01_watch/flume_log_watch/logs/flume_logs_watch.log" #记录shell执行日志

#grep flume异常日志文件，查找上分钟是否有异常
function get_error_message(){
        error_file=$1
        now_log_date=`date -d '-1 minute' +"%d %b %Y %H:%M"` #前一分钟时间以flume的日志时间格式显示
        error_message=`fgrep "$now_log_date" $error_file | grep ERROR`
        # echo "rh_kafka , `date` , $now_log_date" >> $shell_log
        echo $error_message
}

flume_error_message=`get_error_message $flume_error_log`
flume_error_message_last=`cat $shell_log_temp | grep ERROR | wc -l`
if [[ $flume_error_message == "" ]]; then #本次正常
        message="[INFO]"\\n"时间：$now_time"\\n"融合日志入kafka正常,服务器`hostname`"
        if [[ $flume_error_message_last -gt 0 ]]; then
                echo "INFO" > $shell_log_temp
                python $wx "$message"
                echo "发送警告信息====INFO====>"$message >> $shell_log
        fi
        echo $message >> $shell_log
else
        message="[ERROR]"\\n"时间：$now_time"\\n"融合日志入kafka异常,服务器:`hostname`,"\\n"异常信息：$flume_error_message"
        if [[ $flume_error_message_last -lt 1 ]]; then #上次正常
                echo "ERROR" > $shell_log_temp
                python $wx "$message"
                echo "发送警告信息====ERROR====>"$message >> $shell_log
        fi
        echo $message >> $shell_log
fi
```
jx_flume_kafka_log_watch.sh
```
#!/bin/bash
wx="/bbd/ssd01/zz/shell/zz2.py"
now_time=`date '+%Y-%m-%d %H:%M:%S'`
flume_error_log="/bbd/logs/flume/flume_jx_kafka.log_error.log" #flume日志路径
shell_log_temp="/bbd/ssd01/zz/shell/task/01_watch/flume_log_watch/logs/jx_flume_kafka" #记录结果的临时文件
shell_log="/bbd/ssd01/zz/shell/task/01_watch/flume_log_watch/logs/flume_logs_watch.log" #记录shell执行日志

#grep flume异常日志文件，查找上分钟是否有异常
function get_error_message(){
        error_file=$1
        now_log_date=`date -d '-1 minute' +"%d %b %Y %H:%M"` #前一分钟时间以flume的日志时间格式显示
        error_message=`fgrep "$now_log_date" $error_file | grep ERROR`
        # echo "rh_kafka , `date` , $now_log_date" >> $shell_log
        echo $error_message
}

flume_error_message=`get_error_message $flume_error_log`
flume_error_message_last=`cat $shell_log_temp | grep ERROR | wc -l`
if [[ $flume_error_message == "" ]]; then #本次正常
        message="[INFO]"\\n"时间：$now_time"\\n"自建日志入kafka正常,服务器`hostname`"
        if [[ $flume_error_message_last -gt 0 ]]; then
                echo "INFO" > $shell_log_temp
                python $wx "$message"
                echo "发送警告信息====INFO====>"$message >> $shell_log
        fi
        echo $message >> $shell_log
else
        message="[ERROR]"\\n"时间：$now_time"\\n"自建日志入kafka异常,服务器:`hostname`,"\\n"异常信息：$flume_error_message"
        if [[ $flume_error_message_last -lt 1 ]]; then #上次正常
                echo "ERROR" > $shell_log_temp
                python $wx "$message"
                echo "发送警告信息====ERROR====>"$message >> $shell_log
        fi
        echo $message >> $shell_log
fi
```


## 每天早上发送前三天jx、rh日志大小，带宽峰值

每天早上发送前三天jx、rh日志大小，带宽峰值 
1、jx日志在hbase中的大小 
2、采集服务器上的融合日志大小 
3、带宽峰值
00 08 * * *
128.47
/bbd/ssd01/zz/shell/task/01_watch/send_weixin_filesize_stats/send_weixin_message.sh
```
#!/bin/bash
#统计自建hbase日志大小，融合日志大小，带宽峰值 发送微信

base_path="/bbd/ssd01/zz/shell/task/01_watch/send_weixin_filesize_stats"
wx="$base_path/wx.py"

statsdata_size_str=""   #带宽
jxlog_hbasesize_str=""  #jx
rhlog_filesize_str=""   #rh


for num in {3,2,1}; do
        log_day=`date -d "-$num day" "+%Y%m%d"`

        #带宽峰值统计
        statsdata_size=`$base_path/start_search_statsdata_5m.sh $log_day`

        #统计jx日志hbase中的大小
        jxlog_hbasesize=`ssh -p23432 172.18.54.147 "$base_path/jx_hbase_filesize.sh $log_day"`

        #统计rh日志大小
        count=0
        for i in `cat $base_path/flume.ip`;do
                temp=`ssh -p23432 $i "$base_path/rhlogs_filesize.sh  $log_day"`
                count=$(($count+$temp))
        done
        rhlog_filesize=`echo "scale=2;$count/1048576" | bc`

        #拼接发送信息
        if [[ $num -eq 3 ]]; then
                statsdata_size_str=$statsdata_size
                jxlog_hbasesize_str=$jxlog_hbasesize
                rhlog_filesize_str=$rhlog_filesize
        else
                statsdata_size_str="$statsdata_size_str->$statsdata_size"
                jxlog_hbasesize_str="$jxlog_hbasesize_str->$jxlog_hbasesize"
                rhlog_filesize_str="$rhlog_filesize_str->$rhlog_filesize"
        fi

done

last_1day=`date -d "-1 day" "+%Y%m%d"`
last_3day=`date -d "-3 day" "+%Y%m%d"`
message="[INFO]"\\n"时间: $last_3day - $last_1day"\\n"日志大小(G): "\\n"自建: $jxlog_hbasesize_str"\\n"融合: $rhlog_filesize_str"\\n"带宽峰值(G):"\\n"$statsdata_size_str"
python $wx "$message"
```
start_search_statsdata_5m.sh
```
#!/bin/bash

#查最大带宽
last_day=$1
statsdata_5m_message=`python /bbd/ssd01/zz/shell/task/01_watch/send_weixin_filesize_stats/search_statsdata_5m.py $last_day`
maxbandwith_5m=`echo $statsdata_5m_message | awk -F ',' '{print $1}'` #昨天的最大带宽值
maxbandwith_5m_temp=`awk 'BEGIN{printf "%.2f\n",'$maxbandwith_5m'/1000}'`
echo $maxbandwith_5m_temp
```
search_statsdata_5m.py
```
# coding=utf-8
import requests,json,time
import sys

logDay = "20170501"
try:
    logDay_temp = sys.argv[1]
except Exception:
    logDay = logDay
else:
    logDay = logDay_temp
statsdata_5m_index = "statsdata_5m_" + (time.strftime('%Y.%m', time.strptime(logDay, '%Y%m%d'))) + "*"
# print statsdata_5m_index
esurl = "http://esclient.bbdcdn.net:9200/%s/_search" %(statsdata_5m_index)

timeArray = time.strptime(logDay, "%Y%m%d")
timestamp = time.mktime(timeArray)
logTimeunix_begin = int(timestamp)
logTimeunix_end = logTimeunix_begin + 86400
# print (logTimeunix_begin, logTimeunix_end)

total = 0
log_timestamp = 0

searchjson = {"query":{"bool":{"must":[{"range":{"from_time":{"gte":logTimeunix_begin,"lt":logTimeunix_end}}}]}},"size":0,"aggs":{"flowStat":{"filters":{"filters":{"total":{"bool":{"must":[{"term":{"layer":"4"}}]}}}},"aggs":{"totalFlow":{"sum":{"field":"traffic"}},"totalRequest":{"sum":{"field":"req_count"}},"flowStat":{"terms":{"field":"from_time","size":2147483647,"order":{"_term":"asc"}},"aggs":{"traffic_pm":{"sum":{"field":"traffic_pm"}}}}}}}}
data = requests.post(esurl,json=searchjson)
result =json.loads(data.text)
# print result['aggregations']['flowStat']['buckets']['total']['flowStat']['buckets']
for v in result['aggregations']['flowStat']['buckets']['total']['flowStat']['buckets']:
    temp = v['traffic_pm']['value']
    temp_t = v['key']
    # print temp
    if temp > total:
        total = temp
        log_timestamp = temp_t
maxstatsdata_5m = total / 300 * 8 /1000 /1000 # 最大带宽值
maxstatsdata_5m_time = time.strftime('%Y-%m-%d %H:%M:%S +0800',time.localtime(log_timestamp))
# print maxstatsdata_5m # 最大带宽值
# print maxstatsdata_5m_time # 最大流量的时间
print str(maxstatsdata_5m) + "," + maxstatsdata_5m_time
```
jx_hbase_filesize.sh
```
#!/bin/bash
#统计hbase中自建日志表的大小

function filesize(){
        last_day=$1
        jx_hbase_size=`/opt/cloudera/parcels/CDH/bin/hdfs dfs -du -s /hbase/data/default/jx"$last_day" | awk '{print $1}'`
        jx_hbase_sizeG=`awk 'BEGIN{printf "%.2f\n",'$jx_hbase_size'/1073741824}'`
        jx_hbase_sizeG=`echo "scale=2;$jx_hbase_sizeG * 2.3" | bc`
        echo $jx_hbase_sizeG
}

#20171220
echo $(filesize $1)
```

## 自建kafka，lag监控

1、监控自建kafka相关消费者生产者的lag值 
2、统计消费日志条数入es（162服务器）
*/10 * * * *
54.162
/bbd/sata01/shell/task/jx-kafka-watch.sh
```
#!/bin/bash
# 自建日志kafka消费情况监控
# */5 * * * * sh /bbd/sata01/shell/task/jx-kafka-watch.sh > /dev/null 2>&1

# local_hostname=`hostname`
log_basepath="/bbd/sata01/shell/task/log_task"  # 日志根目录
wx_basepath="/bbd/sata01/shell/task"
wx="/bbd/sata01/shell/task/wx.py"
now_month=`date +%Y%m`
last_month=`date -d "-2 month" +%Y%m` # 上一个月
temp_log_temp="$log_basepath/temp_log_$now_month.log" # 记录log信息，保存一个月
temp_lastmonthlog_temp="$log_basepath/temp_log_$last_month.log" # 拼接将要删除的文件名
temp_log_file="$log_basepath/jx-kafka-watch.log" # 记录log信息，记录一条，判断是否要发送log信息（异常、正常信息）
temp_log_file2="$log_basepath/jx-kafka-watch.log" # 记录log信息，记录一条，判断是否要发送log信息（异常、正常信息）
now_time=`date '+%Y-%m-%d %H:%M:%S'`

# 日志临时记录文件处理
if [[ ! -d $log_basepath ]]; then # 文件夹判断是否存在
    mkdir -p $log_basepath
    touch $temp_log_file
elif [[ ! -f $temp_log_file ]]; then # 判断文件是否存在
    touch $temp_log_file
fi
if [[ -f $temp_lastmonthlog_temp ]]; then # 删除上个月的记录
    rm -f $temp_lastmonthlog_temp
fi

error_count_temp=`fgrep ERROR $temp_log_file | wc -l` # 临时日志中的异常个数

jx_topic="nginx-access" #自研日志的kafka主题
jx_group="jx_kafka" # flume从自建的kafka中取数据的组
jx_topic2="strong-nginx-access"
jx_group_hbase="jx_hbase_detail"
jx_group_es="jx_es_detail"
jx_band_topic="strong-nginx-access"
jx_band_group="jx_es_stat_2"

kafka_shfile_path="/bbd/ssd01/app/kafka/bin"
zookeeper="183.131.54.162:2181,183.131.54.163:2181,183.131.54.164:2181"

export JAVA_HOME=/bbd/ssd01/app/jdk
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export ZOOKEEPER_HOME=/usr/local/zookeeper
export PATH=:$PATH:$JAVA_HOME/bin:$ZOOKEEPER_HOME/bin

############################## jx 主题 nginx-access lag统计 ###########
#jx_lag=`$kafka_shfile_path/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker \
#    --group $jx_group --topic $jx_topic --zookeeper $zookeeper \
#    | grep $jx_group | awk '{ print $6 }' | awk '{sum+=$1}END{print sum}'`
#
#jx_es_lag=`$kafka_shfile_path/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker \
#    --group $jx_group_es --topic $jx_topic2 --zookeeper $zookeeper \
#    | grep $jx_group_es | awk '{ print $6 }' | awk '{sum+=$1}END{print sum}'`
#
#jx_hbase_lag=`$kafka_shfile_path/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker \
#    --group $jx_group_hbase --topic $jx_topic2 --zookeeper $zookeeper \
#    | grep $jx_group_hbase | awk '{ print $6 }' | awk '{sum+=$1}END{print sum}'`
#
#jx_band_lag=`$kafka_shfile_path/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker \
#    --group $jx_band_group --topic $jx_band_topic --zookeeper $zookeeper \
#    | grep $jx_band_group | awk '{ print $6 }' | awk '{sum+=$1}END{print sum}'`

############################################################
kafkawatch_path="/bbd/sata01/shell/task/log_kafkaoffset"
jx_kafkaToChannel=$kafkawatch_path/jx_kafkaToChannel
jx_chennelToKafka=$kafkawatch_path/jx_chennelToKafka
jx_chennelToEs=$kafkawatch_path/jx_chennelToEs
jx_chennelToHbase=$kafkawatch_path/jx_chennelToHbase
jx_kafkaToBand=$kafkawatch_path/jx_kafkaToBand
#jx_kafka_sec=$kafkawatch_path/jx_kafka_sec_`date +%Y%m%d`.log
jx_kafka_sec=$kafkawatch_path/jx_kafka_sec.log


# 上次的值（这个主题个个消费者的偏移量）
jx_chennel_of_last=`cat $jx_chennelToKafka | awk '{ print $4 }' | awk '{sum+=$1}END{print sum}'`
jx_es_of_last=`cat $jx_chennelToEs | awk '{ print $4 }' | awk '{sum+=$1}END{print sum}'`
jx_hbase_of_last=`cat $jx_chennelToHbase | awk '{ print $4 }' | awk '{sum+=$1}END{print sum}'`
jx_band_of_last=`cat $jx_kafkaToBand | awk '{ print $4 }' | awk '{sum+=$1}END{print sum}'`

$kafka_shfile_path/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker \
    --group $jx_group --topic $jx_topic --zookeeper $zookeeper \
    | grep $jx_group > $jx_chennelToKafka

$kafka_shfile_path/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker \
    --group $jx_group_es --topic $jx_topic2 --zookeeper $zookeeper \
    | grep $jx_group_es > $jx_chennelToEs

$kafka_shfile_path/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker \
    --group $jx_group_hbase --topic $jx_topic2 --zookeeper $zookeeper \
    | grep $jx_group_hbase > $jx_chennelToHbase

$kafka_shfile_path/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker \
    --group $jx_band_group --topic $jx_band_topic --zookeeper $zookeeper \
    | grep $jx_band_group > $jx_kafkaToBand

# 统计本次的
jx_chennel_of=`cat $jx_chennelToKafka | awk '{ print $4 }' | awk '{sum+=$1}END{print sum}'`
jx_es_of=`cat $jx_chennelToEs | awk '{ print $4 }' | awk '{sum+=$1}END{print sum}'`
jx_hbase_of=`cat $jx_chennelToHbase | awk '{ print $4 }' | awk '{sum+=$1}END{print sum}'`
jx_band_of=`cat $jx_kafkaToBand | awk '{ print $4 }' | awk '{sum+=$1}END{print sum}'`

# 统计5分钟的个个的总量
toChannel_count=$(($jx_chennel_of-$jx_chennel_of_last))
toEs_count=$(($jx_es_of-$jx_es_of_last))
toHbase_count=$(($jx_hbase_of-$jx_hbase_of_last))
toBand_count=$(($jx_band_of-$jx_band_of_last))
#echo `date` "toChennel/chennelToEs/chennelToHbase/chenelToBand: $toChannel_count $toEs_count $toHbase_count $toBand_count" >> $jx_kafka_sec

nowTime=`date "+%Y-%m-%dT%H:%M:%S+0800"`
#logid=`date "+%Y%m%d%H%M"`
#echo \{\"nowTime\":\"$nowTime\",\"logid\":$logid ,\"toChennel\":$toChannel_count, \"chennelToEs\":$toEs_count, \"chennelToHbase\":$toHbase_count, \"chenelToBand\":$toBand_count\} >> $jx_kafka_sec
# topic、group、count、time_rang、
echo \{\"range_date\":\"$nowTime\",\"topic\":\"$jx_topic\",\"group\":\"$jx_group\",\"count\":$toChannel_count,\"desc\":\"自建日志增强入kafka channel\"\} >> $jx_kafka_sec
echo \{\"range_date\":\"$nowTime\",\"topic\":\"$jx_topic2\",\"group\":\"$jx_group_es\",\"count\":$toEs_count,\"desc\":\"自建日志增强入es\"\} >> $jx_kafka_sec
echo \{\"range_date\":\"$nowTime\",\"topic\":\"$jx_topic2\",\"group\":\"$jx_group_hbase\",\"count\":$toHbase_count,\"desc\":\"自建日志增强入hbase\"\} >> $jx_kafka_sec
echo \{\"range_date\":\"$nowTime\",\"topic\":\"$jx_band_topic\",\"group\":\"$jx_band_group\",\"count\":$toBand_count,\"desc\":\"自建日志带宽计算\"\} >> $jx_kafka_sec



#统计本次的lag值（这个主题个个消费者的lag值），供监控模块用
jx_lag=`cat $jx_chennelToKafka | awk '{ print $6 }' | awk '{sum+=$1}END{print sum}'`
jx_es_lag=`cat $jx_chennelToEs | awk '{ print $6 }' | awk '{sum+=$1}END{print sum}'`
jx_hbase_lag=`cat $jx_chennelToHbase | awk '{ print $6 }' | awk '{sum+=$1}END{print sum}'`
jx_band_lag=`cat $jx_kafkaToBand | awk '{ print $6 }' | awk '{sum+=$1}END{print sum}'`

#################################################################

if [[ $jx_lag -gt 20000000 ]] || [[ $jx_es_lag -gt 20000000 ]] || [[ $jx_hbase_lag -gt 20000000 ]]; then # jx lag值
    error_message="[WARN]"\\n"时间:$now_time"\\n"自研日志待处理条数偏高,待处理条数"\\n"入kafka channel:$jx_lag"\\n"入es:$jx_es_lag"\\n"入hbase:$jx_hbase_lag"\\n"带宽计算:$jx_band_lag"
#从kafka到kafka channel未处理条数:$jx_lag  从kafka channel到es未处理条数:$jx_es_lag  从kafka channel到hbase未处理条数:$jx_hbase_lag
#自研日志kafka中未入es的条数偏高, 从kafka到kafka channel的/从kafka channel 到es lag值/从kafka channel到hbase的lag值: $jx_lag/$jx_es_lag/$jx_hbase_lag
    if [[ $error_count_temp -lt 1 ]]; then # 有异常，而且上次定时任务记录的log是正常的（上次就是异常不再重复发送）
        #echo "XXXXXXXXXXXXXXXXXXXX"
        python $wx "$error_message" # 调用python脚本发送信息
        echo `date` "发送异常信息 $error_message" >> $temp_log_temp
    fi
    #echo $error_message
    echo `date`" [ERROR] $error_message" > $temp_log_file # 记录单条的文件
    echo `date`" [ERROR] $error_message" >> $temp_log_temp # 保存一个月的log信息
else
	ok_message="[INFO]"\\n"时间:$now_time"\\n"自研日志入库正常,待处理条数"\\n"入kafka channel:$jx_lag"\\n"入es:$jx_es_lag"\\n"入hbase:$jx_hbase_lag"\\n"带宽计算:$jx_band_lag"
 #从kafka到kafka channel未处理条数:$jx_lag  从kafka channel到es未处理条数:$jx_es_lag  从kafka channel到hbase未处理条数:$jx_hbase_lag
    if [[ $error_count_temp -gt 0 ]]; then # 监控正常，但是定时执行记录的log是异常信息，发送信息（有异常转入正常）
        #echo "VVVVVVVVVVVVVVVVVVVVVVVVVVVVVV"
        python $wx "$ok_message"
        echo `date` "发送正常信息 $ok_message" >> $temp_log_temp
    fi
    #echo $ok_message
    echo `date`" [OK] $ok_message" > $temp_log_file
    echo `date`" [OK] $ok_message" >> $temp_log_temp
fi
```

## 融合kafka,lag监控

1、监控融合kafka相关消费者生产者的lag值 
2、统计消费日志条数入es（162服务器）
* * * * *
54.162
/bbd/sata01/shell/task/rh-kafka-watch.sh
```
#!/bin/bash
# 融合日志kafka消费情况监控
# 30 * * * * sh /bbd/sata01/shell/task/jx-kafka-watch.sh > /dev/null 2>&1

# local_hostname=`hostname`
log_basepath="/bbd/sata01/shell/task/log_task"  # 日志根目录
wx_basepath="/bbd/sata01/shell/task"
wx="/bbd/sata01/shell/task/wx.py"
now_month=`date +%Y%m`
last_month=`date -d "-2 month" +%Y%m` # 上一个月
temp_log_temp="$log_basepath/temp_log_$now_month.log" # 记录log信息，保存一个月
temp_lastmonthlog_temp="$log_basepath/temp_log_$last_month.log" # 拼接将要删除的文件名
temp_log_file="$log_basepath/rh-kafka-watch.log" # 记录log信息，记录一条，判断是否要发送log信息（异常、正常信息）
temp_log_file_2="$log_basepath/rh-kafka-watch_logsize.log"
now_time=`date '+%Y-%m-%d %H:%M:%S'`

# 日志临时记录文件处理
if [[ ! -d $log_basepath ]]; then # 文件夹判断是否存在
    mkdir -p $log_basepath
    touch $temp_log_file
elif [[ ! -f $temp_log_file ]]; then # 判断文件是否存在
    touch $temp_log_file
fi
if [[ -f $temp_lastmonthlog_temp ]]; then # 删除上个月的记录
    rm -f $temp_lastmonthlog_temp
fi

error_count_temp=`fgrep ERROR $temp_log_file | wc -l` # 临时日志中的异常个数

rh_topic="firm_logs"
rh_group_es="rh_es_detail_3"
#rh_group_hbase="firm_logs_hbase1"
rh_group_hbase="rh_hbase_detail_2"
rh_band_topic="firm_logs"
rh_band_group="rh_es_stat_2"

kafka_shfile_path="/bbd/ssd01/app/kafka/bin"
zookeeper="183.131.54.162:2181,183.131.54.163:2181,183.131.54.164:2181"

export JAVA_HOME=/bbd/ssd01/app/jdk
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export ZOOKEEPER_HOME=/usr/local/zookeeper
export PATH=:$PATH:$JAVA_HOME/bin:$ZOOKEEPER_HOME/bin

############################## lag统计 ###########
# rh_es_size=`$kafka_shfile_path/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker \
#     --group $rh_group_es --topic $rh_topic --zookeeper $zookeeper \
#     | grep $rh_group_es | awk '{ print $5 }' | awk '{sum+=$1}END{print sum}'`
# 
# rh_es_lag=`$kafka_shfile_path/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker \
#     --group $rh_group_es --topic $rh_topic --zookeeper $zookeeper \
#     | grep $rh_group_es | awk '{ print $6 }' | awk '{sum+=$1}END{print sum}'`
# 
# rh_hbase_lag=`$kafka_shfile_path/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker \
#     --group $rh_group_hbase --topic $rh_topic --zookeeper $zookeeper \
#     | grep $rh_group_hbase | awk '{ print $6 }' | awk '{sum+=$1}END{print sum}'`
# 
# rh_band_lag=`$kafka_shfile_path/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker \
#     --group $rh_band_group --topic $rh_band_topic --zookeeper $zookeeper \
#     | grep $rh_band_group | awk '{ print $6 }' | awk '{sum+=$1}END{print sum}'`

######################################### 查询kafka offset信息统计效率、监控用 #############################################
# 从kafka中查取的offset信息保存文件
kafkawatch_path="/bbd/sata01/shell/task/log_kafkaoffset"
rh_kafkaToEs=$kafkawatch_path/rh_kafkaToEs
rh_kafkaToHbase=$kafkawatch_path/rh_kafkaToHbase
rh_kafkaToBand=$kafkawatch_path/rh_kafkaToBand
#rh_kafka_sec=$kafkawatch_path/rh_kafka_sec_`date +%Y%m%d`.log
rh_kafka_sec=$kafkawatch_path/rh_kafka_sec.log

# 上次的值（这个主题个个消费者的偏移量）
rh_es_size_last=`cat $rh_kafkaToEs | awk '{ print $5 }' | awk '{sum+=$1}END{print sum}'`
rh_es_of_last=`cat $rh_kafkaToEs | awk '{ print $4 }' | awk '{sum+=$1}END{print sum}'`
rh_hbase_of_last=`cat $rh_kafkaToHbase | awk '{ print $4 }' | awk '{sum+=$1}END{print sum}'`
rh_band_of_last=`cat $rh_kafkaToBand | awk '{ print $4 }' | awk '{sum+=$1}END{print sum}'`

# 本次查询的写入文件
$kafka_shfile_path/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker \
    --group $rh_group_es --topic $rh_topic --zookeeper $zookeeper \
    | grep $rh_group_es > $rh_kafkaToEs
$kafka_shfile_path/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker \
    --group $rh_group_hbase --topic $rh_topic --zookeeper $zookeeper \
    | grep $rh_group_hbase > $rh_kafkaToHbase
$kafka_shfile_path/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker \
    --group $rh_band_group --topic $rh_band_topic --zookeeper $zookeeper \
    | grep $rh_band_group > $rh_kafkaToBand

# 统计本次个个消费者的偏移量w
rh_es_size=`cat $rh_kafkaToEs | awk '{ print $5 }' | awk '{sum+=$1}END{print sum}'`
rh_es_of=`cat $rh_kafkaToEs | awk '{ print $4 }' | awk '{sum+=$1}END{print sum}'`
rh_hbase_of=`cat $rh_kafkaToHbase | awk '{ print $4 }' | awk '{sum+=$1}END{print sum}'`
rh_band_of=`cat $rh_kafkaToBand | awk '{ print $4 }' | awk '{sum+=$1}END{print sum}'`

# 统计5分钟的个个的总量
tokafka_count=$(($rh_es_size-$rh_es_size_last))
kafkaToEs_count=$(($rh_es_of-$rh_es_of_last))
kafkaToHbase_count=$(($rh_hbase_of-$rh_hbase_of_last))
kafkaToBand_count=$(($rh_band_of-$rh_band_of_last))
#echo `date` "tokafka/kafkaToEs/kafkaToHbase/KafkaToBand: $tokafka_count $kafkaToEs_count $kafkaToHbase_count $kafkaToBand_count" >> $rh_kafka_sec

nowTime=`date "+%Y-%m-%dT%H:%M:%S+0800"`
#logid=`date "+%Y%m%d%H%M"`
#echo \{\"nowTime\":\"$nowTime\", \"logid\":$logid , \"tokafka\":$tokafka_count, \"kafkaToEs\":$kafkaToEs_count, \"kafkaToHbase\":$kafkaToHbase_count, \"KafkaToBand\":$kafkaToBand_count\} >> $rh_kafka_sec
echo \{\"range_date\":\"$nowTime\",\"topic\":\"$rh_topic\",\"group\":\"None\",\"count\":$tokafka_count,\"desc\":\"融合日志从file到kafka\"\} >> $rh_kafka_sec
echo \{\"range_date\":\"$nowTime\",\"topic\":\"$rh_topic\",\"group\":\"$rh_group_es\",\"count\":$kafkaToEs_count,\"desc\":\"融合日志入es\"\} >> $rh_kafka_sec
echo \{\"range_date\":\"$nowTime\",\"topic\":\"$rh_topic\",\"group\":\"$rh_group_hbase\",\"count\":$kafkaToHbase_count,\"desc\":\"融合日志入hbase\"\} >> $rh_kafka_sec
echo \{\"range_date\":\"$nowTime\",\"topic\":\"$rh_band_topic\",\"group\":\"$rh_band_group\",\"count\":$kafkaToBand_count,\"desc\":\"融合日志带宽计算\"\} >> $rh_kafka_sec

#统计本次的lag值（这个主题个个消费者的lag值），供监控模块用
rh_es_lag=`cat $rh_kafkaToEs | awk '{ print $6 }' | awk '{sum+=$1}END{print sum}'`
rh_hbase_lag=`cat $rh_kafkaToHbase | awk '{ print $6 }' | awk '{sum+=$1}END{print sum}'`
rh_band_lag=`cat $rh_kafkaToBand | awk '{ print $6 }' | awk '{sum+=$1}END{print sum}'`

############################ 监控模块 #####################################
#rh_logsize_temp=`cat $temp_log_file_2`
#rh_logsize_temp=$(($rh_es_size-$rh_logsize_temp))
#if [[ $rh_logsize_temp -lt 10000 ]];then
#    logsize_error_message="[WARN] 时间:$now_time 融合日志入kafka增量过小,增量:$rh_logsize_temp"
#    python $wx_basepath/zz.py "$logsize_error_message"
#    echo $logsize_error_message >> $temp_log_temp
#fi
#echo `date`"[INFO] " >> /bbd/sata01/shell/task/rh_kakfa_watch_temp.logecho $rh_es_size > $temp_log_file_2
#echo $rh_es_size > $temp_log_file_2

if [[ $rh_es_lag -gt 500000000 ]] || [[ $rh_hbase_lag -gt 500000000 ]]; then
    error_message="[WARN]"\\n"时间:$now_time"\\n"融合日志入库待处理条数偏高,待处理条数"\\n"入es:$rh_es_lag"\\n"入hbase:$rh_hbase_lag"\\n"带宽计算:$rh_band_lag" #从kafka取数据入es待处理的条数:$rh_es_lag 入hbase待处理条数:$rh_hbase_lag
    if [[ $error_count_temp -lt 1 ]]; then # 有异常，而且上次定时任务记录的log是正常的（上次就是异常不再重复发送）
        #echo "XXXXXXXXXXXXXXXXXXXX"
        python $wx "$error_message" # 调用python脚本发送信息
        echo `date` "发送异常信息 $error_message" >> $temp_log_temp
    fi
    #echo $error_message
    echo `date`" [ERROR] $error_message" > $temp_log_file # 记录单条的文件
    echo `date`" [ERROR] $error_message" >> $temp_log_temp # 保存一个月的log信息
else
    ok_message="[INFO]"\\n"时间:$now_time"\\n"融合日志入库正常,待处理条数"\\n"入es:$rh_es_lag"\\n"入hbase:$rh_hbase_lag"\\n"带宽计算:$rh_band_lag"
 #从kafka取数据入es待处理的条数:$rh_es_lag 入hbase待处理条数:$rh_hbase_lag #OK 融合日志入库正常,入es的lag值/入hbase的lag值: $rh_es_lag/$rh_hbase_lag
#echo $ok_message
    if [[ $error_count_temp -gt 0 ]]; then # 监控正常，但是定时执行记录的log是异常信息，发送信息（有异常转入正常）
        #echo "VVVVVVVVVVVVVVVVVVVVVVVVVVVVVV"
        python $wx "$ok_message"
        echo `date` "发送正常信息 $ok_message" >> $temp_log_temp
    fi
    #echo $ok_message
    echo `date`" [OK] $ok_message" > $temp_log_file
    echo `date`" [OK] $ok_message" >> $temp_log_temp
fi
```


## 融合日志文件分布情况统计入es

1、 文件大小统计：统计文件个数，及文件大小占比分布 
时间范围  服务器  磁盘  OK个数   非OK个数      文件总大小   超过某个值（500M）的文件个数 
入到测试环境es中的索引：wushan_rh_log_destri
00 07 * * *
128.47
/bbd/ssd01/zz/shell/task/01_watch/rh_log_distri/rh_log_distri_main.sh
```
#!/bin/bash

# 调用主程序，128.47上执行
# 统计各个磁盘融合日志分布情况
# 1、 文件大小统计：统计文件个数，及文件大小占比分布
# 时间范围  服务器  厂商  OK个数   非OK个数      文件总大小   超过某个值（500M）的文件个数

#log_day=$1
log_day=`date -d "-1 day" +%Y%m%d`
base_path="/bbd/ssd01/zz/shell/task/01_watch/rh_log_distri"
data="/bbd/ssd01/zz/shell/task/01_watch/rh_log_distri/data.json"

#if [[ $# -lt 1 ]]; then
#    echo "参数 < 1，请输入参数日志时间（天）"
#    exit 1
#fi

$base_path/my_ssh.sh flume "/bbd/ssd01/zz/shell/task/01_watch/rh_log_distri/rh_log_distri.sh $log_day" > $data
#curl -XPOST http://172.18.54.181:9200/_bulk --data-binary @$data  > /dev/null 2>&1
#curl -XPOST http://esclient.bbdcdn.net:9200/_bulk --data-binary @$data  > /bbd/ssd01/zz/shell/task/01_watch/rh_log_distri/curl_es.log 2>&1
echo "`date` [INFO] 开始插入数据到es中......................"
curl -XPOST http://esclient.bbdcdn.net:9200/_bulk --data-binary @$data >> /bbd/ssd01/zz/shell/task/01_watch/rh_log_distri/curl_es.log 2>&1
echo "`date` [INFO] 插入数据到es中结束......................"
```
rh_log_distri.sh
```
#!/bin/bash

# 统计各个磁盘融合日志分布情况
# 1、 文件大小统计：统计文件个数，及文件大小占比分布
# 时间范围  服务器  厂商  OK个数   非OK个数      文件总大小   超过某个值（500M）的文件个数

log_day=$1
nowTime=`date "+%Y-%m-%dT%H:%M:%S+0800"`
nowTimeH=`date "+%Y-%m-%dT%H"`

if [[ $# -lt 1 ]]; then
        exit 1
fi
for i in {01..12}; do
        base_path="/bbd/sata$i/firm_logs/$log_day"

        #文件数处理
        log_count=`ls $base_path | wc -l` #总日志文件
        oklog_count=`find $base_path -type f -name "*ok" | wc -l` #处理后的文件个数
        nooklog_count=$(($log_count-$oklog_count)) #这个磁盘这天未处理的文件数

        #日志大小处理
        size_count=`du -s $base_path |awk '{print $1}'`
        size_count=`awk 'BEGIN{printf "%.2f\n",'$size_count/1048576'}'` #一个磁盘文件大小总和
        gtsize_count=0 #大于某个值的文件个数统计
        gtsize=512000 #文件大小比较阈值
        for j in `du -s $base_path/* |awk '{print $1}'`;do
                if [[ $j -ge $gtsize ]]; then
                        gtsize_count=$(($gtsize_count+1))
                fi
        done

	hn=`hostname`
	md5key="$hn$i$nowTimeH"
	md5value=`echo $md5key | md5sum | awk '{print $1}'`

        echo \{\"index\":\{\"_index\":\"logfile_stat\",\"_type\":\"rh_log_destri\",\"_id\":\"$md5value\"}}
        echo \{\"date\":\"$nowTime\", \"hostname\":\"$hn\",\"sata\":\"$i\",\"file_count\":$log_count,\"file_ok\":$oklog_count,\"file_undo\":$nooklog_count,\"file_size\":$size_count,\"bigfile_count\":$gtsize_count\}
done

#logfile_stat               check_date    file_count  file_ok  file_undo  file_size  bigfile_count

```


## 融合日志文件未处理个数监控报警
监控融合日志未处理个数，如果超过某个阈值报警
2个参数分别为rh_file  和 监控融合日志未处理数报警阈值
*/5 * * * *
128.47
/bbd/ssd01/zz/shell/task/01_watch/rh_file_watch_main.sh "rh_file" "1000"
```
#!/bin/bash
#汇总集群监控信息，处理并发送报警信息
group=$1 #监控的组（flume or kafka）
num=$2 #监控融合日志未处理个数阈值
base_path="/bbd/ssd01/zz/shell"
wx_path="$base_path/wx.py" #微信公众号发送信息脚本
flume_ip="$base_path/conf/flume.ip"	#flume服务器ip列表
log_file="$base_path/task/01_watch/logs/$group""_watch.log" #记录上次监控的值（只保存异常信息）
log_file_temp="$base_path/task/01_watch/logs/temp_`date '+%Y%m'`.log"
shell_script="$base_path/task/01_watch/"$group"_watch.sh" #flume服务器上的监控脚本
now_time=`date '+%Y-%m-%d %H:%M:%S'`
message="" #存放所有的flume服务器上flume监控脚本执行结果（返回的异常信息）

if [[ $# -ne 2 ]]; then
	echo "`date` [INFO] "$group"_watch_main: 参数缺少服务监控的组" >> $log_file_temp
	exit 1
fi
if [[ ! -f $flume_ip ]]; then
	echo "`date` [INFO] "$group"_watch_main: 文件$flume_ip不存在..." >> $log_file_temp
	exit 1
fi
if [[ ! -f $log_file ]]; then
	touch $log_file
fi

last_month=`date -d "-2 month" +%Y%m` # 上一个月
log_file_temp_last="$base_path/task/01_watch/logs/temp_$last_month"".log"
if [[ -f $log_file_temp_last ]]; then # 删除上个月的记录
    rm -f $log_file_temp_last
fi

#调用flume个服务器上的脚本，获取监控结果
function server_watch_main(){
	for i in `cat $flume_ip`;do
		temp=`ssh -p23432 $i $shell_script "$num"` #循环调用flume各服务器上的监控脚本
		if [[ $temp != "OK" ]]; then #异常
			if [[ $message ]]; then
				message="$message@###@$temp"
			else
				message=$temp
			fi
		fi
	done
	echo $message
}

#字符转数组
function str_to_arr(){
	str=$1 #字符
	sep=$2 #分隔符
	
	OLD_IFS="$IFS"
	IFS="$sep"
	arr=($str)
	IFS="$OLD_IFS"

	echo "${arr[@]}"
	#echo "("${arr[@]}")"
	#for s in ${arr[@]};do
	#	echo "$s" 
	#done
}

#获取汇总异常信息中所有的主机名
function host_in_message(){
	arr=$1
	hosts=""
	for host_message in ${arr[@]}; do
		temp=${host_message%%,*} #截取主机名
		if [[ $hosts ]]; then
			hosts=$hosts"\\n"$temp
		else
			hosts=$temp
		fi
	done
	echo $hosts
}

host_in_message_rhfile(){
	# c3n-zj-nb1-183-136-128-53:1183,ERROR
	# 针对有异常信息的处理（比正常的多了 ‘:1183’）
	arr=($(echo "$@"))
	hosts=""
	for host_message in ${arr[@]}; do
		temp=${host_message%%:*} #截取主机名
		if [[ $hosts ]]; then
			hosts=$hosts" "$temp
		else
			hosts=$temp
		fi
	done
	echo $hosts
}

now_message=`server_watch_main`	#本次汇总的监控值
last_message=`cat $log_file`	#上次汇总的监控值
send_message=""					#发送微信报警信息

#异常消息处理，判断是否发送信息到微信报警
function message_handler(){
	if [[ $now_message ]]; then #本次ERROR
		now_message_arr=`str_to_arr $now_message "@##@"` #各服务器信息放到数组里
		now_message_hosts=`host_in_message "${now_message_arr[@]}"` #异常主机名列表
		echo $now_message > $log_file
		echo "`date` [ERROR] "$group"_watch_main: $now_message" >> $log_file_temp

		if [[ $last_message ]]; then #上次ERROR本次ERROR
			#特殊处理，处理融合日志文件未消费个数 监控
			if [[ $group == "rh_file" ]]; then
				last_message_arr=`str_to_arr $last_message "@##@"` #各服务器信息放到数组里
				host_in_message_rhfile_last=`host_in_message_rhfile $(echo ${last_message_arr[*]})`
				host_in_message_rhfile_now=`host_in_message_rhfile $(echo ${now_message_arr[*]})`
				if [[ $host_in_message_rhfile_last != $host_in_message_rhfile_now ]]; then
					send_message="[WARN]\\n时间: $now_time\\n融合日志未处理日志个数较多,服务器主机名和未处理文件数:\\n$now_message_hosts"
				fi
			else
				if [[ $now_message != $last_message ]]; then
					send_message="[WARN]\\n时间: $now_time\\n融合日志未处理日志个数较多,服务器主机名和未处理文件数:\\n$now_message_hosts"
				fi
			fi
		else #上次OK本次ERROR
			send_message="[WARN]\\n时间: $now_time\\n融合日志未处理日志个数较多,服务器主机名和未处理文件数:\\n$now_message_hosts"
		fi
	else #本次OK
		if [[ $last_message ]]; then #上次ERROR本次OK
			send_message="[INFO]\\n时间: $now_time\\n各服务器融合日志未处理日志数正常"
		fi
		> $log_file
	fi
}

message_handler
if [[ $send_message ]]; then
	python $wx_path "$send_message" #发送微信消息报警
	echo "`date` [INFO] "$group"_watch_main: 发送了报警消息: $send_message" >> $log_file_temp
else
	echo "`date` [INFO] "$group"_watch_main: "$group"正常或和上次状态一样, 没有发送报警信息" >> $log_file_temp
fi
```
rh_file_watch.sh
```
#!/bin/bash
#监控融合未处理的日志文件个数，超过某个值时报警

num=$1
local_hostname=`hostname`

function rh_file_watch(){
    value=$1
    no_hand_log_count=`find /bbd/sata{01..12}/firm_logs -type f | grep -v \\\.ok | wc -l`
    if [[ $no_hand_log_count -gt $value ]]; then   #有异常
        echo "$local_hostname"":$no_hand_log_count,ERROR" #返回主机名和ERROR（中间用分隔符隔开）
    else
        echo "OK"
    fi
}

message=`rh_file_watch $num`   #调用函数
echo $message
```

## 融合本地磁盘未消费文件、条数统计
统计162 - 166 ，49 - 54服务器每磁盘上的融合未处理的文件个数、未处理文件的总行数
```
* * * * *
128.47
/bbd/ssd01/zz/shell/my_ssh.sh flume "nohup /bbd/ssd01/zz/shell/task/01_watch/diskfile_count_message.sh &"
```
```
#!/bin/bash

# 统计162 - 166 ，213 - 214服务器每磁盘上的未处理的文件个数、未处理文件的总行数

function count_message(){
	hm=`hostname`
	hms="c3n-zj-nb1-115-231-183-213 c3n-zj-nb1-115-231-183-214"
	nowTime=`date "+%Y-%m-%dT%H:%M:%S+0800"`
	base_path="/bbd/sata"
	firm_logs="firm_logs"
	disk_num=12
	topic="firm_logs"
	group="None"

	log_day=$1
	log_file=$2

	if [[ $hms =~ $hm ]]; then
		disk_num=7
	fi
	for i in `seq $disk_num`; do # 遍历12块盘，有2台机器是7块盘
		if [[ $i -lt 10 ]]; then
			i=0$i
		fi
		file_path="$base_path$i/$firm_logs/$log_day"
		files=`find $file_path -type f | grep -v ok`
		file_count=0
		log_count=0
		for file in $files; do
			count_temp=`echo $file | awk -F '\\\.access' '{print $2}' | awk -F \_ '{print $2}'`
			if [[ $count_temp ]]; then
				file_count=$(($file_count+1))
				log_count=$(($log_count+$count_temp))
			fi
		done
		if [[ $file_count -gt 0 ]]; then
			#echo \{\"range_date\":\"$nowTime\",\"hostname\":\"$hm\",\"file_path\":\"$file_path\",\"topic\":\"$topic\",\"file_count\":$file_count,\"log_count\":$log_count\} >> $log_file
			message="{\"range_date\":\"$nowTime\",\"hostname\":\"$hm\",\"file_path\":\"$file_path\",\"topic\":\"$topic\",\"file_count\":$file_count,\"log_count\":$log_count}"
#			echo $message
			curl -XPOST http://183.136.128.47:9200/monitor-file/file/ -d "$message"
			echo $message >> $log_file
		else
			#echo \{\"range_date\":\"$nowTime\",\"hostname\":\"$hm\",\"file_path\":\"$file_path\",\"topic\":\"$topic\",\"file_count\":$file_count,\"log_count\":$log_count\} >> $log_file"_temp"
			message="{\"range_date\":\"$nowTime\",\"hostname\":\"$hm\",\"file_path\":\"$file_path\",\"topic\":\"$topic\",\"file_count\":$file_count,\"log_count\":$log_count}"
#			echo $message
			curl -XPOST http://183.136.128.47:9200/monitor-file/file/ -d "$message"
			echo $message >> $log_file
		fi
		#echo \{\"range_date\":\"$nowTime\",\"hostname\":\"$hm\",\"file_path\":\"$file_path\",\"topic\":\"$topic\",\"file_count\":$file_count,\"log_count\":$log_count,\"desc\":\"统计各磁盘flume未处理的文件数、日志条数\"\} >> $log_file
	done
}

log_day=`date +%Y%m%d`
log_file="/bbd/ssd01/zz/shell/task/01_watch/logs/diskfile_info.log"
count_message $log_day $log_file
#files=$(count_message $log_day $log_file)
```

# 128.47 crontab
## es日志删除
```
* 3 * * * /bin/find /bbd/ssd01/app/elasticsearch-5.1.1/logs/ -mtime +2 -name "*.log*" -exec rm -rf {} \;
```

## nginx日志切分、reload配置
```
00 00 * * * bash /usr/local/openresty/nginx/nginx_logcut > /tmp/nginx_logcut.cron.log
```
nginx_logcut
```
#!/bin/bash
#description:cut nginx log per day.
#define logs dir
LOGS_ACCESS="/usr/local/openresty/nginx/logs"
#define pid file
PID_PATH="/usr/local/openresty/nginx/logs/nginx.pid"
#define date
DATE=`date -d "yesterday" +%F`
DATE_DIR=`date +%Y-%m`
#cut log
if [ ! -d ${LOGS_ACCESS}/${DATE_DIR} ];then
    mkdir ${LOGS_ACCESS}/${DATE_DIR}
fi
    mv ${LOGS_ACCESS}/esrest.access.log ${LOGS_ACCESS}/${DATE_DIR}/esrest.access_$DATE.log
#reload nginx
kill -USR1 `cat ${PID_PATH}`
```
USR1亦通常被用来告知应用程序重载配置文件；例如，向Apache HTTP服务器发送一个USR1信号将导致以下步骤的发生：停止接受新的连接，等待当前连接停止，重新载入配置文件，重新打开日志文件，重启服务器，从而实现相对平滑的不关机的更改
kill -HUP pid 或者 killall -HUP pName：
其中pid是进程标识，pName是进程的名称
如果想要更改配置而不需停止并重新启动服务，可以使用上面两个命令。在对配置文件作必要的更改后，发出该命令以动态更新服务配置。
根据约定，当你发送一个挂起信号(信号1或HUP)时，大多数服务器进程(所有常用的进程)都会进行复位操作并重新加载它们的配置文件


## 删除es中的 下发的下载日志的任务记录
```
*/5 * * * * /bbd/ssd01/flask/app/cronDeleteTaskES1.sh > /bbd/ssd01/flask/app/cronDeleteTaskES1.log
```
cronDeleteTaskES1.log
```
PYTHON_HOME=/bbd/ssd01/flask/flask
threadNum=`ps -ef|grep deleteTaskES1.py|grep -v grep |wc -l`
echo $threadNum
if [ $threadNum -eq "0" ]
then
        echo "starting"
        $PYTHON_HOME/bin/python /bbd/ssd01/flask/app/deleteTaskES2.py
fi
```
deleteTaskES2.py
```
from datetime import datetime
import hashlib
from elasticsearch import Elasticsearch
from elasticsearch import  helpers
import json
import requests
import time

es=Elasticsearch("172.18.183.213:9200",timeout=1000,sniff_on_start=True, sniff_on_connection_fail=True, sniffer_timeout=60,sniff_timeout=10,max_retries=10)
successMap={}
bodys1= {"query": {"bool": {"must": [{"term":{"status":"accepted"}}]}},"from": 0,"size": 10000}
res=es.search(index="logrestore-task1",doc_type="task1",body=json.dumps(bodys1))
#source=res[]
for hit in res['hits']['hits']:
    file_name=hit["_source"]["file_name"]
    print(file_name)
    fix_id=hit["_source"]["taskid"]
    comList=file_name.split("_")
    
    indexNames=[]
    timeArray = time.strptime(comList[2], '%Y%m%d%H%M%S')
    timeStamp = int(time.mktime(timeArray))
    oneAgo = timeStamp - 24 * 60 * 60
    oneAfter = timeStamp + 24 * 60 * 60
    twoAfter = timeStamp + 2 * 24 * 60 * 60
    currentTime=time.localtime(timeStamp)
    oneAgoTime = time.localtime(oneAgo)
    oneAfterTime=time.localtime(oneAfter)
    twoAfterTime=time.localtime(twoAfter)
    timeDateOneAgo=time.strftime("%Y.%m.%d", oneAgoTime)
    timeDateOneAfter=time.strftime("%Y.%m.%d", oneAfterTime)
    timeDateTwoAfter=time.strftime("%Y.%m.%d", twoAfterTime)
    timecurrentDate=time.strftime("%Y.%m.%d", currentTime)
    for pre in ("statsdata_m_","statsdata_5m_"):
        for timeDate in (timeDateOneAgo,timeDateOneAfter,timeDateTwoAfter,timecurrentDate):
            if es.indices.exists(pre+timeDate):
                indexNames.append(pre+timeDate)


    existsIndexName=",".join(indexNames)
    print(existsIndexName)
    subComList=comList[3].split(".")
    index_name="nginx-"+subComList[0]+"-"+str(comList[2])[0:8]
    index_name=index_name.lower()
    index_nameLag="nginx-lag-"+str(comList[2])[0:8]
    print(index_name)
    query2 = {"query": {"bool": {"must": [{"wildcard": {"file_name": file_name+"*"}}]}}}
    try:
        if es.indices.exists(index_nameLag):
            es.delete_by_query(index=index_name+","+index_nameLag+","+existsIndexName,body=json.dumps(query2),doc_type='fluentd,bandwidth',conflicts="proceed",wait_for_completion=True,timeout="60m")
            res1 = es.search(index=index_name+","+index_nameLag+","+existsIndexName, doc_type="fluentd,bandwidth", body=json.dumps(query2))
            print(res1['hits']['total'])
            if res1['hits']['total']!=0:
                continue
        else:
            es.delete_by_query(index=index_name+","+existsIndexName,body=json.dumps(query2),doc_type='fluentd,bandwidth',conflicts="proceed",wait_for_completion=True,timeout="60m")
            res1 = es.search(index=index_name + "," + existsIndexName,doc_type="fluentd,bandwidth", body=json.dumps(query2))
            print(res1['hits']['total'])
            if res1['hits']['total'] != 0:
                continue
        if successMap.get(fix_id) is None:
            successMap[fix_id]=[]
        successMap.get(fix_id).append(file_name)
        print("delete success")
        #successList.append(file_name)
        m2 = hashlib.md5()
        m2.update((file_name +"-"+ fix_id).encode("UTF-8"))
        body = {
            "file_name": file_name,
            "status": "deleted",
            "taskid": fix_id
        }
        es.index(index="logrestore-task1", doc_type='task1', body=body, id=m2.hexdigest())
    except Exception as e:
        print(e)
        print("1")

print(len(successMap))
for key in successMap:
    subSuccessMap={"fix_id":key,"log_name":successMap[key]}
    print(subSuccessMap)
    #r = requests.post("http://boss.sinobbd.com/api/deleteStatusNotify", data=json.dumps(subSuccessMap))
   # print(r.json())
```

## 创建索引，插入数据测试
```
30 9,17 * * * /bbd/ssd01/app/zrk/indexTest.sh > /bbd/ssd01/app/zrk/indexTest.log
```
indexTest.sh
```
curl -X DELETE "http://localhost:9200/nginx-test,nginx-tx-test,ksfilelog-test,kshttplog-test,statsdata_test,ksratio_test"

curl -XPUT "http://localhost:9200/nginx-test/fluentd/1" -d '{"firm_name": "BS",
"response_time": "71",
"http_range": "-",
"upstream_response_time": "71",
"time_local": "03/Sep/2017:08:01:02 +0800",
"time_unix": "1504396862",
"clientip": "61.49.238.167",
"method": "GET",
"domain": "m.weekly.caixin.com",
"request": "/favicon.ico",
"protocol": "HTTP/1.1",
"http_status": "301",
"body_bytes_sent": 719,
"referer": "http://m.weekly.caixin.com/m/2017-09-02/101139632.html?utm_source=TouTiao&utm_medium=toutiaomxml&utm_campaign=Hezuo",
"user_agent": "Mozilla/5.0+(Linux;+Android+7.0;+KNT-AL20+Build/HUAWEIKNT-AL20;+wv)+AppleWebKit/537.36+(KHTML,+like+Gecko)+Version/4.0+Chrome/55.0.2883.91+Mobile+Safari/537.36+JsSdk/2+NewsArticle/6.3.4+NetType/wifi",
"hit_status_detail": "TCP_MISS",
"hit_status": "0",
"upstream_addr": "60.12.124.145",
"server_addr": "220.194.215.163",
"X_Info_Fetcher": "-",
"X_Info_ObjSize": "-",
"X_Info_request_id": "-",
"X_Info_MD5": "-",
"prov": "北京",
"country": "中国",
"city": "北京",
"ISP": "联通",
"latitude": "0",
"longitude": "0",
"blockid": "-",
"referer_host": "m.weekly.caixin.com",
"search_engine": "-",
"hackerid": "-",
"layer": "4",
"def1": "-",
"file_name": "20202851_m.weekly.caixin.com_20170903080000_BS.access_724",
"line_num": "642",
"rawdata": "",
"except_desc": ""}'
sleep 5s
curl -XPUT "http://localhost:9200/nginx-tx-test/fluentd/1" -d '{"firm_name": "BS",
"response_time": "71",
"http_range": "-",
"upstream_response_time": "71",
"time_local": "03/Sep/2017:08:01:02 +0800",
"time_unix": "1504396862",
"clientip": "61.49.238.167",
"method": "GET",
"domain": "m.weekly.caixin.com",
"request": "/favicon.ico",
"protocol": "HTTP/1.1",
"http_status": "301",
"body_bytes_sent": 719,
"referer": "http://m.weekly.caixin.com/m/2017-09-02/101139632.html?utm_source=TouTiao&utm_medium=toutiaomxml&utm_campaign=Hezuo",
"user_agent": "Mozilla/5.0+(Linux;+Android+7.0;+KNT-AL20+Build/HUAWEIKNT-AL20;+wv)+AppleWebKit/537.36+(KHTML,+like+Gecko)+Version/4.0+Chrome/55.0.2883.91+Mobile+Safari/537.36+JsSdk/2+NewsArticle/6.3.4+NetType/wifi",
"hit_status_detail": "TCP_MISS",
"hit_status": "0",
"upstream_addr": "60.12.124.145",
"server_addr": "220.194.215.163",
"X_Info_Fetcher": "-",
"X_Info_ObjSize": "-",
"X_Info_request_id": "-",
"X_Info_MD5": "-",
"prov": "北京",
"country": "中国",
"city": "北京",
"ISP": "联通",
"latitude": "0",
"longitude": "0",
"blockid": "-",
"referer_host": "m.weekly.caixin.com",
"search_engine": "-",
"hackerid": "-",
"layer": "4",
"def1": "-",
"file_name": "20202851_m.weekly.caixin.com_20170903080000_BS.access_724",
"line_num": "642",
"rawdata": "",
"except_desc": ""}'
sleep 5s
curl -XPUT "http://localhost:9200/ksfilelog-test/fluentd/1" -d '{"reason": "",
"country": "中国",
"city": "大陆其它",
"sys": "ANDROID_4.4.4",
"clientTime": "2017-08-31 00:20:53 +0800",
"result": "success",
"dev": "OPPO(R8107)",
"host": "gxmov.a.yximgs.com",
"serverTime": "2017-08-31 00:21:25 +0800",
"addr": "中国|贵州省|毕节市|七星关区",
"dnsServers": "",
"prov": "贵州",
"fileLength": "0",
"retry": "false",
"cost": "290",
"line_num": "363097",
"file_name": "all.gxmov.a.yximgs.com.08-31-00.log_an",
"ISP": "移动",
"length": "516950",
"url": "http://117.187.23.226/upic/2017/08/30/21/BMjAxNzA4MzAyMTU4MDBfNjM5NjU2MzAzXzMxMDM5Mjk3NTBfMl8z_b.mp4",
"@timestamp": "1504143288146",
"clientIp": "117.136.17.238",
"serverIp": "117.187.23.226",
"desc": ""}'
sleep 5s
curl -XPUT "http://localhost:9200/kshttplog-test/ksratio/1" -d '{"server_time": 1504196700000,
"@timestamp": 1504200904387,
"fail_ratio": 0.544,
"total_count": 1711543,
"slow_ratio": 0.807,
"slow_count": 13810,
"fs_ratio": 1.351,
"fail_count": 9317}'
sleep 5s
curl -XPUT "http://localhost:9200/statsdata_test/bandwidth/1" -d '{"firm_name": "JX",
"traffic": 39906,
"layer": "1",
"traffic_pm": 39906,
"from_time": 1490544600,
"hit_status": "0",
"domain": "gxcdn.xiangfubao.com.cn",
"prov": "浙江",
"time_local": 1490544600000,
"req_count": 3,
"def2": "spk",
"ISP": "电信",
"http_status": 200}' 
sleep 5s

curl -XPUT "http://localhost:9200/ksratio_test/ksratio/1" -d '{"server_time": 1490996400000,
"@timestamp": 1492765868145,
"fail_ratio": 0.649,
"total_count": 14024,
"slow_ratio": 0.734,
"slow_count": 103,
"fs_ratio": 1.383,
"fail_count": 91}'
#delete

#curl -X DELETE "http://localhost:9200/nginx-test,nginx-tx-test,ksfilelog-test,kshttplog-test,statsdata_test"
```

## 同步mysql数据到es中
```
0 8 * * *  /bbd/ssd01/app/zrk/mysqlToES.sh > /bbd/ssd01/app/zrk/mysqlToES.log
```
mysqlToES.sh
```
. /etc/profile
startDate=`date -d "-1 day" +%Y-%m-%d`

endDate=`date +%Y-%m-%d`
echo $startDate
echo $endDate
startTime=`date -d "$startDate" +%s`
endTime=`date -d "$endDate" +%s`
echo $startTime
echo $endTime

sed -i "s/datetime>=/datetime>=$startTime/g" /bbd/ssd01/app/logstash-5.2.0/config/sql/sqlTest.txt
sed -i "s/datetime</datetime<$endTime/g" /bbd/ssd01/app/logstash-5.2.0/config/sql/sqlTest.txt

nohup /bbd/ssd01/app/logstash-5.2.0/bin/logstash -f /bbd/ssd01/app/logstash-5.2.0/config/jdbcES.yml &

echo $startTime
echo $endTime
sleep 60s
echo "select a.domain,a.datetime,a.traffic, b.alias as firm_name from bbd_report_traffic_5m_firm as a join firm as b on a.firm_id=b.id where a.datetime>= and a.datetime<" >/bbd/ssd01/app/logstash-5.2.0/config/sql/sqlTest.txt
```

## mysql to es test
```
0 7 * * * /bbd/ssd01/app/zrk/testmysqlToES.sh > /bbd/ssd01/app/zrk/testmysqlToES.log
```
testmysqlToES.sh
```
. /etc/profile
startDate=`date -d "-1 day" +%Y-%m-%d`

endDate=`date +%Y-%m-%d`
echo $startDate
echo $endDate
startTime=`date -d "$startDate" +%s`
endTime=`date -d "$endDate" +%s`
echo $startTime
echo $endTime

sed -i "s/datetime>=/datetime>=$startTime/g" /bbd/ssd01/app/logstash-5.2.0/config/sql/sqlTest1.sql
sed -i "s/datetime</datetime<$endTime/g" /bbd/ssd01/app/logstash-5.2.0/config/sql/sqlTest1.sql

nohup /bbd/ssd01/app/logstash-5.2.0/bin/logstash -f /bbd/ssd01/app/logstash-5.2.0/config/jdbcEStest.yml &

echo $startTime
echo $endTime
sleep 60s
echo "select a.domain,a.datetime*1000 as from_time,a.traffic, b.alias as firm_name from bbd_report_traffic_5m_firm as a join firm as b on a.firm_id=b.id where a.datetime>= and a.datetime<" >/bbd/ssd01/app/logstash-5.2.0/config/sql/sqlTest1.sql
```
jdbcEStest.yml
```
input {
    jdbc {
      jdbc_connection_string => "jdbc:mysql://rm-m5e4x4v63nhx1gx1eo.mysql.rds.aliyuncs.com:3268/bbd_cdn"
      jdbc_user => "readonly_bigdata"
      jdbc_password => "7ee214e8b094__"
      jdbc_driver_library => "/bbd/ssd01/app/logstash-5.2.0/mysql-connector-java-5.1.26-bin.jar"
      jdbc_driver_class => "com.mysql.jdbc.Driver"
      #jdbc_paging_enabled => "true"
      #jdbc_page_size => "50000"
      statement_filepath => "/bbd/ssd01/app/logstash-5.2.0/config/sql/sqlTest1.sql"
     # schedule => "* 14 * * *"
      type => "jdbc"
    }
}
output {
        elasticsearch {
        hosts => "183.136.128.47:9200"
        index => "zrktestcontacts_%{+YYYY.MM.dd}"
        document_type => "zrktestcontacts"
    }
}
```

## cronAggESToES.sh
```
5 7 * * * /bbd/ssd01/flask/app/cronAggESToES.sh > /bbd/ssd01/flask/app/cronAggESToES.log
```
cronAggESToES.sh
```
startDate=`date -d "-1 day" +%Y.%m.%d`

sourceIndex=statsdata_5m_$startDate
sourceType=bandwidth
resultIndex=aggtmp_$startDate
PYTHON_HOME=/bbd/ssd01/flask/flask
echo $sourceIndex
echo $sourceType
echo $resultIndex
threadNum=`ps -ef|grep aggESToES.py|grep -v grep |wc -l`
echo $threadNum
if [ $threadNum -eq "0" ]
then
        echo "starting"
        $PYTHON_HOME/bin/python /bbd/ssd01/flask/app/aggESToES.py $sourceIndex $sourceType $resultIndex
fi
```
aggESToES.py
```
import hashlib
import json
import time
import sys
import requests
from elasticsearch import Elasticsearch
from elasticsearch import helpers

es = Elasticsearch("183.136.128.47:9200", timeout=1000, sniff_on_start=True, sniff_on_connection_fail=True,
                   sniffer_timeout=60, sniff_timeout=10, max_retries=10)
successMap = {}
bodys1 = {"size":1,"aggs": {"firmNames": { "terms": { "field": "firm_name","size":100000000 },"aggs": {"domains": { "terms": { "field": "domain","size":100000000 } ,  "aggs": {"logDates": { "terms": {"field": "log_date","size":10000000 } ,"aggs": {"traffics": { "sum": { "field": "traffic" }}}}}}}}}}
res = es.search(index=sys.argv[1], doc_type=sys.argv[2], body=json.dumps(bodys1))
#r=requests.post("http://183.136.128.47:9200/statsdata_5m_2017.11.08/bandwidth/_search",json.dumps(bodys1))
#res = json.loads(r.text)
#print(res)
# source=res[]
actions=[]
for hit in res['aggregations']['firmNames']['buckets']:
    #print(hit)
    #hit1=json.loads(hit)
    firm_name=hit["key"]
    for domain1 in hit["domains"]["buckets"]:
        domain=domain1["key"]
       # print(domain)
        for logdate1 in domain1["logDates"]["buckets"]:
            log_date=logdate1["key"]
            traffic=logdate1["traffics"]["value"]
            action = {
                "_op_type": "index",
                "_index": sys.argv[3],
                "_type": "task",
                "_source": {
                    "firm_name": firm_name,
                    "domain": domain,
                    "from_time": log_date,
                    "traffic":traffic
                }
            }
            actions.append(action)
try:
    #print(actions[0])
    print(len(actions))
    helpers.bulk(es, actions)
    print("access task success")
        # return "access task success"
except Exception as err:
    print("access task failed")
```

## 删除jenkins的任务日志
```
29 00 * * * sh /bbd/ssd01/zz/shell/task/delete_jenkins_logs/delete_jenkins_logs.sh
```
delete_jenkins_logs.sh
```
OLD_IFS="$IFS"
IFS=$'\x0A'
for i in `ls /var/lib/jenkins/jobs/`; do find /var/lib/jenkins/jobs/$i/builds/* -mtime +7 -type d | xargs rm -rf; done
```


## rh日志下载异常监控
```
*/5 * * * * /bbd/ssd01/app/zrk/valiLogProcess.sh > /dev/null 2>&1
```
valiLogProcess.sh
```
local_hostname=`hostname`
log_basepath="/bbd/ssd01/zrk/task/log_task"  # 日志根目录
wx_basepath="/bbd/ssd01/zrk/task"
now_month=`date +%Y%m`
now_date=`date +%Y%m%d-%H:%M:%S`
last_month=`date -d "-1 month" +%Y%m` # 上一个月
temp_log_temp="$log_basepath/temp_log_$now_month.log" # 记录log信息，保存一个月
temp_lastmonthlog_temp="$log_basepath/temp_log_$last_month.log" # 拼接将要删除的文件名
temp_log_file="$log_basepath/validownloadLog-watch.log" # 记录log信息，记录一条，判断是否要发送log信息（异常、正常信息）
indexDate=`date -d "-2 hour" +%Y.%m.%d`
indexTime=`date -d "-3 hour" +%s`
indexName=statsdata_5m_$indexDate

#echo $indexName
#echo $indexTime
# 日志临时记录文件处理
if [[ ! -d $log_basepath ]]; then # 文件夹判断是否存在
    mkdir -p $log_basepath
    touch $temp_log_file
elif [[ ! -f $temp_log_file ]]; then # 判断文件是否存在
    touch $temp_log_file
fi
if [[ -f $temp_lastmonthlog_temp ]]; then # 删除上个月的记录
        rm -f $temp_lastmonthlog_temp
fi


error_count_temp=`fgrep bbd $temp_log_file | wc -l`

#hbase_count=`ps -ef|grep HMaster |grep -v grep |wc -l`
#hbase_port=`netstat -ntulp | grep 60010 | wc -l`
echo $error_count_temp
#/bbd/ssd01/flask/flask/bin/python /bbd/ssd01/flask/app/getMissLog.py $indexName $indexTime
/bbd/ssd01/app/zrk/base_sshfr.sh "find /bbd/sata{01..12}/firm_logs/ -amin -180 |head" /bbd/ssd01/app/zrk/flume >$temp_log_file
error_count_now=`fgrep bbd $temp_log_file | wc -l`
echo $error_count_now
if [[ ! $error_count_now -gt 0 ]]; then
    error_message="[ERROR] 日期：$now_date 日志下载程序三小时内未下载任何文件，下载程序可能有异常"
    if [[  $error_count_temp -lt 1 ]]; then # 有异常，而且上次定时任务记录的log是正常的（上次就是异常不再重复发送）
        #echo "XXXXXXXXXXXXXXXXXXXX"
        #python $wx_basepath/wx.py "$error_message" # 调用python脚本发送信息
        echo $error_message
    fi
    #echo $error_message
    echo `date`" [ERROR] $error_message bbd" > $temp_log_file # 记录单条的文件
    echo `date`" [ERROR] $error_message bbd" >> $temp_log_temp # 保存一个月的log信息
else
 #   me1=`cat $temp_log_file`
  #  echo $me1
    ok_message="[INFO] 日期：$now_date 融合日志下载已恢复"
    if [[ $error_count_temp -gt 0 ]]; then # 监控正常，但是定时执行记录的log是异常信息，发送信息（有异常转入正常）
        #echo "VVVVVVVVVVVVVVVVVVVVVVVVVVVVVV"
        #python $wx_basepath/wx.py "$ok_message"
        echo $ok_message
    fi
    #echo $ok_message
    echo `date`" [OK] $ok_message" > $temp_log_file
    echo `date`" [OK] $ok_message" >> $temp_log_temp
fi
```
base_sshfr.sh
```
ips=$2
proc=$1
#descfile=$2

for ip in `cat $ips`
do
        echo $ip
        ssh -p23432 $ip $proc
        #ssh -p23432 $ip $proc &
done
```

# 142 crontab
## 时钟、hadoop日志删除
```
29 * * * * /usr/sbin/ntpdate ntp1.cachecn.net ntp.api.bz >/dev/null 2>&1 ;/sbin/hwclock -w 
* 3 * * * /bin/find /var/log/hadoop-hdfs/ -mtime +2 -name "*.log*" -exec rm -rf {} \;
* 3 * * * /bin/find /var/log/hadoop-*/ -name "hdfs-audit.log.*" -exec rm -rf {} \;
```

## 自建表创建
00 01 * * * sh /bbd/ssd01/app/zrk/createJXtable.sh > /dev/null 2>&1
```
#!/bin/bash
prefix=jx
DatePre=31
DateAgo=60
hbasePath=/opt/cloudera/parcels/CDH/bin
datePre=`date -d "+$DatePre day" +%Y%m%d`
dateAgo=`date -d "-$DateAgo day" +%Y%m%d`
echo $dateAgo
tablename=$prefix$datePre
echo $tablename
`$hbasePath/hbase shell <<-EOF
create "$tablename",{METADATA => {'SPLIT_POLICY' => 'org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy'}},{NAME => 'cf',COMPRESSION => 'SNAPPY' },{SPLITS => ['0','1','2','3','4','5','6','7','8','9','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']}
disable "$prefix$dateAgo"
drop "$prefix$dateAgo"
quit
EOF` >/dev/null 2>&1
```

## 融合表创建
00 02 * * * sh /bbd/ssd01/app/zrk/createRHtable.sh >/dev/null 2>&1
```
#!/bin/bash
prefix=nginx
DatePre=31
DateAgo=60
hbasePath=/bbd/sata02/parcels/CDH-5.8.3-1.cdh5.8.3.p0.2/bin
datePre=`date -d "+$DatePre day" +%Y%m%d`
dateAgo=`date -d "-$DateAgo day" +%Y%m%d`
echo $dateAgo
tablename=$prefix$datePre
echo $tablename
`$hbasePath/hbase shell <<-EOF
create "$tablename",{NAME => 'cf',COMPRESSION => 'SNAPPY' },{SPLITS => ['0','1','2','3','4','5','6','7','8','9']}
disable "$prefix$dateAgo"
drop "$prefix$dateAgo"
quit
EOF` >/dev/null 2>&1
```

## hbase进程监控
```
*/5 * * * * sh /bbd/ssd01/zrk/task/hbase-watch.sh > /dev/null 2>&1
```
hbase-watch.sh
```
#!/bin/bash
# 162 - 166 hbase进程、端口监控
# */5 * * * * sh /bbd/sata01/shell/task/hbase-watch.sh > /dev/null 2>&1
local_hostname=`hostname`
log_basepath="/bbd/ssd01/zrk/task/log_task"  # 日志根目录
wx_basepath="/bbd/ssd01/zrk/task"
now_month=`date +%Y%m`
now_date=`date +%Y%m%d-%H:%M:%S`
last_month=`date -d "-1 month" +%Y%m` # 上一个月
temp_log_temp="$log_basepath/temp_log_$now_month.log" # 记录log信息，保存一个月
temp_lastmonthlog_temp="$log_basepath/temp_log_$last_month.log" # 拼接将要删除的文件名
temp_log_file="$log_basepath/hbase-watch.log" # 记录log信息，记录一条，判断是否要发送log信息（异常、正常信息）

# 日志临时记录文件处理
if [[ ! -d $log_basepath ]]; then # 文件夹判断是否存在
    mkdir -p $log_basepath
    touch $temp_log_file
elif [[ ! -f $temp_log_file ]]; then # 判断文件是否存在
    touch $temp_log_file
fi
if [[ -f $temp_lastmonthlog_temp ]]; then # 删除上个月的记录
	rm -f $temp_lastmonthlog_temp
fi


error_count_temp=`fgrep ERROR $temp_log_file | wc -l`
hbase_count=`ps -ef|grep RegionServer |grep -v grep |wc -l`
hbase_port=`netstat -ntulp | grep 60030 | wc -l`

if [[ !$hbase_count -gt 0 ]] || [[ !$hbase_port -gt 0 ]]; then # 没有hbase进程，端口没有开启
    error_message="[WARN] 日期：$now_date 主机名：$local_hostname 进程名： RegionServer 状态：dead"
    if [[ $error_count_temp -lt 1 ]]; then # 有异常，而且上次定时任务记录的log是正常的（上次就是异常不再重复发送）
        #echo "XXXXXXXXXXXXXXXXXXXX"
        python $wx_basepath/wx.py "$error_message" # 调用python脚本发送信息
        #echo $error_message
    fi
    #echo $error_message
    echo `date`" [ERROR] $error_message" > $temp_log_file # 记录单条的文件
    echo `date`" [ERROR] $error_message" >> $temp_log_temp # 保存一个月的log信息
else
    ok_message="[INFO] 日期：$now_date 主机名：$local_hostname 进程名： RegionServer 状态：ok"
    if [[ $error_count_temp -gt 0 ]]; then # 监控正常，但是定时执行记录的log是异常信息，发送信息（有异常转入正常）
        #echo "VVVVVVVVVVVVVVVVVVVVVVVVVVVVVV"
        python $wx_basepath/wx.py "$ok_message"
        #echo $ok_message
    fi
    #echo $ok_message
    echo `date`" [OK] $ok_message" > $temp_log_file
    echo `date`" [OK] $ok_message" >> $temp_log_temp
fi
```

# 145 crontab
## 快手统计runIspProv
```
*/5 * * * * sh  /bbd/sata09/bigdata/sinobbd/task/kshttpstat/runIspProv.sh &
```
```
# start
SELF_NAME=`basename $0`
PID_FILE=/var/run/$SELF_NAME.pid
# running lock 
if ([ -f $PID_FILE ] && kill -0 `cat $PID_FILE` 2>/dev/null); then
#        if grep -q $SELF_NAME /proc/`cat $PID_FILE 2>/dev/null`/cmdline; then
                echo "$SELF_NAME already is running... "
                exit 9
        fi
#fi
echo $$ >$PID_FILE
trap "rm -f $PID_FILE; exit 1" 2

# body
/bbd/sata09/bigdata/sinobbd/task/band/jdk1.8.0_111/bin/java -jar  /bbd/sata09/bigdata/sinobbd/task/kshttpstat/KsHttpStatISPProv.jar 1>>/bbd/sata09/bigdata/sinobbd/task/kshttpstat/logs/ispProvtask`date -d today +%Y%m%d`.log  2>>/bbd/sata09/bigdata/sinobbd/task/kshttpstat/logs/errorIspProv.log &
echo start running...
sleep 10

# end
rm -f $PID_FILE; exit 0
```

## 快手统计Isp
```
*/5 * * * * sh /bbd/sata09/bigdata/sinobbd/task/kshttpstat/runIsp.sh &
```
```
# start
SELF_NAME=`basename $0`
PID_FILE=/var/run/$SELF_NAME.pid
# running lock 
if ([ -f $PID_FILE ] && kill -0 `cat $PID_FILE` 2>/dev/null); then
#        if grep -q $SELF_NAME /proc/`cat $PID_FILE 2>/dev/null`/cmdline; then
                echo "$SELF_NAME already is running... "
                exit 9
#        fi
fi
echo $$ >$PID_FILE
trap "rm -f $PID_FILE; exit 1" 2

# body
/bbd/sata09/bigdata/sinobbd/task/band/jdk1.8.0_111/bin/java -jar  /bbd/sata09/bigdata/sinobbd/task/kshttpstat/KsHttpStatISP.jar 1>>/bbd/sata09/bigdata/sinobbd/task/kshttpstat/logs/ispTask`date -d today +%Y%m%d`.log  2>>/bbd/sata09/bigdata/sinobbd/task/kshttpstat/logs/errorIsp.log &
echo start running...
sleep 10

# end
rm -f $PID_FILE; exit 0
```

## 快手统计Total
```
*/5 * * * * sh /bbd/sata09/bigdata/sinobbd/task/kshttpstat/runTotal.sh &
```
```
# start
SELF_NAME=`basename $0`
PID_FILE=/var/run/$SELF_NAME.pid
# running lock 
if ([ -f $PID_FILE ] && kill -0 `cat $PID_FILE` 2>/dev/null); then
#        if grep -q $SELF_NAME /proc/`cat $PID_FILE 2>/dev/null`/cmdline; then
                echo "$SELF_NAME already is running... "
                exit 9
 #       fi
fi
echo $$ >$PID_FILE
trap "rm -f $PID_FILE; exit 1" 2

# body
/bbd/sata09/bigdata/sinobbd/task/band/jdk1.8.0_111/bin/java -jar  /bbd/sata09/bigdata/sinobbd/task/kshttpstat/KsHttpStatTot.jar 1>>/bbd/sata09/bigdata/sinobbd/task/kshttpstat/logs/ispTotaltask`date -d today +%Y%m%d`.log  2>>/bbd/sata09/bigdata/sinobbd/task/kshttpstat/logs/errorTotal.log &
echo start running...
sleep 10

# end
rm -f $PID_FILE; exit 0
```


## 按月带宽统计 run5mto1h
```
10 */1 * * * sh  /bbd/sata09/bigdata/sinobbd/task/band5mto1h/run5mto1h &
```
```
# start
SELF_NAME=`basename $0`
PID_FILE=/var/run/$SELF_NAME.pid
# running lock 
if ([ -f $PID_FILE ] && kill -0 `cat $PID_FILE` 2>/dev/null); then
       # if grep -q $SELF_NAME /proc/`cat $PID_FILE 2>/dev/null`/cmdline; then
                echo "$SELF_NAME already is running... "
                exit 9
        #fi
fi
echo $$ >$PID_FILE
trap "rm -f $PID_FILE; exit 1" 2

# body
/bbd/sata09/bigdata/sinobbd/task/band/jdk1.8.0_111/bin/java -jar  /bbd/sata09/bigdata/sinobbd/task/band5mto1h/Bandwidth5mTo1h.jar 1>>/bbd/sata09/bigdata/sinobbd/task/band5mto1h/logs/task`date -d today +%Y%m%d`.log  2>>/bbd/sata09/bigdata/sinobbd/task/band5mto1h/logs/error.log &
echo start running...
sleep 10

# end
rm -f $PID_FILE; exit 0
```


## 按月带宽统计 run5mto1h_fcdn
```
20 */1 * * * sh  /bbd/sata09/bigdata/sinobbd/task/band5mto1h_fcdn/run5mto1h_fcdn &
```
```
# start
SELF_NAME=`basename $0`
PID_FILE=/var/run/$SELF_NAME.pid
# running lock 
if ([ -f $PID_FILE ] && kill -0 `cat $PID_FILE` 2>/dev/null); then
       # if grep -q $SELF_NAME /proc/`cat $PID_FILE 2>/dev/null`/cmdline; then
                echo "$SELF_NAME already is running... "
                exit 9
       # fi
fi
echo $$ >$PID_FILE
trap "rm -f $PID_FILE; exit 1" 2

# body
/bbd/sata09/bigdata/sinobbd/task/band/jdk1.8.0_111/bin/java -jar  /bbd/sata09/bigdata/sinobbd/task/band5mto1h_fcdn/Bandwidth5mTo1h.jar 1>>/bbd/sata09/bigdata/sinobbd/task/band5mto1h_fcdn/logs/task`date -d today +%Y%m%d`.log  2>>/bbd/sata09/bigdata/sinobbd/task/band5mto1h_fcdn/logs/error.log &
echo start running...
sleep 10

# end
rm -f $PID_FILE; exit 0
```


## 按月带宽统计 redoRun5mto1h
```
30 */1 * * * sh  /bbd/sata09/bigdata/sinobbd/task/redoBand5mto1h/redoRun5mto1h &
```
```
# start
SELF_NAME=`basename $0`
PID_FILE=/var/run/$SELF_NAME.pid
# running lock 
if ([ -f $PID_FILE ] && kill -0 `cat $PID_FILE` 2>/dev/null); then
       # if grep -q $SELF_NAME /proc/`cat $PID_FILE 2>/dev/null`/cmdline; then
                echo "$SELF_NAME already is running... "
                exit 9
       # fi
fi
echo $$ >$PID_FILE
trap "rm -f $PID_FILE; exit 1" 2

# body
/bbd/sata09/bigdata/sinobbd/task/band/jdk1.8.0_111/bin/java -jar  /bbd/sata09/bigdata/sinobbd/task/band5mto1h_fcdn/Bandwidth5mTo1h.jar 1>>/bbd/sata09/bigdata/sinobbd/task/band5mto1h_fcdn/logs/task`date -d today +%Y%m%d`.log  2>>/bbd/sata09/bigdata/sinobbd/task/band5mto1h_fcdn/logs/error.log &
echo start running...
sleep 10

# end
rm -f $PID_FILE; exit 0
```





# 54.135 crontab
## 合并es分片
```
1 1 * * * /usr/bin/curator /bbd/ssd01/es-tools/merge_indices.yml >>/bbd/ssd01/es-tools/mergeinfo.log 2>&1
```
```
actions:
  1:
    action: forcemerge
    description: "Perform a forceMerge on selected indices to 'max_num_segments' per shard"
    options:
      max_num_segments: 2
      delay: 120
      timeout_override:
      continue_if_exception: False
      disable_action: False
    filters:
    - filtertype: opened
      exclude: False
    - filtertype: age
      source: creation_date
      #source: name
      direction: older
      #timestring: '%Y%m%d'
      unit: days
      unit_count: 2
    - filtertype: age
      source: creation_date
      direction: younger
      #timestring: '%Y%m%d'
      unit: days
      unit_count: 3
    - filtertype: pattern
      kind: prefix
      exclude: true
      value: .monitor
```


## 修改es路由
```
55 2 * * * /bbd/ssd01/es-tools/relocate.sh >>/bbd/ssd01/es-tools/relocateinfo.log 2>&1
```
```
rh_date=`date -d '-2days' '+%Y%m%d'`

curl -XPUT http://esclient.bbdcdn.net:9200/nginx-*-${rh_date}/_settings -d '
    {
      "index.routing.allocation.include.tag" : "hot,raid5"
    }'

self_date=`date -d '-1days' '+%Y.%m.%d'`
curl -XPUT http://esclient.bbdcdn.net:9200/nginx-${self_date}/_settings -d '
    {
      "index.routing.allocation.include.tag" : "hot,raid5"
    }'

st_date=`date -d '-1days' '+%Y.%m.%d'`
sm_date=`date -d '-1days' '+%Y.%m'`

curl -XPUT http://esclient.bbdcdn.net:9200/statsdata_*_${st_date}/_settings -d '
    {
      "index.routing.allocation.include.tag" : "hot,raid5"
    }'

curl -XPUT http://esclient.bbdcdn.net:9200/statsdata_h_${sm_date}/_settings -d '
    {
      "index.routing.allocation.include.tag" : "hot,raid5"
    }'
```

## 副本数有1变成2
```
1 3 * * * /usr/bin/curator /bbd/ssd01/es-tools/replica.yml >>/bbd/ssd01/es-tools/replicainfo.log 2>&1
```
```
actions:
  1:
    action: replicas
    description: "Set the number of replicas per shard for selected indices"
    options:
      count: 1
      wait_for_completion: False
      timeout_override:
      continue_if_exception: False
      disable_action: False
    filters:
    - filtertype: opened
      exclude: False
    - filtertype: age
      source: creation_date
      #source: name
      direction: older
      #timestring: '%Y%m%d'
      unit: days
      unit_count: 1
    - filtertype: age
      source: creation_date
      direction: younger
      #timestring: '%Y%m%d'
      unit: days
      unit_count: 2
    - filtertype: pattern
      kind: prefix
      value: nginx-20
```


## 删除es中.monitoring索引
```
1 9 * * * /usr/bin/curator /bbd/ssd01/es-tools/del_indices.yml >>/bbd/ssd01/es-tools/delinfo.log 2>&1
```
```
actions:
  1:
    action: delete_indices
    description: >-
      delete indices older than 2 days (based on index
      name), for logstash- prefixed indices.
    options:
      ignore_empty_list: True
      timeout_override:
      continue_if_exception: False
      disable_action: False
    filters:
    - filtertype: pattern
      kind: prefix
      value: .monitoring-es-2-*
      exclude:
    - filtertype: age
      source: name
      direction: older
      timestring: '%Y.%m.%d'
      unit: days
      unit_count: 2
      exclude:
  2:
    action: delete_indices
    description: >-
      delete indices older than 2 days (based on index
      name), for logstash- prefixed indices.
    options:
      ignore_empty_list: True
      timeout_override:
      continue_if_exception: False
      disable_action: False
    filters:
    - filtertype: pattern
      kind: prefix
      value: .monitoring-kibana-2-*
      exclude:
    - filtertype: age
      source: name
      direction: older
      timestring: '%Y.%m.%d'
      unit: days
      unit_count: 2
      exclude:
```

## 删除快手索引
```
1 10 * * * /usr/bin/curator /bbd/ssd01/es-tools/del_indices_ksfilelog.yml >>/bbd/ssd01/es-tools/delinfo.log 2>&1
```
```
actions:
  1:
    action: delete_indices
    description: >-
     delete indices older than 1 months (based on creation_date), for ksfilelog- prefixed indices.
    options:
      ignore_empty_list: True
      timeout_override:
      continue_if_exception: False
      disable_action: False
    filters:
    - filtertype: pattern
      kind: prefix
      value: ksfilelog-
      exclude:
    - filtertype: age
      source: creation_date
      direction: older
      unit: months
      unit_count: 1
```

## 删除jx、rh索引
```
1 9 */2 * * /usr/bin/curator /bbd/ssd01/es-tools/del_indices_nginx.yml >>/bbd/ssd01/es-tools/delnginxinfo.log 2>&1
```
```
actions:
  1:
    action: close
    description: >-
      close indices older than 30 days (based on creation_date), for nginx- prefixed indices.
    options:
      ignore_empty_list: True
      timeout_override:
      continue_if_exception: False
      disable_action: False
    filters:
    - filtertype: opened
      exclude: False
    - filtertype: pattern
      kind: prefix
      value: nginx-
      exclude:
    - filtertype: age
      source: creation_date
      direction: older
      unit: days
      unit_count: 30
  2:
    action: delete_indices
    description: >-
     delete indices older than 6 months (based on creation_date), for nginx- prefixed indices.
    options:
      ignore_empty_list: True
      timeout_override:
      continue_if_exception: False
      disable_action: False
    filters:
    - filtertype: closed
      exclude: False
    - filtertype: pattern
      kind: prefix
      value: nginx-
      exclude:
    - filtertype: age
      source: creation_date
      direction: older
      unit: days
      unit_count: 180
```


# 微信报警
## wx.py
```
#!/usr/bin/env python 
#coding: utf-8
import time
import urllib,urllib2,requests
import json
import sys
import argparse

reload(sys)
sys.setdefaultencoding("utf-8")

class WeChatMSG(object):
    def __init__(self,content):
        self.gettoken_url = 'https://qyapi.weixin.qq.com/cgi-bin/gettoken'
        self.gettoken_content = {
                            'corpid' : 'ww6f4afe29f8a62102',
                            'corpsecret' : 'J-ZTRxt3yOHUsQkk-dow3r6Uw28on1HxMZNOxt7iAag' ,
                            }
        self.main_content = {
                            "toparty":"3",
                            "agentid":"1000003",
                            "msgtype": "text",
                            "text":{
                            "content":content,
                                    }
                            }

    def get_access_token(self,string):
        token_result = json.loads(string.read())
        access_token=  token_result['access_token']
        return access_token.encode('utf-8')
    def geturl(self,url,data):
        data = self.encodeurl(data)
        response = urllib2.urlopen('%s?%s' % (url,data))
        return response.read().decode('utf-8')

    def posturl(self,url,data,isjson = True):
        if isjson:
            data = json.dumps(data,ensure_ascii=False)
        response = urllib2.urlopen(url,data)
        return response.read().decode('utf-8')
    def encodeurl(self,dict):
        data = ''
        for k,v in dict.items():
            data += '%s=%s%s' % (k,v,'&')
        return data
def send_msg(content):
    msgsender = WeChatMSG(content)
    access_token_response = msgsender.geturl(msgsender.gettoken_url, msgsender.gettoken_content)
    access_token =  json.loads(access_token_response)['access_token']
    sendmsg_url = 'https://qyapi.weixin.qq.com/cgi-bin/message/send?access_token=%s' % access_token
    print msgsender.posturl(sendmsg_url,msgsender.main_content)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument("message")
    args = parser.parse_args()
    send_msg(args.message.replace("\\n","\n"))
    #send_msg(args.message)
```

## zz.py
```
#!/usr/bin/env python 
#coding: utf-8
import time
import urllib,urllib2,requests
import json
import sys
import argparse

reload(sys)
sys.setdefaultencoding("utf-8")

class WeChatMSG(object):
    def __init__(self,content):
        self.gettoken_url = 'https://qyapi.weixin.qq.com/cgi-bin/gettoken'
        self.gettoken_content = {
                            'corpid' : 'ww31c2923479e2aa50',
                            'corpsecret' : 'xFO937EFvI32Y4U9AmPtPSfYU1v4c-hsvhOnK9q_9V0' ,
                            }
        self.main_content = {
                            "toparty":"2",
                            "agentid":"1000002",
                            "msgtype": "text",
                            "text":{
                            "content":content,
                                    }
                            }

    def get_access_token(self,string):
        token_result = json.loads(string.read())
        access_token=  token_result['access_token']
        return access_token.encode('utf-8')
    def geturl(self,url,data):
        data = self.encodeurl(data)
        response = urllib2.urlopen('%s?%s' % (url,data))
        return response.read().decode('utf-8')

    def posturl(self,url,data,isjson = True):
        if isjson:
            data = json.dumps(data,ensure_ascii=False)
        response = urllib2.urlopen(url,data)
        return response.read().decode('utf-8')
    def encodeurl(self,dict):
        data = ''
        for k,v in dict.items():
            data += '%s=%s%s' % (k,v,'&')
        return data
def send_msg(content):
    msgsender = WeChatMSG(content)
    access_token_response = msgsender.geturl(msgsender.gettoken_url, msgsender.gettoken_content)
    access_token =  json.loads(access_token_response)['access_token']
    sendmsg_url = 'https://qyapi.weixin.qq.com/cgi-bin/message/send?access_token=%s' % access_token
    print msgsender.posturl(sendmsg_url,msgsender.main_content)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument("message")
    args = parser.parse_args()
    send_msg(args.message.replace("\\n","\n"))
    #send_msg(args.message)

```
## zz2.py
```
#!/usr/bin/env python 
#coding: utf-8
import time
import urllib,urllib2,requests
import json
import sys
import argparse

reload(sys)
sys.setdefaultencoding("utf-8")

class WeChatMSG(object):
    def __init__(self,content):
        self.gettoken_url = 'https://qyapi.weixin.qq.com/cgi-bin/gettoken'
        self.gettoken_content = {
                            'corpid' : 'ww31c2923479e2aa50',
                            'corpsecret' : 'NK4pKCERslX2BTJKs0v1xplG55JU70rGiRfq3_oKwUM' ,
                            }
        self.main_content = {
                            "toparty":"3",
                            "agentid":"1000003",
                            "msgtype": "text",
                            "text":{
                            "content":content,
                                    }
                            }

    def get_access_token(self,string):
        token_result = json.loads(string.read())
        access_token=  token_result['access_token']
        return access_token.encode('utf-8')
    def geturl(self,url,data):
        data = self.encodeurl(data)
        response = urllib2.urlopen('%s?%s' % (url,data))
        return response.read().decode('utf-8')

    def posturl(self,url,data,isjson = True):
        if isjson:
            data = json.dumps(data,ensure_ascii=False)
        response = urllib2.urlopen(url,data)
        return response.read().decode('utf-8')
    def encodeurl(self,dict):
        data = ''
        for k,v in dict.items():
            data += '%s=%s%s' % (k,v,'&')
        return data
def send_msg(content):
    msgsender = WeChatMSG(content)
    access_token_response = msgsender.geturl(msgsender.gettoken_url, msgsender.gettoken_content)
    access_token =  json.loads(access_token_response)['access_token']
    sendmsg_url = 'https://qyapi.weixin.qq.com/cgi-bin/message/send?access_token=%s' % access_token
    print msgsender.posturl(sendmsg_url,msgsender.main_content)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument("message")
    args = parser.parse_args()
    send_msg(args.message.replace("\\n","\n"))
```
