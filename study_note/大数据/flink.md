# 一、下载安装

## 1. 地址

官网地址：

https://flink.apache.org/downloads.html

### Binaries

|                                     | Scala 2.11                                                   | Scala 2.12                                                   |
| ----------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Apache Flink 1.7.1 only             | [Download](https://www.apache.org/dyn/closer.lua/flink/flink-1.7.1/flink-1.7.1-bin-scala_2.11.tgz) ([asc](https://www.apache.org/dist/flink/flink-1.7.1/flink-1.7.1-bin-scala_2.11.tgz.asc), [sha512](https://www.apache.org/dist/flink/flink-1.7.1/flink-1.7.1-bin-scala_2.11.tgz.sha512)) | [Download](https://www.apache.org/dyn/closer.lua/flink/flink-1.7.1/flink-1.7.1-bin-scala_2.12.tgz) ([asc](https://www.apache.org/dist/flink/flink-1.7.1/flink-1.7.1-bin-scala_2.12.tgz.asc), [sha512](https://www.apache.org/dist/flink/flink-1.7.1/flink-1.7.1-bin-scala_2.12.tgz.sha512)) |
| Apache Flink 1.7.1 with Hadoop® 2.8 | [Download](https://www.apache.org/dyn/closer.lua/flink/flink-1.7.1/flink-1.7.1-bin-hadoop28-scala_2.11.tgz) ([asc](https://www.apache.org/dist/flink/flink-1.7.1/flink-1.7.1-bin-hadoop28-scala_2.11.tgz.asc), [sha512](https://www.apache.org/dist/flink/flink-1.7.1/flink-1.7.1-bin-hadoop28-scala_2.11.tgz.sha512)) | [Download](https://www.apache.org/dyn/closer.lua/flink/flink-1.7.1/flink-1.7.1-bin-hadoop28-scala_2.12.tgz) ([asc](https://www.apache.org/dist/flink/flink-1.7.1/flink-1.7.1-bin-hadoop28-scala_2.12.tgz.asc), [sha512](https://www.apache.org/dist/flink/flink-1.7.1/flink-1.7.1-bin-hadoop28-scala_2.12.tgz.sha512)) |
| Apache Flink 1.7.1 with Hadoop® 2.7 | [Download](https://www.apache.org/dyn/closer.lua/flink/flink-1.7.1/flink-1.7.1-bin-hadoop27-scala_2.11.tgz) ([asc](https://www.apache.org/dist/flink/flink-1.7.1/flink-1.7.1-bin-hadoop27-scala_2.11.tgz.asc), [sha512](https://www.apache.org/dist/flink/flink-1.7.1/flink-1.7.1-bin-hadoop27-scala_2.11.tgz.sha512)) | [Download](https://www.apache.org/dyn/closer.lua/flink/flink-1.7.1/flink-1.7.1-bin-hadoop27-scala_2.12.tgz) ([asc](https://www.apache.org/dist/flink/flink-1.7.1/flink-1.7.1-bin-hadoop27-scala_2.12.tgz.asc), [sha512](https://www.apache.org/dist/flink/flink-1.7.1/flink-1.7.1-bin-hadoop27-scala_2.12.tgz.sha512)) |
| Apache Flink 1.7.1 with Hadoop® 2.6 | [Download](https://www.apache.org/dyn/closer.lua/flink/flink-1.7.1/flink-1.7.1-bin-hadoop26-scala_2.11.tgz) ([asc](https://www.apache.org/dist/flink/flink-1.7.1/flink-1.7.1-bin-hadoop26-scala_2.11.tgz.asc), [sha512](https://www.apache.org/dist/flink/flink-1.7.1/flink-1.7.1-bin-hadoop26-scala_2.11.tgz.sha512)) | [Download](https://www.apache.org/dyn/closer.lua/flink/flink-1.7.1/flink-1.7.1-bin-hadoop26-scala_2.12.tgz) ([asc](https://www.apache.org/dist/flink/flink-1.7.1/flink-1.7.1-bin-hadoop26-scala_2.12.tgz.asc), [sha512](https://www.apache.org/dist/flink/flink-1.7.1/flink-1.7.1-bin-hadoop26-scala_2.12.tgz.sha512)) |
| Apache Flink 1.7.1 with Hadoop® 2.4 | [Download](https://www.apache.org/dyn/closer.lua/flink/flink-1.7.1/flink-1.7.1-bin-hadoop24-scala_2.11.tgz) ([asc](https://www.apache.org/dist/flink/flink-1.7.1/flink-1.7.1-bin-hadoop24-scala_2.11.tgz.asc), [sha512](https://www.apache.org/dist/flink/flink-1.7.1/flink-1.7.1-bin-hadoop24-scala_2.11.tgz.sha512)) | [Download](https://www.apache.org/dyn/closer.lua/flink/flink-1.7.1/flink-1.7.1-bin-hadoop24-scala_2.12.tgz) ([asc](https://www.apache.org/dist/flink/flink-1.7.1/flink-1.7.1-bin-hadoop24-scala_2.12.tgz.asc), [sha512](https://www.apache.org/dist/flink/flink-1.7.1/flink-1.7.0-bin-hadoop24-scala_2.12.tgz.sha512)) |
| Apache Flink 1.6.3 only             | [Download](https://www.apache.org/dyn/closer.lua/flink/flink-1.6.3/flink-1.6.3-bin-scala_2.11.tgz) ([asc](https://www.apache.org/dist/flink/flink-1.6.3/flink-1.6.3-bin-scala_2.11.tgz.asc), [sha512](https://www.apache.org/dist/flink/flink-1.6.3/flink-1.6.3-bin-scala_2.11.tgz.sha512)) | Not supported.                                               |
| Flink 1.6.3 with Hadoop® 2.8        | [Download](https://www.apache.org/dyn/closer.lua/flink/flink-1.6.3/flink-1.6.3-bin-hadoop28-scala_2.11.tgz) ([asc](https://www.apache.org/dist/flink/flink-1.6.3/flink-1.6.3-bin-hadoop28-scala_2.11.tgz.asc), [sha512](https://www.apache.org/dist/flink/flink-1.6.3/flink-1.6.3-bin-hadoop28-scala_2.11.tgz.sha512)) | Not supported.                                               |
| Flink 1.6.3 with Hadoop® 2.7        | [Download](https://www.apache.org/dyn/closer.lua/flink/flink-1.6.3/flink-1.6.3-bin-hadoop27-scala_2.11.tgz) ([asc](https://www.apache.org/dist/flink/flink-1.6.3/flink-1.6.3-bin-hadoop27-scala_2.11.tgz.asc), [sha512](https://www.apache.org/dist/flink/flink-1.6.3/flink-1.6.3-bin-hadoop27-scala_2.11.tgz.sha512)) | Not supported.                                               |
| Flink 1.6.3 with Hadoop® 2.6        | [Download](https://www.apache.org/dyn/closer.lua/flink/flink-1.6.3/flink-1.6.3-bin-hadoop26-scala_2.11.tgz) ([asc](https://www.apache.org/dist/flink/flink-1.6.3/flink-1.6.3-bin-hadoop26-scala_2.11.tgz.asc), [sha512](https://www.apache.org/dist/flink/flink-1.6.3/flink-1.6.3-bin-hadoop26-scala_2.11.tgz.sha512)) | Not supported.                                               |
| Flink 1.6.3 with Hadoop® 2.4        | [Download](https://www.apache.org/dyn/closer.lua/flink/flink-1.6.3/flink-1.6.3-bin-hadoop24-scala_2.11.tgz) ([asc](https://www.apache.org/dist/flink/flink-1.6.3/flink-1.6.3-bin-hadoop24-scala_2.11.tgz.asc), [sha512](https://www.apache.org/dist/flink/flink-1.6.3/flink-1.6.3-bin-hadoop24-scala_2.11.tgz.sha512)) | Not supported.                                               |
| Apache 1.5.6 Flink only             | [Download](https://www.apache.org/dyn/closer.lua/flink/flink-1.5.6/flink-1.5.6-bin-scala_2.11.tgz) ([asc](https://www.apache.org/dist/flink/flink-1.5.6/flink-1.5.6-bin-scala_2.11.tgz.asc), [sha512](https://www.apache.org/dist/flink/flink-1.5.6/flink-1.5.6-bin-scala_2.11.tgz.sha512)) | Not supported.                                               |
| Flink 1.5.6 with Hadoop® 2.8        | [Download](https://www.apache.org/dyn/closer.lua/flink/flink-1.5.6/flink-1.5.6-bin-hadoop28-scala_2.11.tgz) ([asc](https://www.apache.org/dist/flink/flink-1.5.6/flink-1.5.6-bin-hadoop28-scala_2.11.tgz.asc), [sha512](https://www.apache.org/dist/flink/flink-1.5.6/flink-1.5.6-bin-hadoop28-scala_2.11.tgz.sha512)) | Not supported.                                               |
| Flink 1.5.6 with Hadoop® 2.7        | [Download](https://www.apache.org/dyn/closer.lua/flink/flink-1.5.6/flink-1.5.6-bin-hadoop27-scala_2.11.tgz) ([asc](https://www.apache.org/dist/flink/flink-1.5.6/flink-1.5.6-bin-hadoop27-scala_2.11.tgz.asc), [sha512](https://www.apache.org/dist/flink/flink-1.5.6/flink-1.5.6-bin-hadoop27-scala_2.11.tgz.sha512)) | Not supported.                                               |
| Flink 1.5.6 with Hadoop® 2.6        | [Download](https://www.apache.org/dyn/closer.lua/flink/flink-1.5.6/flink-1.5.6-bin-hadoop26-scala_2.11.tgz) ([asc](https://www.apache.org/dist/flink/flink-1.5.6/flink-1.5.6-bin-hadoop26-scala_2.11.tgz.asc), [sha512](https://www.apache.org/dist/flink/flink-1.5.6/flink-1.5.6-bin-hadoop26-scala_2.11.tgz.sha512)) | Not supported.                                               |
| Flink 1.5.6 with Hadoop® 2.4        | [Download](https://www.apache.org/dyn/closer.lua/flink/flink-1.5.6/flink-1.5.6-bin-hadoop24-scala_2.11.tgz) ([asc](https://www.apache.org/dist/flink/flink-1.5.6/flink-1.5.6-bin-hadoop24-scala_2.11.tgz.asc), [sha512](https://www.apache.org/dist/flink/flink-1.5.6/flink-1.5.6-bin-hadoop24-scala_2.11.tgz.sha512)) | Not supported.                                               |

## 2. 安装

> Apache Flink 1.7.1 only 是不依赖hadoop的
>
> Apache Flink 1.7.1 with Hadoop 为整合hadoop的

```
tar -zxvf flink-1.7.1-bin-scala_2.11.tgz

vim /etc/profile
# flink
export FLINK_HOME=/zz/app/flink
export PATH=$FLINK_HOME/bin:$PATH

vim conf/masters
master

vim conf/slaves
work1
work2

vim conf/flink-conf.yaml
jobmanager.rpc.address: master
taskmanager.numberOfTaskSlots: 2
```



#### ZooKeeper-based HA Mode

| Key                                                   | Default                  | Description                                                  |
| ----------------------------------------------------- | ------------------------ | ------------------------------------------------------------ |
| high-availability.zookeeper.client.acl                | "open"                   | Defines the ACL (open\|creator) to be configured on ZK node. The configuration value can be set to “creator” if the ZooKeeper server configuration has the “authProvider” property mapped to use SASLAuthenticationProvider and the cluster is configured to run in secure mode (Kerberos). |
| high-availability.zookeeper.client.connection-timeout | 15000                    | Defines the connection timeout for ZooKeeper in ms.          |
| high-availability.zookeeper.client.max-retry-attempts | 3                        | Defines the number of connection retries before the client gives up. |
| high-availability.zookeeper.client.retry-wait         | 5000                     | Defines the pause between consecutive retries in ms.         |
| high-availability.zookeeper.client.session-timeout    | 60000                    | Defines the session timeout for the ZooKeeper session in ms. |
| high-availability.zookeeper.path.checkpoint-counter   | "/checkpoint-counter"    | ZooKeeper root path (ZNode) for checkpoint counters.         |
| high-availability.zookeeper.path.checkpoints          | "/checkpoints"           | ZooKeeper root path (ZNode) for completed checkpoints.       |
| high-availability.zookeeper.path.jobgraphs            | "/jobgraphs"             | ZooKeeper root path (ZNode) for job graphs                   |
| high-availability.zookeeper.path.latch                | "/leaderlatch"           | Defines the znode of the leader latch which is used to elect the leader. |
| high-availability.zookeeper.path.leader               | "/leader"                | Defines the znode of the leader which contains the URL to the leader and the current leader session ID. |
| high-availability.zookeeper.path.mesos-workers        | "/mesos-workers"         | The ZooKeeper root path for persisting the Mesos worker information. |
| high-availability.zookeeper.path.root                 | "/flink"                 | The root path under which Flink stores its entries in ZooKeeper. |
| high-availability.zookeeper.path.running-registry     | "/running_job_registry/" |                                                              |
| high-availability.zookeeper.quorum                    | (none)                   | The ZooKeeper quorum to use, when running Flink in a high-availability mode with ZooKeeper. |



# standalone

```
vim /etc/profile
export FLINK_HOME=/home/hadoop/flink
export PATH=$PATH:$FLINK_HOME/bin

source /etc/profile

vim conf/masters
master:8081

vim conf/slaves
work1
work2

vim conf/flink-conf.yaml
taskmanager.numberOfTaskSlots: 2
jobmanager.rpc.address: master
```

# HA

配置参考地址：

https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/jobmanager_high_availability.html

```
vim /etc/profile
export FLINK_HOME=/home/hadoop/flink
export PATH=$PATH:$FLINK_HOME/bin

source /etc/profile

vim conf/flink-conf.yaml
jobmanager.rpc.address: localhost
state.backend: filesystem
state.backend.fs.checkpointdir: file:///zz/data/flink/flink-checkpoints
# 启用高可用
high-availability: zookeeper
high-availability.zookeeper.quorum: master:2181,work1:2181,work2:2181
high-availability.zookeeper.path.root：/ flink
high-availability.cluster-id: /flink_only_ha
high-availability.storageDir: file:///zz/data/flink/high_availability_storageDir
high-availability.zookeeper.client.acl: open


vim zoo.cfg
server.1=master:2888:3888
server.2=work1:2888:3888
server.3=work2:2888:3888

vim master
master:8081
work1:8081

vim conf/slaves
master
work1
work2
```



# 官网

flink kafka：

https://ci.apache.org/projects/flink/flink-docs-stable/dev/connectors/kafka.html

中文官网：https://flink.xskoo.com/

# 博客地址：

Flink读写系列之-读mysql并写入mysql：

https://blog.csdn.net/javajxz008/article/details/83182063

Flink+kafka+redis实时计算wordcount：





Flink开发中遇到的问题及解法
https://blog.csdn.net/mytobaby00/article/details/79861162


Flink开发实战二 ——实战案例
https://blog.csdn.net/aa518189/article/details/82755074


Flink状态计算实例与状态数据恢复(checkpoint)
https://blog.csdn.net/javajxz008/article/details/86218148


优化Flink应用的4种方式
https://blog.csdn.net/dev_csdn/article/details/78330867


Hadoop学习之路（二十四）YARN的资源调度
https://www.cnblogs.com/qingyunzong/p/8615096.html




# temp
## flink yarn-session的两种使用方式

https://blog.csdn.net/xu470438000/article/details/79633190



## Flink提交任务至yarn

https://blog.csdn.net/asfjgvajfghaklsbf/article/details/82899872

> 我们可以使用run选项运行Flink作业。这个脚本可以自动获取到YARN session的地址
>
> 线上脚本： nohup bin/flink run -s hdfs:///flink/savepoints/savepoint-bcabee-bf3f54a3b924 -c **** jars/**** test > Flink-RealtimeDAU.log 2>&1 &
>
> 通过-s 可以指定savepoints地址，来重跑job。
>
> Savepoint是什么
>
> Flink的savepoint是一个全局的、一致性的快照（snapshot）。其包含两方面：
>
> 数据源所有数据的位置；
> 并行操作的状态
> “全局一致”是指所有的输入源数据在指定的位置，所有的并行操作的状态都被完全checkpoint了。
>
> 如果你的应用在过去某个时间点做了savepoint，那你随时可以从前面的savepoint更新发布应用。这时，新的应用会从savepoint中的操作状态进行初始化，并从savepoint的数据源位置开始重新处理所有数据。
>
> 3.启动之后如何停止运行的程序
>
> 关闭jobmanager
> 线上脚本：bin/flink cancel -s hdfs:///flink/savepoints /savepoints-* -yid application_1535964220442_0034
>
> 通过cancel命令进行停止
>
> 或者通过yarn application -kill applicationId 直接将yarn-session停止掉
>
> 或者通过 flink list 获得 jobId 
>
> bin/flink cancel -s hdfs:///flink/savepoints/savepoint-* jobId 
>
> 其中-s为可选操作
>
> ---------------------
> 第二种方式 
>
> nohup bin/flink run -m yarn-cluster -yn 7  -s hdfs:///flink/savepoints/savepoint-* -c *.* jars/**** test > Flink-RealtimeDAU.log 2>&1 &
>
> 其中的-yn是指TaskManager的个数，必须指定。



## flink 任务提交参数(孙敏)

```shell
# 以后台运行的方式提交 flink job，yarn 执行
nohup ${flink_Home}/bin/flink run 
-c org.apache.flink.examples.java.wordcount.WordCount \ #指定jar包中程序的入口
-ynm  flink_job_wordcount \ #flink job 在yarn-cluster的application name
-m yarn-cluster  \ # 以yarn-cluster的方式运行程序
-yjm  1024m \ # yarn-cluster 下启动的jobmanager
-yn 2  \ # 启动taskManager 的数量
-ytm  1024m \ # yarn-cluster taskManager 内存
-ys   3 \ # yarn-cluster jobManager 申请核数
-yqu  default  \ #yarn-cluster 资源队列名称
-p 16 \ #指定程序运行的并行度 ，此项选择使用
-q  \ #指定flink job 禁止输出日志，此项慎重使用
-s hdfs:///flink/savepoints/savepoint-bcabee-bf3f54a3b924 \ #flink job 的保存节点位置
./examples/batch/WordCount.jar  \ # jar包位置
--input file:///home/xiaosi/a.txt  \ #flink job输入的参数
--output file:///home/xiaosi/result.txt  \ # flink job 输入的参数 
> Flink-RealtimeDAU.log 2>&1 &
#其中的-yn是指TaskManager的个数，必须指定
```

## 历史任务

https://flink.xskoo.com/monitoring/historyserver.html

HistoryServer允许您查询已由JobManager归档的已完成作业的状态和统计信息。

配置HistoryServer *和* JobManager后，可以通过相应的启动脚本启动和停止HistoryServer：

```shell
# Start or stop the HistoryServer
bin/historyserver.sh (start|start-foreground|stop)
```

默认情况下，此服务器绑定`localhost`并侦听端口`8082`。

目前，您只能将其作为独立进程运行。

**配置**

配置键`jobmanager.archive.fs.dir`，`historyserver.archive.fs.refresh-interval`需要进行调整以存档和显示存档的作业。

**JobManager**

已完成作业的归档发生在JobManager上，JobManager将归档的作业信息上载到文件系统目录。您可以`flink-conf.yaml`通过设置目录来配置目录以存档已完成的作业`jobmanager.archive.fs.dir`。

```yaml
# Directory to upload completed job information
jobmanager.archive.fs.dir: hdfs:///completed-jobs
```

**HistoryServer**

可以将HistoryServer配置为监视via中以逗号分隔的目录列表`historyserver.archive.fs.dir`。为新存档定期轮询已配置的目录; 轮询间隔可以通过配置`historyserver.archive.fs.refresh-interval`。

```yaml
# Monitor the following directories for completed jobs
historyserver.archive.fs.dir: hdfs:///completed-jobs

# Refresh every 10 seconds
historyserver.archive.fs.refresh-interval: 10000
```

包含的存档将下载并缓存在本地文件系统中。通过此配置本地目录`historyserver.web.tmpdir`。

查看配置页面以获取[完整的配置选项列表](https://flink.xskoo.com/ops/config.html#history-server)。

**可用请求**

以下是可用请求列表，其中包含示例JSON响应。所有请求都是样本表单`http://hostname:8082/jobs`，下面我们仅列出URL 的*路径*部分。

尖括号中的值是变量，例如`http://hostname:port/jobs/<jobid>/exceptions`必须请求例如`http://hostname:port/jobs/7684be6004e4e955c2a558a9bc463f65/exceptions`。

- `/config`
- `/jobs/overview`
- `/jobs/<jobid>`
- `/jobs/<jobid>/vertices`
- `/jobs/<jobid>/config`
- `/jobs/<jobid>/exceptions`
- `/jobs/<jobid>/accumulators`
- `/jobs/<jobid>/vertices/<vertexid>`
- `/jobs/<jobid>/vertices/<vertexid>/subtasktimes`
- `/jobs/<jobid>/vertices/<vertexid>/taskmanagers`
- `/jobs/<jobid>/vertices/<vertexid>/accumulators`
- `/jobs/<jobid>/vertices/<vertexid>/subtasks/accumulators`
- `/jobs/<jobid>/vertices/<vertexid>/subtasks/<subtasknum>`
- `/jobs/<jobid>/vertices/<vertexid>/subtasks/<subtasknum>/attempts/<attempt>`
- `/jobs/<jobid>/vertices/<vertexid>/subtasks/<subtasknum>/attempts/<attempt>/accumulators`
- `/jobs/<jobid>/plan`



## 命令

> 线上脚本： bin/yarn-session.sh -n 7 -s 8 -jm 3072 -tm 32768 -qu root.*.*-nm *-* -d
> 其中申请7个taskManager 每个8核 每个taskmanager有32768M内存 
>
> 
>
> yarn application -kill application_1548656154852_0008
> ./bin/flink run -yn 1 examples/streaming/SocketWindowWordCount.jar --hostname master --port 9001



## help

### flink --help

```
[hadoop@master flink]$ flink --help
2019-01-28 00:26:01,101 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - Found Yarn properties file under /tmp/.yarn-properties-hadoop.
2019-01-28 00:26:01,101 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - Found Yarn properties file under /tmp/.yarn-properties-hadoop.
./flink <ACTION> [OPTIONS] [ARGUMENTS]

The following actions are available:

Action "run" compiles and runs a program.

  Syntax: run [OPTIONS] <jar-file> <arguments>
  "run" action options:
     -c,--class <classname>               Class with the program entry point
                                          ("main" method or "getPlan()" method.
                                          Only needed if the JAR file does not
                                          specify the class in its manifest.
     -C,--classpath <url>                 Adds a URL to each user code
                                          classloader  on all nodes in the
                                          cluster. The paths must specify a
                                          protocol (e.g. file://) and be
                                          accessible on all nodes (e.g. by means
                                          of a NFS share). You can use this
                                          option multiple times for specifying
                                          more than one URL. The protocol must
                                          be supported by the {@link
                                          java.net.URLClassLoader}.
     -d,--detached                        If present, runs the job in detached
                                          mode
     -n,--allowNonRestoredState           Allow to skip savepoint state that
                                          cannot be restored. You need to allow
                                          this if you removed an operator from
                                          your program that was part of the
                                          program when the savepoint was
                                          triggered.
     -p,--parallelism <parallelism>       The parallelism with which to run the
                                          program. Optional flag to override the
                                          default value specified in the
                                          configuration.
     -q,--sysoutLogging                   If present, suppress logging output to
                                          standard out.
     -s,--fromSavepoint <savepointPath>   Path to a savepoint to restore the job
                                          from (for example
                                          hdfs:///flink/savepoint-1537).
     -sae,--shutdownOnAttachedExit        If the job is submitted in attached
                                          mode, perform a best-effort cluster
                                          shutdown when the CLI is terminated
                                          abruptly, e.g., in response to a user
                                          interrupt, such as typing Ctrl + C.
  Options for yarn-cluster mode:
     -d,--detached                        If present, runs the job in detached
                                          mode
     -m,--jobmanager <arg>                Address of the JobManager (master) to
                                          which to connect. Use this flag to
                                          connect to a different JobManager than
                                          the one specified in the
                                          configuration.
     -sae,--shutdownOnAttachedExit        If the job is submitted in attached
                                          mode, perform a best-effort cluster
                                          shutdown when the CLI is terminated
                                          abruptly, e.g., in response to a user
                                          interrupt, such as typing Ctrl + C.
     -yD <property=value>                 use value for given property
     -yd,--yarndetached                   If present, runs the job in detached
                                          mode (deprecated; use non-YARN
                                          specific option instead)
     -yh,--yarnhelp                       Help for the Yarn session CLI.
     -yid,--yarnapplicationId <arg>       Attach to running YARN session
     -yj,--yarnjar <arg>                  Path to Flink jar file
     -yjm,--yarnjobManagerMemory <arg>    Memory for JobManager Container with
                                          optional unit (default: MB)
     -yn,--yarncontainer <arg>            Number of YARN container to allocate
                                          (=Number of Task Managers)
     -ynl,--yarnnodeLabel <arg>           Specify YARN node label for the YARN
                                          application
     -ynm,--yarnname <arg>                Set a custom name for the application
                                          on YARN
     -yq,--yarnquery                      Display available YARN resources
                                          (memory, cores)
     -yqu,--yarnqueue <arg>               Specify YARN queue.
     -ys,--yarnslots <arg>                Number of slots per TaskManager
     -yst,--yarnstreaming                 Start Flink in streaming mode
     -yt,--yarnship <arg>                 Ship files in the specified directory
                                          (t for transfer)
     -ytm,--yarntaskManagerMemory <arg>   Memory per TaskManager Container with
                                          optional unit (default: MB)
     -yz,--yarnzookeeperNamespace <arg>   Namespace to create the Zookeeper
                                          sub-paths for high availability mode
     -z,--zookeeperNamespace <arg>        Namespace to create the Zookeeper
                                          sub-paths for high availability mode

  Options for default mode:
     -m,--jobmanager <arg>           Address of the JobManager (master) to which
                                     to connect. Use this flag to connect to a
                                     different JobManager than the one specified
                                     in the configuration.
     -z,--zookeeperNamespace <arg>   Namespace to create the Zookeeper sub-paths
                                     for high availability mode



Action "info" shows the optimized execution plan of the program (JSON).

  Syntax: info [OPTIONS] <jar-file> <arguments>
  "info" action options:
     -c,--class <classname>           Class with the program entry point ("main"
                                      method or "getPlan()" method. Only needed
                                      if the JAR file does not specify the class
                                      in its manifest.
     -p,--parallelism <parallelism>   The parallelism with which to run the
                                      program. Optional flag to override the
                                      default value specified in the
                                      configuration.


Action "list" lists running and scheduled programs.

  Syntax: list [OPTIONS]
  "list" action options:
     -r,--running     Show only running programs and their JobIDs
     -s,--scheduled   Show only scheduled programs and their JobIDs
  Options for yarn-cluster mode:
     -m,--jobmanager <arg>            Address of the JobManager (master) to
                                      which to connect. Use this flag to connect
                                      to a different JobManager than the one
                                      specified in the configuration.
     -yid,--yarnapplicationId <arg>   Attach to running YARN session
     -z,--zookeeperNamespace <arg>    Namespace to create the Zookeeper
                                      sub-paths for high availability mode

  Options for default mode:
     -m,--jobmanager <arg>           Address of the JobManager (master) to which
                                     to connect. Use this flag to connect to a
                                     different JobManager than the one specified
                                     in the configuration.
     -z,--zookeeperNamespace <arg>   Namespace to create the Zookeeper sub-paths
                                     for high availability mode



Action "stop" stops a running program (streaming jobs only).

  Syntax: stop [OPTIONS] <Job ID>
  "stop" action options:

  Options for yarn-cluster mode:
     -m,--jobmanager <arg>            Address of the JobManager (master) to
                                      which to connect. Use this flag to connect
                                      to a different JobManager than the one
                                      specified in the configuration.
     -yid,--yarnapplicationId <arg>   Attach to running YARN session
     -z,--zookeeperNamespace <arg>    Namespace to create the Zookeeper
                                      sub-paths for high availability mode

  Options for default mode:
     -m,--jobmanager <arg>           Address of the JobManager (master) to which
                                     to connect. Use this flag to connect to a
                                     different JobManager than the one specified
                                     in the configuration.
     -z,--zookeeperNamespace <arg>   Namespace to create the Zookeeper sub-paths
                                     for high availability mode



Action "cancel" cancels a running program.

  Syntax: cancel [OPTIONS] <Job ID>
  "cancel" action options:
     -s,--withSavepoint <targetDirectory>   Trigger savepoint and cancel job.
                                            The target directory is optional. If
                                            no directory is specified, the
                                            configured default directory
                                            (state.savepoints.dir) is used.
  Options for yarn-cluster mode:
     -m,--jobmanager <arg>            Address of the JobManager (master) to
                                      which to connect. Use this flag to connect
                                      to a different JobManager than the one
                                      specified in the configuration.
     -yid,--yarnapplicationId <arg>   Attach to running YARN session
     -z,--zookeeperNamespace <arg>    Namespace to create the Zookeeper
                                      sub-paths for high availability mode

  Options for default mode:
     -m,--jobmanager <arg>           Address of the JobManager (master) to which
                                     to connect. Use this flag to connect to a
                                     different JobManager than the one specified
                                     in the configuration.
     -z,--zookeeperNamespace <arg>   Namespace to create the Zookeeper sub-paths
                                     for high availability mode



Action "savepoint" triggers savepoints for a running job or disposes existing ones.

  Syntax: savepoint [OPTIONS] <Job ID> [<target directory>]
  "savepoint" action options:
     -d,--dispose <arg>       Path of savepoint to dispose.
     -j,--jarfile <jarfile>   Flink program JAR file.
  Options for yarn-cluster mode:
     -m,--jobmanager <arg>            Address of the JobManager (master) to
                                      which to connect. Use this flag to connect
                                      to a different JobManager than the one
                                      specified in the configuration.
     -yid,--yarnapplicationId <arg>   Attach to running YARN session
     -z,--zookeeperNamespace <arg>    Namespace to create the Zookeeper
                                      sub-paths for high availability mode

  Options for default mode:
     -m,--jobmanager <arg>           Address of the JobManager (master) to which
                                     to connect. Use this flag to connect to a
                                     different JobManager than the one specified
                                     in the configuration.
     -z,--zookeeperNamespace <arg>   Namespace to create the Zookeeper sub-paths
                                     for high availability mode



Action "modify" modifies a running job (e.g. change of parallelism).

  Syntax: modify <Job ID> [OPTIONS]
  "modify" action options:
     -h,--help                           Show the help message for the CLI
                                         Frontend or the action.
     -p,--parallelism <newParallelism>   New parallelism for the specified job.
     -v,--verbose                        This option is deprecated.
  Options for yarn-cluster mode:
     -m,--jobmanager <arg>            Address of the JobManager (master) to
                                      which to connect. Use this flag to connect
                                      to a different JobManager than the one
                                      specified in the configuration.
     -yid,--yarnapplicationId <arg>   Attach to running YARN session
     -z,--zookeeperNamespace <arg>    Namespace to create the Zookeeper
                                      sub-paths for high availability mode

  Options for default mode:
     -m,--jobmanager <arg>           Address of the JobManager (master) to which
                                     to connect. Use this flag to connect to a
                                     different JobManager than the one specified
                                     in the configuration.
     -z,--zookeeperNamespace <arg>   Namespace to create the Zookeeper sub-paths
                                     for high availability mode


```

```
./flink <ACTION> [OPTIONS] [ARGUMENTS]
 
可以使用以下操作：
 
命令 "run" 编译并运行程序。
 
  Syntax: run [OPTIONS] <jar-file> <arguments>
  "run" action options:
     -c,--class <classname>               程序入口类
                                          ("main" 方法 或 "getPlan()" 方法)
                                          仅在 JAR 文件没有在 manifest 中指定类的时候使用
     -C,--classpath <url>                 在群集中的所有节点上向每个用户代码类加载器添加URL。
                                          路径必须指定协议（例如文件：//），并且可以在所有节点上访问（例如，通过NFS共享）。
                                          您可以多次使用此选项来指定多个URL。该协议必须由 {@link java.net.URLClassLoader} 支持。
     -d,--detached                        以独立模式运行任务
     -n,--allowNonRestoredState           允许跳过无法还原的保存点状态。
                                          当触发保存点的时候，
                                          你需要允许这个行为如果以从你的应用程序中移除一个算子 
     -p,--parallelism <parallelism>       运行程序的并行度。 可以选择覆盖配置中指定的默认值。
     -q,--sysoutLogging                   将日志输出到标准输出
     -s,--fromSavepoint <savepointPath>   从保存点的路径中恢复作业 (例如
                                          hdfs:///flink/savepoint-1537)
  Options for yarn-cluster mode:
     -d,--detached                        以独立模式运行任务
     -m,--jobmanager <arg>                连接 JobManager（主）的地址。
                                          使用此标志连接一个不同的 JobManager 在配置中指定的
     -yD <property=value>                 使用给定属性的值
     -yd,--yarndetached                   以独立模式运行任务（过期的；用 non-YARN 选项代替）
     -yh,--yarnhelp                       Yarn session CLI 的帮助信息
     -yid,--yarnapplicationId <arg>       用来运行 YARN Session 的 ID
     -yj,--yarnjar <arg>                  Flink jar 文件的路径
     -yjm,--yarnjobManagerMemory <arg>    JobManager 容器的内存可选单元（默认值: MB)
     -yn,--yarncontainer <arg>            分配 YARN 容器的数量(=TaskManager 的数量)
     -ynm,--yarnname <arg>                给应用程序一个自定义的名字显示在 YARN 上
     -yq,--yarnquery                      显示 YARN 的可用资源（内存，队列）
     -yqu,--yarnqueue <arg>               指定 YARN 队列
     -ys,--yarnslots <arg>                每个 TaskManager 的槽位数量
     -yst,--yarnstreaming                 以流式处理方式启动 Flink
     -yt,--yarnship <arg>                 在指定目录中传输文件
                                          (t for transfer)
     -ytm,--yarntaskManagerMemory <arg>   每个 TaskManager 容器的内存可选单元（默认值: MB）
     -yz,--yarnzookeeperNamespace <arg>   用来创建高可用模式的 Zookeeper 的子路径的命名空间。
     -ynl,--yarnnodeLabel <arg>           指定 YARN 应用程序  YARN 节点标签
     -z,--zookeeperNamespace <arg>        用来创建高可用模式的 Zookeeper 的子路径的命名空间。
 
  Options for default mode:
     -m,--jobmanager <arg>           连接 JobManager（主）的地址。
                                     使用此标志连接一个不同的 JobManager 在配置中指定的。
     -z,--zookeeperNamespace <arg>   用来创建高可用模式的 Zookeeper 的子路径的命名空间。
 
 
 
Action "info" 显示程序的优化执行计划(JSON).
 
  Syntax: info [OPTIONS] <jar-file> <arguments>
  "info" action options:
     -c,--class <classname>           具有程序入口的类
                                      ("main" 方法 或 "getPlan()" 方法)
                                      仅在如果 JAR 文件没有在 manifest 中指定类的时候使用
     -p,--parallelism <parallelism>   运行程序的并行度。 可以选择覆盖配置中指定的默认值。
 
 
Action "list" 罗列出正在运行和调度的作业
 
  Syntax: list [OPTIONS]
  "list" action options:
     -r,--running     只显示运行中的程序和他们的 JobID
     -s,--scheduled   只显示调度的程序和他们的 JobID
  Options for yarn-cluster mode:
     -m,--jobmanager <arg>            连接 JobManager（主）的地址。
                                      使用此标志连接一个不同的 JobManager 在配置中指定的。
     -yid,--yarnapplicationId <arg>   用来运行 YARN Session 的 ID。
     -z,--zookeeperNamespace <arg>    用来创建高可用模式的 Zookeeper 的子路径的命名空间。
 
  Options for default mode:
     -m,--jobmanager <arg>           连接 JobManager（主）的地址。
                                     使用此标志连接一个不同的 JobManager 在配置中指定的。
     -z,--zookeeperNamespace <arg>   用来创建高可用模式的 Zookeeper 的子路径的命名空间。
 
 
 
Action "stop" 停止正在运行的程序 （仅限流式处理作业）
 
  Syntax: stop [OPTIONS] <Job ID>
  "stop" action options:
 
  Options for yarn-cluster mode:
     -m,--jobmanager <arg>            连接 JobManager（主）的地址。
                                      使用此标志连接一个不同的 JobManager 在配置中指定的。
     -yid,--yarnapplicationId <arg>   用来运行 YARN Session 的 ID。
     -z,--zookeeperNamespace <arg>    用来创建高可用模式的 Zookeeper 的子路径的命名空间。
 
  Options for default mode:
     -m,--jobmanager <arg>           连接 JobManager（主）的地址。
                                     使用此标志连接一个不同的 JobManager 在配置中指定的。
     -z,--zookeeperNamespace <arg>   用来创建高可用模式的 Zookeeper 的子路径的命名空间。
 
 
 
Action "cancel" 取消正在运行的程序。
 
  Syntax: cancel [OPTIONS] <Job ID>
  "cancel" action options:
     -s,--withSavepoint <targetDirectory>   触发保存点和取消作业。
                                            目标目录是可选的。
                                            如果没有指定目录，使用默认配置
                                            (state.savepoints.dir)。
  Options for yarn-cluster mode:
     -m,--jobmanager <arg>            连接 JobManager（主）的地址。
                                      使用此标志连接一个不同的 JobManager 在配置中指定的。
     -yid,--yarnapplicationId <arg>   用来运行 YARN Session 的 ID。
     -z,--zookeeperNamespace <arg>    用来创建高可用模式的 Zookeeper 的子路径的命名空间。
 
  Options for default mode:
     -m,--jobmanager <arg>           连接 JobManager（主）的地址。
                                     使用此标志连接一个不同的 JobManager 在配置中指定的。
     -z,--zookeeperNamespace <arg>   用来创建高可用模式的 Zookeeper 的子路径的命名空间。
 
 
 
Action "savepoint" 触发运行作业的保存点，或处理现有作业。
 
  Syntax: savepoint [OPTIONS] <Job ID> [<target directory>]
  "savepoint" action options:
     -d,--dispose <arg>       保存点的处理路径。
     -j,--jarfile <jarfile>   Flink 程序的 JAR 文件。
  Options for yarn-cluster mode:
     -m,--jobmanager <arg>            连接 JobManager（主）的地址。
                                      使用此标志连接一个不同的 JobManager 在配置中指定的。
     -yid,--yarnapplicationId <arg>   用来运行 YARN Session 的 ID。
     -z,--zookeeperNamespace <arg>    用来创建高可用模式的 Zookeeper 的子路径的命名空间。
 
  Options for default mode:
     -m,--jobmanager <arg>           连接 JobManager（主）的地址。
                                     使用此标志连接一个不同的 JobManager 在配置中指定的。
     -z,--zookeeperNamespace <arg>   用来创建高可用模式的 Zookeeper 的子路径的命名空间。
 
 
 
Action "modify" 修改正在运行的作业 （例如：修改并行度）.
 
  Syntax: modify <Job ID> [OPTIONS]
  "modify" action options:
     -h,--help                           用来显示命令行的帮助信息。
     -p,--parallelism <newParallelism>   指定作业新的并行度。
     -v,--verbose                        这个选项过期了
  Options for yarn-cluster mode:
     -m,--jobmanager <arg>            连接 JobManager（主）的地址。
                                      使用此标志连接一个不同的 JobManager 在配置中指定的。
     -yid,--yarnapplicationId <arg>   用来运行 YARN Session 的 ID。
     -z,--zookeeperNamespace <arg>    用来创建高可用模式的 Zookeeper 的子路径的命名空间。
 
  Options for default mode:
     -m,--jobmanager <arg>           要连接的JobManager（主节点）的地址。
                                     使用此标志可连接到与配置中指定的不同的 JobManager。
     -z,--zookeeperNamespace <arg>   用来创建高可用模式的 Zookeeper 的子路径的命名空间。
```



## 优化

### 配置临时I / O目录
>https://flink.xskoo.com/ops/config.html
>
>虽然Flink的目标是尽可能多地处理主内存中的数据，但是需要处理的内存比内存更多的数据并不少见.Flink的运行时用于将临时数据写入磁盘以处理这些情况.
>
>该`taskmanager.tmp.dirs`参数指定Flink写入临时文件的目录列表.目录的路径需要用'：'（冒号字符）分隔.Flink将同时向（从）每个配置的目录写入（或读取）一个临时文件.这样，临时I / O可以均匀地分布在多个独立的I / O设备（如硬盘）上，以提高性能.要利用快速I / O设备（例如，SSD，RAID，NAS），可以多次指定目录.
>
>如果`taskmanager.tmp.dirs`未显式指定参数，Flink会将临时数据写入 算子操作系统的临时目录，例如Linux系统中的*/ tmp*.

### 网络缓冲区

> 如果您看到异常`java.io.IOException: Insufficient number of network buffers`，则需要调整用于网络缓冲区的内存量，以便程序在您的TaskManager上运行
>
> 网络缓冲区是通信层的关键资源.它们用于在通过网络传输之前缓冲记录，并在将传入数据解析为记录并将其传递给应用程序之前缓冲传入数据.足够数量的网络缓冲区对于实现良好的吞吐量至关重要.
>
> 使用配置 “网络缓冲区占jvm内存的百分比” 取代 “直接设置网络缓冲区的数量”
>
> - `taskmanager.network.memory.fraction`：用于网络缓冲区的JVM内存的百分比（DEFAULT：0.1），
> - `taskmanager.network.memory.min`：网络缓冲区的最小内存大小（默认值：64MB），
> - `taskmanager.network.memory.max`：网络缓冲区的最大内存大小（默认值：1GB），和
> - `taskmanager.memory.segment-size`：内存管理器和网络堆栈使用的内存缓冲区大小（以字节为单位）（默认值：32KB）

### flink的并行度



https://blog.csdn.net/rlnLo2pNEfx9c/article/details/80809738

- 算子层面修改

  ```scala
  val env = StreamExecutionEnvironment.getExecutionEnvironment
  
  val text = [...]
  
  # setParallelism 修改并行度
  val wordCounts = text
     .flatMap{ _.split(" ") map { (_, 1) } }
     .keyBy(0)
     .timeWindow(Time.seconds(5))
     .sum(1).setParallelism(5)
  
  wordCounts.print()
  
  env.execute("Word Count Example")
  ```

  

- 执行环境层修改

  > flink程序执行需要执行环境上下文。执行环境为其要执行的操作算子，数据源，数据sinks都是设置了默认的并行度。执行环境的并行度可以通过操作算子显示指定并行度来覆盖掉。
  > 默认的执行环境并行度可以通过调用setParallelism()来设置。例如，操作算子，数据源，数据接收器，并行度都设置为3，那么在执行环境层面，设置方式如下

  ```
  val env = StreamExecutionEnvironment.getExecutionEnvironment
  
  env.setParallelism(3)
  
  val text = [...]
  
  val wordCounts = text
     .flatMap{ _.split(" ") map { (_, 1) } }
     .keyBy(0)
     .timeWindow(Time.seconds(5))
     .sum(1)
  
  wordCounts.print()
  
  env.execute("Word Count Example")
  ```

  

- 提交job 到flink的时候

  ```shell
  # -p设置并行度
  ./bin/flink run -p 3 ../examples/*WordCount-java*.jar
  ```

- 配置文件设置

  ```yaml
  # vim conf/flink-conf.yaml
  parallelism.default: 3
  ```

- 配置taskManager的槽数

  > 集群中Flink的taskmanager提供处理slot。Slots数量最合适的是跟taskmanager的cores数量成正比。当然，taskmanager.numberOfTaskSlots的推荐值就是[cpu核心](https://www.baidu.com/s?wd=cpu%E6%A0%B8%E5%BF%83&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)的数目。

  ```
  # vim conf/flink-conf.yaml
  taskmanager.numberOfTaskSlots: 2
  ```







# 自定义source

实现并行度为1的自定义source
实现SourceFunction 
一般不需要实现容错性保证
处理好cancel方法(cancel应用的时候，这个方法会被调用)

实现并行化的自定义source
实现ParallelSourceFunction 
或者继承RichParallelSourceFunction 